cntk chat windows build status linux build status the microsoft cognitive toolkit https cntk ai is a unified deep learning toolkit that describes neural networks as a series of computational steps via a directed graph in this directed graph leaf nodes represent input values or network parameters while other nodes represent matrix operations upon their inputs cntk allows users to easily realize and combine popular model types such as feed forward dnns convolutional nets cnns and recurrent networks rnns lstms it implements stochastic gradient descent sgd error backpropagation learning with automatic differentiation and parallelization across multiple gpus and servers cntk has been available under an open source license since april 2015 it is our hope that the community will take advantage of cntk to share ideas more quickly through the exchange of open source working code installation setup cntk windows python only script driven manual linux python only script driven manual docker cntk backend for keras setup cntk development environment windows script driven manual linux manual installing nightly packages if you prefer to use latest cntk bits from master use one of the cntk nightly packages nightly packages for windows nightly packages for linux learning cntk you can learn more about using and contributing to cntk with the following resources general documentation python api documentation evaluation documentation c c net python java manual tutorials examples pretrained models blog presentations license more information contribute to cntk faq feedback disclaimer cntk is in active use at microsoft and constantly evolving there will be bugs microsoft open source code of conduct this project has adopted the microsoft open source code of conduct for more information see the code of conduct faq or contact opencode microsoft com with any additional questions or comments news you can find more news on the official project feed project changelog 2018 04 16 cntk 2 5 1 repack cntk 2 5 with third party libraries included in the bundles python wheel packages 2018 03 15 cntk 2 5 change profiler details output format to be chrome tracing enable per node timing working example here per node timing creates items in profiler details when profiler is enabled usage in python python import cntk as c c debugging debug set node timing true c debugging start profiler optional c debugging enable profiler optional executions print node timing c debugging stop profiler example profiler details view in chrome tracing cpu inference performance improvements using mkl accelerates some common tensor ops in intel cpu inference for float32 especially for fully connected networks can be turned on off by cntk cntk py enable cpueval optimization cntk cntk py disable cpueval optimization 1bitsgd incorporated into cntk 1bitsgd source code is now available with cntk license mit license under source 1bitsgd 1bitsgd build target was merged into existing gpu target new loss function hierarchical softmax thanks yaochengji for the contribution distributed training with mulitple learners trainer now accepts multiple parameter learners for distributed training with this change different parameters of a network can be learned by different learners in a single training session this also facilitates distributed training for gans for more information please refer to the basic gan distributed py and the cntk learners distributed multi learner test py operators added meanvariancenormalization operator bug fixes fixed convergence issue in tutorial 201b fixed pooling unpooling to support free dimension for sequences fixed crash in cntkbinaryformat deserializer when crossing sweep boundary fixed shape inference bug in rnn step function for scalar broadcasting fixed a build bug when mpi no improved distributed training aggregation speed by increasing packing threshold and expose the knob in v2 fixed a memory leak in mkl layout fixed a bug in cntk convert api in misc converter py which prevents converting complex networks onnx updates cntk exported onnx models are now onnx checker compliant added onnx support for cntks optimizedrnnstack operator lstm only added support for lstm and gru operators added support for experimental onnx op meanvariancenormalization added support for experimental onnx op identity added support for exporting cntks layernormalization layer using onnx meanvariancenormalization op bug or minor fixes axis attribute is optional in cntks onnx concat operator bug fix in onnx broadcasting for scalars bug fix in onnx convtranspose operator backward compatibility bug fix in leakyrelu argument â€˜alpha reverted to type double misc added a new api find by uid under cntk logging graph 2018 02 28 cntk supports nightly build if you prefer to use latest cntk bits from master use one of the cntk nightly package nightly packages for windows nightly packages for linux alternatively you can also click corresponding build badge to land to nightly build page 2018 01 31 cntk 2 4 highlights moved to cuda9 cudnn 7 and visual studio 2017 removed python 3 4 support added volta gpu and fp16 support better onnx support cpu perf improvement more ops ops top k operation in the forward pass it computes the top largest k values and corresponding indices along the specified axis in the backward pass the gradient is scattered to the top k elements an element not in the top k gets a zero gradient gather operation now supports an axis argument squeeze and expand dims operations for easily removing and adding singleton axes zeros like and ones like operations in many situations you can just rely on cntk correctly broadcasting a simple 0 or 1 but sometimes you need the actual tensor depth to space rearranges elements in the input tensor from the depth dimension into spatial blocks typical use of this operation is for implementing sub pixel convolution for some image super resolution models space to depth rearranges elements in the input tensor from the spatial dimensions to the depth dimension it is largely the inverse of depthtospace sum operation create a new function instance that computes element wise sum of input tensors softsign operation create a new function instance that computes the element wise softsign of a input tensor asinh operation create a new function instance that computes the element wise asinh of a input tensor log softmax operation create a new function instance that computes the logsoftmax normalized values of a input tensor hard sigmoid operation create a new function instance that computes the hard sigmoid normalized values of a input tensor element and element not element or element xor element wise logic operations reduce l1 operation computes the l1 norm of the input tensors element along the provided axes reduce l2 operation computes the l2 norm of the input tensors element along the provided axes reduce sum square operation computes the sum square of the input tensors element along the provided axes image scaler operation alteration of image by scaling its individual values onnx there have been several improvements to onnx support in cntk updates updated onnx reshape op to handle inferreddimension adding producer name and producer version fields to onnx models handling the case when neither auto pad nor pads atrribute is specified in onnx conv op bug fixes fixed bug in onnx pooling op serialization bug fix to create onnx inputvariable with only one batch axis bug fixes and updates to implementation of onnx transpose op to match updated spec bug fixes and updates to implementation of onnx conv convtranspose and pooling ops to match updated spec operators group convolution fixed bug in group convolution output of cntk convolution op will change for groups 1 more optimized implementation of group convolution is expected in the next release better error reporting for group convolution in convolution layer halide binary convolution the cntk build can now use optional halide libraries to build cntk binaryconvolution so dll library that can be used with the netopt module the library contains optimized binary convolution operators that perform better than the python based binarized convolution operators to enable halide in the build please download halide release and set halide path environment varibale before starting a build in linux you can use configure with halide directory to enable it for more information on how to use this feature please refer to how to use network optimization see more in the release notes get the release from the cntk releases page 2018 01 22 cntk support for cuda 9 cntk now supports cuda 9 cudnn 7 this requires an update to build environment to ubuntu 16 gcc 5 for linux and visual studio 2017 vctools 14 11 for windows with cuda 9 cntk also added a preview for 16 bit floating point a k a fp16 computation please check out the example of fp16 in resnet50 here notes on fp16 preview fp16 implementation on cpu is not optimized and its not supposed to be used in cpu inference directly user needs to convert the model to 32 bit floating point before running on cpu loss criterion for fp16 training needs to be 32bit for accumulation without overflow using cast function please check the example above readers do not have fp16 output unless using numpy to feed data cast from fp32 to fp16 is needed please check the example above fp16 gradient aggregation is currently only implemented on gpu using nccl2 distributed training with fp16 with mpi is not supported fp16 math is a subset of current fp32 implementation some model may get feature not implemented exception using fp16 fp16 is currently not supported in brainscript please use python for fp16 to setup build and runtime environment on windows install visual studio 2017 with following workloads and components from command line use community version installer as example vs community exe add microsoft visualstudio workload nativedesktop add microsoft visualstudio workload manageddesktop add microsoft visualstudio workload universal add microsoft component pythontools add microsoft visualstudio component vc tools 14 11 install nvidia cuda 9 from powershell run devinstall ps1 start vctools 14 11 command line run cmd k vs2017installdir \vc\auxiliary\build\vcvarsall bat x64 vcvars ver 14 11 open cntk sln from the vctools 14 11 command line note that starting cntk sln other than vctools 14 11 command line would causes cuda 9 build error to setup build and runtime environment on linux using docker please build unbuntu 16 04 docker image using dockerfiles here for other linux systems please refer to the dockerfiles to setup dependent libraries for cntk 2017 12 05 cntk 2 3 1 release of cognitive toolkit v 2 3 1 cntk support for onnx format is now out of preview mode if you want to try onnx you can build from master or pip install one of the below wheels that matches your python environment for windows cpu only python 2 7 https cntk ai pythonwheel cpu only cntk 2 3 1 cp27 cp27m win amd64 whl python 3 4 https cntk ai pythonwheel cpu only cntk 2 3 1 cp34 cp34m win amd64 whl python 3 5 https cntk ai pythonwheel cpu only cntk 2 3 1 cp35 cp35m win amd64 whl python 3 6 https cntk ai pythonwheel cpu only cntk 2 3 1 cp36 cp36m win amd64 whl for windows gpu python 2 7 https cntk ai pythonwheel gpu cntk 2 3 1 cp27 cp27m win amd64 whl python 3 4 https cntk ai pythonwheel gpu cntk 2 3 1 cp34 cp34m win amd64 whl python 3 5 https cntk ai pythonwheel gpu cntk 2 3 1 cp35 cp35m win amd64 whl python 3 6 https cntk ai pythonwheel gpu cntk 2 3 1 cp36 cp36m win amd64 whl linux cpu only python 2 7 https cntk ai pythonwheel cpu only cntk 2 3 1 cp27 cp27mu linux x86 64 whl python 3 4 https cntk ai pythonwheel cpu only cntk 2 3 1 cp34 cp34m linux x86 64 whl python 3 5 https cntk ai pythonwheel cpu only cntk 2 3 1 cp35 cp35m linux x86 64 whl python 3 6 https cntk ai pythonwheel cpu only cntk 2 3 1 cp36 cp36m linux x86 64 whl linux gpu python 2 7 https cntk ai pythonwheel gpu cntk 2 3 1 cp27 cp27mu linux x86 64 whl python 3 4 https cntk ai pythonwheel gpu cntk 2 3 1 cp34 cp34m linux x86 64 whl python 3 5 https cntk ai pythonwheel gpu cntk 2 3 1 cp35 cp35m linux x86 64 whl python 3 6 https cntk ai pythonwheel gpu cntk 2 3 1 cp36 cp36m linux x86 64 whl you can also try one of the below nuget package cntk cpu only build cntk gpu build cntk uwp cpu only build cntk cpu only model evaluation libraries mkl based