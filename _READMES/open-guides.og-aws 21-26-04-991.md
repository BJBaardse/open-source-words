the open guide to amazon web services ⇦ join us credits ∙ contributing guidelines table of contents purpose why an open guide scope legend aws in general general information learning and career development managing aws managing servers and applications specific aws services basics tips gotchas security and iam 📗 📘 📙 s3 📗 📘 📙 ec2 📗 📘 📙 cloudwatch 📗 📘 📙 amis 📗 📘 📙 auto scaling 📗 📘 📙 ebs 📗 📘 📙 efs 📗 📘 📙 load balancers 📗 📘 📙 clb elb 📗 📘 📙 alb 📗 📘 📙 elastic ips 📗 📘 📙 glacier 📗 📘 📙 rds 📗 📘 📙 rds mysql and mariadb 📗 📘 📙 rds aurora 📗 📘 📙 rds sql server 📗 📘 📙 dynamodb 📗 📘 📙 elasticache 📗 📘 📙 ecs 📗 📘 fargate 📗 📘 📙 lambda 📗 📘 📙 api gateway 📗 📘 📙 step functions 📗 📘 📙 route 53 📗 📘 cloudformation 📗 📘 📙 vpcs network security and security groups 📗 📘 📙 kms 📗 📘 📙 cloudfront 📗 📘 📙 directconnect 📗 📘 redshift 📗 📘 📙 emr 📗 📘 📙 kinesis streams 📗 📘 📙 kinesis firehose 📙 device farm 📗 📘 📙 mobile hub 📗 📘 📙 iot 📗 📘 📙 ses 📗 📘 📙 certificate manager 📗 📘 📙 waf 📗 📘 📙 opsworks 📗 📘 📙 batch 📗 📘 sqs 📗 📘 📙 sns 📗 📘 📙 special topics high availability billing and cost management further reading legal disclaimer license figures and tables figure tools and services market landscape a selection of third party companies products figure aws data transfer costs visual overview of data transfer costs table service matrix how aws services compare to alternatives table aws product maturity and releases aws product releases table storage durability availability and price a quantitative comparison why an open guide a lot of information on aws is already written most people learn aws by reading a blog or a “getting started guide” and referring to the standard aws references nonetheless trustworthy and practical information and recommendations arent easy to come by awss own documentation is a great but sprawling resource few have time to read fully and it doesnt include anything but official facts so omits experiences of engineers the information in blogs or stack overflow is also not consistently up to date this guide is by and for engineers who use aws it aims to be a useful living reference that consolidates links tips gotchas and best practices it arose from discussion and editing over beers by several engineers who have used aws extensively before using the guide please read the license and disclaimer please help this is an early in progress draft its our first attempt at assembling this information so is far from comprehensive still and likely to have omissions or errors please help by joining the slack channel we like to talk about aws in general even if you only have questions — discussion helps the community and guides improvements and contributing to the guide this guide is open to contributions so unlike a blog it can keep improving like any open source effort we combine efforts but also review to ensure high quality scope currently this guide covers selected “core” services such as ec2 s3 load balancers ebs and iam and partial details and tips around other services we expect it to expand it is not a tutorial but rather a collection of information you can read and return to it is for both beginners and the experienced the goal of this guide is to be brief keep it dense and use links practical basic facts concrete details advice gotchas and other “folk knowledge” current we can keep updating it and anyone can contribute improvements thoughtful the goal is to be helpful rather than present dry facts thoughtful opinion with rationale is welcome suggestions notes and opinions based on real experience can be extremely valuable we believe this is both possible with a guide of this format unlike in some other venues this guide is not sponsored by aws or aws affiliated vendors it is written by and for engineers who use aws legend 📒 marks standard official aws pages and docs 🔹 important or often overlooked tip ❗ “serious” gotcha used where risks or time or resource costs are significant critical security risks mistakes with significant financial cost or poor architectural choices that are fundamentally difficult to correct 🔸 “regular” gotcha limitation or quirk used where consequences are things not working breaking or not scaling gracefully 📜 undocumented feature folklore 🐥 relatively new and perhaps immature services or features ⏱ performance discussions ⛓ lock in products or decisions that are likely to tie you to aws in a new or significant way — that is later moving to a non aws alternative would be costly in terms of engineering effort 🚪 alternative non aws options 💸 cost issues discussion and gotchas 🕍 a mild warning attached to “full solution” or opinionated frameworks that may take significant time to understand and or might not fit your needs exactly the opposite of a point solution the cathedral is a nod to raymonds metaphor 📗📘📙 colors indicate basics tips and gotchas respectively 🚧 areas where correction or improvement are needed possibly with link to an issue — do help general information when to use aws aws is the dominant public cloud computing provider in general “cloud computing” can refer to one of three types of cloud “public ” “private ” and “hybrid ” aws is a public cloud provider since anyone can use it private clouds are within a single usually large organization many companies use a hybrid of private and public clouds the core features of aws are infrastructure as a service iaas — that is virtual machines and supporting infrastructure other cloud service models include platform as a service paas which typically are more fully managed services that deploy customers applications or software as a service saas which are cloud based applications aws does offer a few products that fit into these other models too in business terms with infrastructure as a service you have a variable cost model — it is opex not capex though some pre purchased contracts are still capex awss annual revenue was 17 46 billion in 2017 according to their sec 10 k filing or roughly 10 of amazon coms total 2017 revenue main reasons to use aws if your company is building systems or products that may need to scale and you have technical know how and you want the most flexible tools and youre not significantly tied into different infrastructure already and you dont have internal regulatory or compliance reasons you cant use a public cloud based solution and youre not on a microsoft first tech stack and you dont have a specific reason to use google cloud and you can afford manage or negotiate its somewhat higher costs then aws is likely a good option for your company each of those reasons above might point to situations where other services are preferable in practice many if not most tech startups as well as a number of modern large companies can or already do benefit from using aws many large enterprises are partly migrating internal infrastructure to azure google cloud and aws costs billing and cost management are such big topics that we have an entire section on this 🔹ec2 vs other services most users of aws are most familiar with ec2 aws flagship virtual server product and possibly a few others like s3 and clbs but aws products now extend far beyond basic iaas and often companies do not properly understand or appreciate all the many aws services and how they can be applied due to the sharply growing number of services their novelty and complexity branding confusion and fear of ⛓lock in to proprietary aws technology although a bit daunting its important for technical decision makers in companies to understand the breadth of the aws services and make informed decisions we hope this guide will help 🚪aws vs other cloud providers while aws is the dominant iaas provider 31 market share in this 2016 estimate there is significant competition and alternatives that are better suited to some companies this gartner report has a good overview of the major cloud players google cloud platform gcp arrived later to market than aws but has vast resources and is now used widely by many companies including a few large ones it is gaining market share not all aws services have similar or analogous services in gcp and vice versa in particular gcp offers some more advanced machine learning based services like the vision speech and natural language apis its not common to switch once youre up and running but it does happen spotify migrated from aws to google cloud there is more discussion on quora about relative benefits of particular note is that vpcs in gcp are global by default with subnetworks per region while aws vpcs have to live within a particular region this gives gcp an edge if youre designing applications with geo replication from the beginning its also possible to share one gcp vpc between multiple projects roughly analogous to aws accounts while in aws youd have to peer them its also possible to peer gcp vpcs in a similar manner to how its done in aws microsoft azure is the de facto choice for companies and teams that are focused on a microsoft stack and it has now placed significant emphasis on linux as well in china aws footprint is relatively small the market is dominated by alibabas aliyun companies at very large scale may want to reduce costs by managing their own infrastructure for example dropbox migrated to their own infrastructure other cloud providers such as digital ocean offer similar services sometimes with greater ease of use more personalized support or lower cost however none of these match the breadth of products mind share and market domination aws now enjoys traditional managed hosting providers such as rackspace offer cloud solutions as well 🚪aws vs paas if your goal is just to put up a single service that does something relatively simple and youre trying to minimize time managing operations engineering consider a platform as a service such as heroku the aws approach to paas elastic beanstalk is arguably more complex especially for simple use cases 🚪aws vs web hosting if your main goal is to host a website or blog and you dont expect to be building an app or more complex service you may wish consider one of the myriad web hosting services 🚪aws vs managed hosting traditionally many companies pay managed hosting providers to maintain physical servers for them then build and deploy their software on top of the rented hardware this makes sense for businesses who want direct control over hardware due to legacy performance or special compliance constraints but is usually considered old fashioned or unnecessary by many developer centric startups and younger tech companies complexity aws will let you build and scale systems to the size of the largest companies but the complexity of the services when used at scale requires significant depth of knowledge and experience even very simple use cases often require more knowledge to do “right” in aws than in a simpler environment like heroku or digital ocean this guide may help geographic locations aws has data centers in over a dozen geographic locations known as regions in europe east asia north and south america and now australia and india it also has many more edge locations globally for reduced latency of services like cloudfront see the current list of regions and edge locations including upcoming ones if your infrastructure needs to be in close physical proximity to another service for latency or throughput reasons for example latency to an ad exchange viability of aws may depend on the location ⛓lock in as you use aws its important to be aware when you are depending on aws services that do not have equivalents elsewhere lock in may be completely fine for your company or a significant risk its important from a business perspective to make this choice explicitly and consider the cost operational business continuity and competitive risks of being tied to aws aws is such a dominant and reliable vendor many companies are comfortable with using aws to its full extent others can tell stories about the dangers of “cloud jail” when costs spiral generally the more aws services you use the more lock in you have to aws — that is the more engineering resources time and money it will take to change to other providers in the future basic services like virtual servers and standard databases are usually easy to migrate to other providers or on premises others like load balancers and iam are specific to aws but have close equivalents from other providers the key thing to consider is whether engineers are architecting systems around specific aws services that are not open source or relatively interchangeable for example lambda api gateway kinesis redshift and dynamodb do not have substantially equivalent open source or commercial service equivalents while ec2 rds mysql or postgres emr and elasticache more or less do see more below where these are noted with ⛓ combining aws and other cloud providers many customers combine aws with other non aws services for example legacy systems or secure data might be in a managed hosting provider while other systems are aws or a company might only use s3 with another provider doing everything else however small startups or projects starting fresh will typically stick to aws or google cloud only hybrid cloud in larger enterprises it is common to have hybrid deployments encompassing private cloud or on premises servers and aws — or other enterprise cloud providers like ibm bluemix microsoft azure netapp or emc major customers who uses aws and google cloud awss list of customers includes large numbers of mainstream online properties and major brands such as netflix pinterest spotify moving to google cloud airbnb expedia yelp zynga comcast nokia and bristol myers squibb azures list of customers includes companies such as nbc universal 3m and honeywell inc google clouds list of customers is large as well and includes a few mainstream sites such as snapchat best buy dominos and sony music which services to use aws offers a lot of different services — about a hundred at last count most customers use a few services heavily a few services lightly and the rest not at all what services youll use depends on your use cases choices differ substantially from company to company immature and unpopular services just because aws has a service that sounds promising it doesnt mean you should use it some services are very narrow in use case not mature are overly opinionated or have limitations so building your own solution may be better we try to give a sense for this by breaking products into categories must know infrastructure most typical small to medium size users will focus on the following services first if you manage use of aws systems you likely need to know at least a little about all of these even if you dont use them you should learn enough to make that choice intelligently iam user accounts and identities you need to think about accounts early on ec2 virtual servers and associated components including amis machine images load balancers clbs and albs autoscaling capacity scaling adding and removing servers based on load ebs network attached disks elastic ips assigned ip addresses s3 storage of files route 53 dns and domain registration vpc virtual networking network security and co location you automatically use cloudfront cdn for hosting content cloudwatch alerts paging monitoring managed services existing software solutions you could run on your own but with managed deployment rds managed relational databases managed mysql postgres and amazons own aurora database emr managed hadoop elasticsearch managed elasticsearch elasticache managed redis and memcached optional but important infrastructure these are key and useful infrastructure components that are less widely known and used you may have legitimate reasons to prefer alternatives so evaluate with care to be sure they fit your needs ⛓lambda running small fully managed tasks “serverless” cloudtrail aws api logging and audit often neglected but important ⛓🕍cloudformation templatized configuration of collections of aws resources 🕍elastic beanstalk fully managed paas deployment of packaged java net php node js python ruby go and docker applications 🐥efs network filesystem compatible with nfsv4 1 ⛓🕍ecs docker container cluster management note docker can also be used directly without ecs ⛓ecr hosted private docker registry 🐥config aws configuration inventory history change notifications 🐥x ray trace analysis and debugging for distributed applications such as microservices special purpose infrastructure these services are focused on specific use cases and should be evaluated if they apply to your situation many also are proprietary architectures so tend to tie you to aws ⛓dynamodb low latency nosql key value store ⛓glacier slow and cheap alternative to s3 ⛓kinesis streaming distributed log service ⛓sqs message queueing service ⛓redshift data warehouse 🐥quicksight business intelligence service ses send and receive e mail for marketing or transactions ⛓api gateway proxy manage and secure api calls ⛓iot manage bidirectional communication over http websockets and mqtt between aws and clients often but not necessarily “things” like appliances or sensors ⛓waf web firewall for cloudfront to deflect attacks ⛓kms store and manage encryption keys securely inspector security audit trusted advisor automated tips on reducing cost or making improvements 🐥certificate manager manage ssl tls certificates for aws services 🐥⛓fargate docker containers management backend for ecs compound services these are similarly specific but are full blown services that tackle complex problems and may tie you in usefulness depends on your requirements if you have large or significant need you may have these already managed by in house systems and engineering teams machine learning machine learning model training and classification lex automatic speech recognition asr and natural language understanding nlu polly text to speech engine in the cloud rekognition service for image recognition ⛓🕍data pipeline managed etl service ⛓🕍swf managed state tracker for distributed polyglot job workflow ⛓🕍lumberyard 3d game engine mobile app development sns manage app push notifications and other end user notifications ⛓🕍cognito user authentication via facebook twitter etc device farm cloud based device testing mobile analytics analytics solution for app usage 🕍mobile hub comprehensive managed mobile app framework enterprise services these are relevant if you have significant corporate cloud based or hybrid needs many smaller companies and startups use other solutions like google apps or box larger companies may also have their own non aws it solutions appstream windows apps in the cloud with access from many devices workspaces windows desktop in the cloud with access from many devices workdocs formerly zocalo enterprise document sharing workmail enterprise managed e mail and calendaring service directory service microsoft active directory in the cloud direct connect dedicated network connection between office or data center and aws storage gateway bridge between on premises it and cloud storage service catalog it service approval and compliance probably dont need to know services bottom line our informal polling indicates these services are just not broadly used — and often for good reasons snowball if you want to ship petabytes of data into or out of amazon using a physical appliance read on snowmobile appliances are great but if youve got exabyte scale data to get into amazon nothing beats a tractor trailer full of drives codecommit git service youre probably already using github or your own solution stackshare has informal stats 🕍codepipeline continuous integration you likely have another solution already 🕍codedeploy deployment of code to ec2 servers again you likely have another solution 🕍opsworks management of your deployments using chef or as of november 2017 puppet enterprise while chef is popular it seems few people use opsworks since it involves going in on a whole different code deployment framework aws in plain english offers more friendly explanation of what all the other different services are tools and services market landscape there are now enough cloud and “big data” enterprise companies and products that few can keep up with the market landscape see the big data evolving landscape – 2016 for one attempt at this weve assembled a landscape of a few of the services this is far from complete but tries to emphasize services that are popular with aws practitioners — services that specifically help with aws or a complementary or tools almost anyone using aws must learn 🚧 suggestions to improve this figure please file an issue common concepts 📒 the aws general reference covers a bunch of common concepts that are relevant for multiple services aws allows deployments in regions which are isolated geographic locations that help you reduce latency or offer additional redundancy regions contain availability zones azs which are typically the first tool of choice for high availability azs are physically separate from one another even within the same region and may span multiple physical data centers while they are connected via low latency links natural disasters afflicting one should not affect others each service has api endpoints for each region endpoints differ from service to service and not all services are available in each region as listed in these tables amazon resource names arns are specially formatted identifiers for identifying resources they start with arn and are used in many services and in particular for iam policies service matrix many services within aws can at least be compared with google cloud offerings or with internal google services and often times you could assemble the same thing yourself with open source software this table is an effort at listing these rough correspondences remember that this table is imperfect as in almost every case there are subtle differences of features service aws google cloud google internal microsoft azure other providers open source “build your own” virtual server ec2 compute engine gce virtual machine digitalocean openstack paas elastic beanstalk app engine app engine web apps heroku appfog openshift meteor appscale cloud foundry convox serverless microservices lambda api gateway functions function apps pubnub blocks auth0 webtask kong tyk container cluster manager ecs fargate container engine kubernetes borg or omega container service kubernetes mesos aurora object storage s3 cloud storage gfs storage account digitalocean spaces swift hdfs minio block storage ebs persistent disk storage account digitalocean volumes nfs sql datastore rds cloud sql sql database mysql postgresql sharded rdbms cloud spanner f1 spanner crate io cockroachdb bigtable cloud bigtable bigtable hbase key value store column store dynamodb cloud datastore megastore tables documentdb cassandra couchdb rethinkdb redis memory cache elasticache app engine memcache redis cache memcached redis search cloudsearch elasticsearch managed search algolia qbox elastic cloud elasticsearch solr data warehouse redshift bigquery dremel sql data warehouse oracle ibm sap hp many others greenplum business intelligence quicksight data studio 360 power bi tableau lock manager dynamodb weak chubby lease blobs in storage account zookeeper etcd consul message broker sqs sns iot pub sub pubsub2 service bus rabbitmq kafka 0mq streaming distributed log kinesis dataflow pubsub2 event hubs kafka streams apex flink spark streaming storm mapreduce emr dataproc mapreduce hdinsight datalake analytics qubole hadoop monitoring cloudwatch monitoring borgmon monitor prometheus metric management borgmon tsdb application insights graphite influxdb opentsdb grafana riemann prometheus cdn cloudfront cloud cdn cdn akamai fastly cloudflare limelight networks apache traffic server load balancer clb alb load balancing gfe load balancer application gateway nginx haproxy apache traffic server dns route53 dns dns bind email ses sendgrid mandrill postmark git hosting codecommit cloud source repositories visual studio team services github bitbucket gitlab user authentication cognito firebase authentication azure active directory oauth io mobile app analytics mobile analytics firebase analytics hockeyapp mixpanel mobile app testing device farm firebase test lab xamarin test cloud browserstack sauce labs testdroid managing ssl tls certificates certificate manager lets encrypt comodo symantec globalsign automatic speech recognition and natural language understanding lex cloud speech api natural language api cognitive services aylien text analysis api ambiverse natural language understanding api stanfords core nlp suite apache opennlp apache uima spacy text to speech engine in the cloud polly nuance vocalware ibm mimic espeak marytts image recognition rekognition vision api cognitive services ibm watson clarifai tensorflow opencv file share and sync workdocs google docs onedrive dropbox box citrix file share owncloud machine learning sagemaker deeplens ml ml engine auto ml ml studio watson ml 🚧 please help fill this table in selected resources with more detail on this chart google internal mapreduce bigtable spanner f1 vs spanner bigtable vs megastore aws product maturity and releases its important to know the maturity of each aws product here is a mostly complete list of first release date with links to the release notes most recently released services are first not all services are available in all regions see this table service original release availability cli support hipaa compliant pci dss compliant 🐥x ray 2016 12 general ✓ 🐥lex 2016 11 preview 🐥polly 2016 11 general ✓ 🐥rekognition 2016 11 general ✓ 🐥athena 2016 11 general ✓ 🐥batch 2016 11 general ✓ 🐥database migration service 2016 03 general ✓ ✓ 🐥certificate manager 2016 01 general ✓ 🐥iot 2015 08 general ✓ 🐥waf 2015 10 general ✓ ✓ ✓ 🐥data pipeline 2015 10 general ✓ 🐥elasticsearch 2015 10 general ✓ 🐥aurora 2015 07 general ✓ ✓3 ✓3 🐥service catalog 2015 07 general ✓ 🐥device farm 2015 07 general ✓ 🐥codepipeline 2015 07 general ✓ 🐥codecommit 2015 07 general ✓ 🐥api gateway 2015 07 general ✓ ✓1 ✓ 🐥config 2015 06 general ✓ ✓ 🐥efs 2015 05 general ✓ 🐥machine learning 2015 04 general ✓ lambda 2014 11 general ✓ ✓ ecs 2014 11 general ✓ ✓ ✓ kms 2014 11 general ✓ ✓ codedeploy 2014 11 general ✓ kinesis 2013 12 general ✓ ✓11 cloudtrail 2013 11 general ✓ ✓ appstream 2013 11 preview cloudhsm 2013 03 general ✓ ✓ silk 2013 03 obsolete opsworks 2013 02 general ✓ ✓ redshift 2013 02 general ✓ ✓ ✓ elastic transcoder 2013 01 general ✓ glacier 2012 08 general ✓ ✓ ✓ cloudsearch 2012 04 general ✓ swf 2012 02 general ✓ ✓ storage gateway 2012 01 general ✓ dynamodb 2012 01 general ✓ ✓ ✓ directconnect 2011 08 general ✓ ✓ ✓ elasticache 2011 08 general ✓ cloudformation 2011 04 general ✓ ✓ ses 2011 01 general ✓ elastic beanstalk 2010 12 general ✓ ✓ route 53 2010 10 general ✓ ✓ iam 2010 09 general ✓ ✓ sns 2010 04 general ✓ ✓ emr 2010 04 general ✓ ✓ ✓ rds 2009 12 general ✓ ✓2 ✓9 vpc 2009 08 general ✓ ✓ ✓ snowball 2015 10 general ✓ ✓ snowmobile 2016 11 general cloudwatch 2009 05 general ✓ ✓ cloudfront 2008 11 general ✓ ✓4 ✓ fulfillment web service 2008 03 obsolete simpledb 2007 12 ❗nearly obsolete ✓ ✓ devpay 2007 12 general flexible payments service 2007 08 retired ec2 2006 08 general ✓ ✓5 6 7 ✓6 7 10 sqs 2006 07 general ✓ ✓ ✓ s3 2006 03 general ✓ ✓8 ✓ alexa top sites 2006 01 general ❗http only alexa web information service 2005 10 general ❗http only footnotes 1 excludes use of amazon api gateway caching 2 rds mysql oracle and postgresql engines only 3 mysql compatible aurora edition only 4 excludes lambda edge 5 includes ec2 systems manager 6 includes elastic block storage ebs 7 includes elastic load balancing 8 includes s3 transfer acceleration 9 includes rds mysql oracle postgresql sql server and mariadb 10 includes auto scaling 11 streams only compliance many applications have strict requirements around reliability security or data privacy the aws compliance page has details about awss certifications which include pci dss level 1 soc 1 2 and 3 hipaa and iso 9001 security in the cloud is a complex topic based on a shared responsibility model where some elements of compliance are provided by aws and some are provided by your company several third party vendors offer assistance with compliance security and auditing on aws if you have substantial needs in these areas assistance is a good idea from inside china aws services outside china are generally accessible though there are at times breakages in service there are also aws services inside china getting help and support forums for many problems its worth searching or asking for help in the discussion forums to see if its a known issue premium support aws offers several levels of premium support the first tier called developer support lets you file support tickets with 12 to 24 hour turnaround time it starts at 29 but once your monthly spend reaches around 1000 it changes to a 3 surcharge on your bill the higher level support services are quite expensive — and increase your bill by up to 10 many large and effective companies never pay for this level of support they are usually more helpful for midsize or larger companies needing rapid turnaround on deeper or more perplexing problems keep in mind a flexible architecture can reduce need for support you shouldnt be relying on aws to solve your problems often for example if you can easily re provision a new server it may not be urgent to solve a rare kernel level issue unique to one ec2 instance if your ebs volumes have recent snapshots you may be able to restore a volume before support can rectify the issue with the old volume if your services have an issue in one availability zone you should in any case be able to rely on a redundant zone or migrate services to another zone larger customers also get access to aws enterprise support with dedicated technical account managers tams and shorter response time slas there is definitely some controversy about how useful the paid support is the support staff dont always seem to have the information and authority to solve the problems that are brought to their attention often your ability to have a problem solved may depend on your relationship with your account rep account manager if you are at significant levels of spend thousands of us dollars plus per month you may be assigned or may wish to ask for a dedicated account manager these are a great resource even if youre not paying for premium support build a good relationship with them and make use of them for questions problems and guidance assign a single point of contact on your companys side to avoid confusing or overwhelming them contact the main web contact point for aws is here many technical requests can be made via these channels consulting and managed services for more hands on assistance aws has established relationships with many consulting partners and managed service partners the big consultants wont be cheap but depending on your needs may save you costs long term by helping you set up your architecture more effectively or offering specific expertise e g security managed service providers provide longer term full service management of cloud resources aws professional services aws provides consulting services alone or in combination with partners restrictions and other notes 🔸lots of resources in amazon have limits on them this is actually helpful so you dont incur large costs accidentally you have to request that quotas be increased by opening support tickets some limits are easy to raise and some are not some of these are noted in sections below obtaining current limits and usage limit information for a service may be available from the service api trusted advisor both or neither in which case youll need to contact support this page from the awslimitchecker tools documentation provides a nice summary of available retrieval options for each limit the tool itself is also valuable for automating limit checks 🔸aws terms of service are extensive much is expected boilerplate but it does contain important notes and restrictions on each service in particular there are restrictions against using many aws services in safety critical systems those appreciative of legal humor may wish to review clause 57 10 related topics openstack is a private cloud alternative to aws used by large companies that wish to avoid public cloud offerings learning and career development certifications certifications aws offers certifications for it professionals who want to demonstrate their knowledge certified cloud practitioner certified solutions architect associate certified developer associate certified sysops administrator associate certified solutions architect professional certified devops engineer professional getting certified if youre interested in studying for and getting certifications this practical overview tells you a lot of what you need to know the official page is here and there is an faq do you need a certification especially in consulting companies or when working in key tech roles in large non tech companies certifications are important credentials in others including in many tech companies and startups certifications are not common or considered necessary in fact fairly or not some silicon valley hiring managers and engineers see them as a “negative” signal on a resume managing aws managing infrastructure state and change a great challenge in using aws to build complex systems and with devops in general is to manage infrastructure state effectively over time in general this boils down to three broad goals for the state of your infrastructure visibility do you know the state of your infrastructure what services you are using and exactly how do you also know when you — and anyone on your team — make changes can you detect misconfigurations problems and incidents with your service automation can you reconfigure your infrastructure to reproduce past configurations or scale up existing ones without a lot of extra manual work or requiring knowledge thats only in someones head can you respond to incidents easily or automatically flexibility can you improve your configurations and scale up in new ways without significant effort can you add more complexity using the same tools do you share review and improve your configurations within your team much of what we discuss below is really about how to improve the answers to these questions there are several approaches to deploying infrastructure with aws from the console to complex automation tools to third party services all of which attempt to help achieve visibility automation and flexibility aws configuration management the first way most people experiment with aws is via its web interface the aws console but using the console is a highly manual process and often works against automation or flexibility so if youre not going to manage your aws configurations manually what should you do sadly there are no simple universal answers — each approach has pros and cons and the approaches taken by different companies vary widely and include directly using apis and building tooling on top yourself using command line tools and using third party tools and services aws console the aws console lets you control much but not all functionality of aws via a web interface ideally you should only use the aws console in a few specific situations its great for read only usage if youre trying to understand the state of your system logging in and browsing it is very helpful it is also reasonably workable for very small systems and teams for example one engineer setting up one server that doesnt change often it can be useful for operations youre only going to do rarely like less than once a month for example a one time vpc setup you probably wont revisit for a year in this case using the console can be the simplest approach ❗think before you use the console the aws console is convenient but also the enemy of automation reproducibility and team communication if youre likely to be making the same change multiple times avoid the console favor some sort of automation or at least have a path toward automation as discussed next not only does using the console preclude automation which wastes time later but it prevents documentation clarity and standardization around processes for yourself and your team command line tools the aws command line interface cli used via the aws command is the most basic way to save and automate aws operations dont underestimate its power it also has the advantage of being well maintained — it covers a large proportion of all aws services and is up to date in general whenever you can prefer the command line to the aws console for performing operations 🔹even in the absence of fancier tools you can write simple bash scripts that invoke aws with specific arguments and check these into git this is a primitive but effective way to document operations youve performed it improves automation allows code review and sharing on a team and gives others a starting point for future work 🔹for use that is primarily interactive not scripted consider instead using the aws shell tool from aws it is easier to use with auto completion and a colorful ui but still works on the command line if youre using saws a previous version of the program you should migrate to aws shell apis and sdks sdks for using aws apis are available in most major languages with go ios java javascript python ruby and php being most heavily used aws maintains a short list but the awesome aws list is the most comprehensive and current note support for c is still new retry logic an important aspect to consider whenever using sdks is error handling under heavy use a wide variety of failures from programming errors to throttling to aws related outages or failures can be expected to occur sdks typically implement exponential backoff to address this but this may need to be understood and adjusted over time for some applications for example it is often helpful to alert on some error codes and not on others ❗dont use apis directly although aws documentation includes lots of api details its better to use the sdks for your preferred language to access apis sdks are more mature robust and well maintained than something youd write yourself boto a good way to automate operations in a custom way is boto3 also known as the amazon sdk for python boto2 the previous version of this library has been in wide use for years but now there is a newer version with official support from amazon so prefer boto3 for new projects boto3 contains a variety of apis that operate at either a high level or a low level here some explanation of both the low level apis client apis are mapped to aws cloud service specific apis and all service operations are supported by clients clients are generated from a json service definition file the high level option resource apis allows you to avoid calling the network at the low level and instead provide an object oriented way to interact with aws cloud services boto3 has a lot of helpful features like waiters which provide a structure that allows for code to wait for changes to occur in the cloud for example when you are creating an ec2 instance and need wait until the instance is running in order to perform another task if you find yourself writing a bash script with more than one or two cli commands youre probably doing it wrong stop and consider writing a boto script instead this has the advantages that you can check return codes easily so success of each step depends on success of past steps grab interesting bits of data from responses like instance ids or dns names add useful environment information for example tag your instances with git revisions or inject the latest build identifier into your initialization script general visibility 🔹tagging resources is an essential practice especially as organizations grow to better understand your resource usage for example through automation or convention you can add tags for the org or developer that “owns” that resource for the product that resource supports to label lifecycles such as temporary resources or one that should be deprovisioned in the future to distinguish production critical infrastructure e g serving systems vs backend pipelines to distinguish resources with special security or compliance requirements to once enabled allocate cost note that cost allocation tags only apply on a forward looking basis you cant retroactively apply them to items already billed for many years there was a notorious 10 tag limit per resource which could not be raised and caused many companies significant pain as of 2016 this was raised to 50 tags per resource 🔹in 2017 aws introduced the ability to enforce tagging on instance and volume creation deprecating portions of third party tools such as cloud custodian managing servers and applications aws vs server configuration this guide is about aws not devops or server configuration management in general but before getting into aws in detail its worth noting that in addition to the configuration management for your aws resources there is the long standing problem of configuration management for servers themselves philosophy herokus twelve factor app principles list some established general best practices for deploying applications pets vs cattle treat servers like cattle not pets that is design systems so infrastructure is disposable it should be minimally worrisome if a server is unexpectedly destroyed the concept of immutable infrastructure is an extension of this idea minimize application state on ec2 instances in general instances should be able to be killed or die unexpectedly with minimal impact state that is in your application should quickly move to rds s3 dynamodb efs or other data stores not on that instance ebs is also an option though it generally should not be the bootable volume and ebs will require manual or automated re mounting server configuration management there is a large set of open source tools for managing configuration of server instances these are generally not dependent on any particular cloud infrastructure and work with any variety of linux or in many cases a variety of operating systems leading configuration management tools are puppet chef ansible and saltstack these arent the focus of this guide but we may mention them as they relate to aws containers and aws docker and the containerization trend are changing the way many servers and services are deployed in general containers are designed as a way to package up your application s and all of their dependencies in a known way when you build a container you are including every library or binary your application needs outside of the kernel a big advantage of this approach is that its easy to test and validate a container locally without worrying about some difference between your computer and the servers you deploy on a consequence of this is that you need fewer amis and boot scripts for most deployments the only boot script you need is a template that fetches an exported docker image and runs it companies that are embracing microservice architectures will often turn to container based deployments aws launched ecs as a service to manage clusters via docker in late 2014 though many people still deploy docker directly themselves see the ecs section for more details visibility store and track instance metadata such as instance id availability zone etc and deployment info application build id git revision etc in your logs or reports the instance metadata service can help collect some of the aws data youll need use log management services be sure to set up a way to view and manage logs externally from servers cloud based services such as sumo logic splunk cloud scalyr and loggly are the easiest to set up and use and also the most expensive which may be a factor depending on how much log data you have major open source alternatives include elasticsearch logstash and kibana the “elastic stack” and graylog if you can afford it you have little data or lots of money and dont have special needs it makes sense to use hosted services whenever possible since setting up your own scalable log processing systems is notoriously time consuming track and graph metrics the aws console can show you simple graphs from cloudwatch you typically will want to track and graph many kinds of metrics from cloudwatch and your applications collect and export helpful metrics everywhere you can and as long as volume is manageable enough you can afford it services like librato keenio and datadog have fancier features or better user interfaces that can save a lot of time a more detailed comparison is here use prometheus or graphite as timeseries databases for your metrics both are open source grafana can visualize with dashboards the stored metrics of both timeseries databases also open source tips for managing servers ❗timezone settings on servers unless absolutely necessary always set the timezone on servers to utc see instructions for your distribution such as ubuntu centos or amazon linux numerous distributed systems rely on time for synchronization and coordination and utc provides the universal reference plane it is not subject to daylight savings changes and adjustments in local time it will also save you a lot of headache debugging elusive timezone issues and provide coherent timeline of events in your logging and audit systems ntp and accurate time if you are not using amazon linux which comes preconfigured you should confirm your servers configure ntp correctly to avoid insidious time drift which can then cause all sorts of issues from breaking api calls to misleading logs this should be part of your automatic configuration for every server if time has already drifted substantially generally 1000 seconds remember ntp wont shift it back so you may need to remediate manually for example like this on ubuntu testing immutable infrastructure if you want to be proactive about testing your services ability to cope with instance termination or failure it can be helpful to introduce random instance termination during business hours which will expose any such issues at a time when engineers are available to identify and fix them netflixs simian army specifically chaos monkey is a popular tool for this alternatively chaos lambda by the bbc is a lightweight option which runs on aws lambda security and iam we cover security basics first since configuring user accounts is something you usually have to do early on when setting up your system security and iam basics 📒 iam homepage ∙ user guide ∙ faq the aws security blog is one of the best sources of news and information on aws security iam is the service you use to manage accounts and permissioning for aws managing security and access control with aws is critical so every aws administrator needs to use and understand iam at least at a basic level iam identities include users people or services that are using aws groups containers for sets of users and their permissions and roles containers for permissions assigned to aws service instances permissions for these identities are governed by policies you can use aws pre defined policies or custom policies that you create iam manages various kinds of authentication for both users and for software services that may need to authenticate with aws including passwords to log into the console these are a username and password for real users access keys which you may use with command line tools these are two strings one the “id” which is an upper case alphabetic string of the form axxxxxxxxxxxxxxxxxxx and the other is the secret which is a 40 character mixed case base64 style string these are often set up for services not just users 📜 access keys that start with akia are normal keys access keys that start with asia are session temporary keys from sts and will require an additional sessiontoken parameter to be sent along with the id and secret multi factor authentication mfa which is the highly recommended practice of using a keychain fob or smartphone app as a second layer of protection for user authentication iam allows complex and fine grained control of permissions dividing users into groups assigning permissions to roles and so on there is a policy language that can be used to customize security policies in a fine grained way an excellent high level overview of iam policy concepts lives at iam policies in a nutshell 🔸the policy language has a complex and error prone json syntax thats quite confusing so unless you are an expert it is wise to base yours off trusted examples or aws own pre defined managed policies at the beginning iam policy may be very simple but for large systems it will grow in complexity and need to be managed with care 🔹make sure one person perhaps with a backup in your organization is formally assigned ownership of managing iam policies make sure every administrator works with that person to have changes reviewed this goes a long way to avoiding accidental and serious misconfigurations it is best to give each user or service the minimum privileges needed to perform their duties this is the principle of least privilege one of the foundations of good security organize all iam users and groups according to levels of access they need iam has the permission hierarchy of explicit deny the most restrictive policy wins explicit allow access permissions to any resource has to be explicitly given implicit deny all permissions are implicitly denied by default you can test policy permissions via the aws iam policy simulator tool tool this is particularly useful if you write custom policies security and iam tips 🔹use iam to create individual user accounts and use iam accounts for all users from the beginning this is slightly more work but not that much that way you define different users and groups with different levels of privilege if you want choose from amazons default suggestions of administrator power user etc this allows credential revocation which is critical in some situations if an employee leaves or a key is compromised you can revoke credentials with little effort you can set up active directory federation to use organizational accounts in aws ❗enable mfa on your account you should always use mfa and the sooner the better — enabling it when you already have many users is extra work unfortunately it cant be enforced in software so an administrative policy has to be established most users can use the google authenticator app on ios or android to support two factor authentication for the root account consider a hardware fob ❗restrict use of significant iam credentials as much as possible remember that in the cloud loss of a highly capable iam credential could essentially mean “game over ” for your deployment your users or your whole company do not use the iam root user account other than when you initially create your account create custom iam users and or roles and use those for your applications instead lock up access and use of the root credentials as much as possible ideally they should be effectively “offline ” for critical deployments this means attached to an actual mfa device physically secured and rarely used ❗turn on cloudtrail one of the first things you should do is enable cloudtrail even if you are not a security hawk there is little reason not to do this from the beginning so you have data on what has been happening in your aws account should you need that information youll likely also want to set up a log management service to search and access these logs 🔹use iam roles for ec2 rather than assign iam users to applications like services and then sharing the sensitive credentials define and assign roles to ec2 instances and have applications retrieve credentials from the instance metadata assign iam roles by realm — for example to development staging and production if youre setting up a role it should be tied to a specific realm so you have clean separation this prevents for example a development instance from connecting to a production database best practices aws list of best practices is worth reading in full up front iam reference this interactive reference for all iam actions effects and resources is great to have open while writing new or trying to understand existing iam policies multiple accounts decide on whether you want to use multiple aws accounts and research how to organize access across them factors to consider number of users importance of isolation resource limits permission granularity security api limits regulatory issues workload size of infrastructure cost of multi account “overhead” internal aws service management tools may need to be custom built or adapted 🔹it can help to use separate aws accounts for independent parts of your infrastructure if you expect a high rate of aws api calls since aws throttles calls at the aws account level inspector is an automated security assessment service from aws that helps identify common security risks this allows validation that you adhere to certain security practices and may help with compliance trusted advisor addresses a variety of best practices but also offers some basic security checks around iam usage security group configurations and mfa at paid support tiers trusted advisor exposes additional checks around other areas such as reserved instance optimization use kms for managing keys aws offers kms for securely managing encryption keys which is usually a far better option than handling key security yourself see below aws waf is a web application firewall to help you protect your applications from common attack patterns security auditing security monkey is an open source tool that is designed to assist with security audits scout2 is an open source tool that uses aws apis to assess an environments security posture scout2 is stable and actively maintained 🔹export and audit security settings you can audit security policies simply by exporting settings using aws apis e g using a boto script like secconfig py from this 2013 talk and then reviewing and monitoring changes manually or automatically security and iam gotchas and limitations ❗dont share user credentials its remarkably common for first time aws users to create one account and one set of credentials access key or password and then use them for a while sharing among engineers and others within a company this is easy but dont do this this is an insecure practice for many reasons but in particular if you do you will have reduced ability to revoke credentials on a per user or per service basis for example if an employee leaves or a key is compromised which can lead to serious complications ❗instance metadata throttling the instance metadata service has rate limiting on api calls if you deploy iam roles widely as you should and have lots of services you may hit global account limits easily one solution is to have code or scripts cache and reuse the credentials locally for a short period say 2 minutes for example they can be put into the aws credentials file but must also be refreshed automatically but be careful not to cache credentials for too long as they expire note the other dynamic metadata also changes over time and should not be cached a long time either 🔸some iam operations are slower than other api calls many seconds since aws needs to propagate these globally across regions ❗the uptime of iams api has historically been lower than that of the instance metadata api be wary of incorporating a dependency on iams api into critical paths or subsystems — for example if you validate a users iam group membership when they log into an instance and arent careful about precaching group membership or maintaining a back door you might end up locking users out altogether when the api isnt available ❗dont check in aws credentials or secrets to a git repository there are bots that scan github looking for credentials use scripts or tools such as git secrets to prevent anyone on your team from checking in sensitive information to your git repositories s3 s3 basics 📒 homepage ∙ developer guide ∙ faq ∙ pricing s3 simple storage service is aws standard cloud storage service offering file opaque “blob” storage of arbitrary numbers of files of almost any size from 0 to 5tb prior to 2011 the maximum size was 5 gb larger sizes are now well supported via multipart support items or objects are placed into named buckets stored with names which are usually called keys the main content is the value objects are created deleted or updated large objects can be streamed but you cannot modify parts of a value you need to update the whole object partial data access can work via s3 select every object also has metadata which includes arbitrary key value pairs and is used in a way similar to http headers some metadata is system defined some are significant when serving http content from buckets or cloudfront and you can also define arbitrary metadata for your own use s3 uris although often bucket and key names are provided in apis individually its also common practice to write an s3 location in the form s3 bucket name path to key where the key here is path to key youll also see s3n and s3a prefixes in hadoop systems s3 vs glacier ebs and efs aws offers many storage services and several besides s3 offer file type abstractions glacier is for cheaper and infrequently accessed archival storage ebs unlike s3 allows random access to file contents via a traditional filesystem but can only be attached to one ec2 instance at a time efs is a network filesystem many instances can connect to but at higher cost see the comparison table s3 tips for most practical purposes you can consider s3 capacity unlimited both in total size of files and number of objects the number of objects in a bucket is essentially also unlimited customers routinely have millions of objects ❗permissions 🔸if youre storing business data on amazon s3 its important to manage permissions sensibly in 2017 companies like dow jones and verizon saw data breaches due to poorly chosen s3 configuration for sensitive data fixing this later can be a difficult task if you have a lot of assets and internal users 🔸there are 3 different ways to grant permissions to access amazon s3 content in your buckets iam policies use the familiar identity and authentication management permission scheme to control access to specific operations bucket policies grant or deny permissions to an entire bucket you might use this when hosting a website in s3 to make the bucket publicly readable or to restrict access to a bucket by ip address amazons sample bucket policies show a number of use cases where these policies come in handy access control lists acls can also be applied to every bucket and object stored in s3 acls grant additional permissions beyond those specified in iam or bucket policies acls can be used to grant access to another aws user or to predefined groups like the general public this is powerful but can be dangerous because you need to inspect every object to see who has access 🔸aws predefined access control groups allow access that may not be what youd expect from their names all users or everyone grants permission to the general public not only to users defined in your own aws account if an object is available to all users then it can be retrieved with a simple http request of the form http s3 amazonaws com bucket name filename no authorization or signature is required to access data in this category authenticated users grants permissions to anyone with an aws account again not limited to your own users because anyone can sign up for aws for all intents and purposes this is also open to the general public log delivery group is used by aws to write logs to buckets and should be safe to enable on the buckets that need it a typical use case of this acl is used in conjunction with the requester pays functionality of s3 ❗ bucket permissions and object permissions are two different things and independent of each other a private object in a public bucket can be seen when listing the bucket but not downloaded at the same time a public object in a private bucket wont be seen because the bucket contents cant be listed but can still be downloaded by anyone who knows its exact key users that dont have access to set bucket permissions can still make objects public if they have s3 putobjectacl or s3 putobjectversionacl permissions 🐥in august 2017 aws added aws config rules to ensure your s3 buckets are secure ❗these aws config rules only check the security of your bucket policy and bucket level acls you can still create object acls that grant additional permissions including opening files to the whole world 🔹do create new buckets if you have different types of data with different sensitivity levels this is much less error prone than complex permissions rules for example if data is for administrators only like log data put it in a new bucket that only administrators can access for more guidance see how to secure an amazon s3 bucket deep dive into s3 access controls how do s3 permissions work bucket naming buckets are chosen from a global namespace across all regions even though s3 itself stores data in whichever s3 region you select so youll find many bucket names are already taken creating a bucket means taking ownership of the name until you delete it bucket names have a few restrictions on them bucket names can be used as part of the hostname when accessing the bucket or its contents like bucket name s3 us east 1 amazonaws com as long as the name is dns compliant a common practice is to use the company name acronym or abbreviation to prefix or suffix if you prefer dns style hierarchy all bucket names but please dont use a check on this as a security measure — this is highly insecure and easily circumvented 🔸bucket names with periods in them can cause certificate mismatches when used with ssl use instead since this then conforms with both ssl expectations and is dns compliant versioning s3 has optional versioning support so that all versions of objects are preserved on a bucket this is mostly useful if you want an archive of changes or the ability to back out mistakes caution it lacks the featureset of full version control systems like git durability durability of s3 is extremely high since internally it keeps several replicas if you dont delete it by accident you can count on s3 not losing your data aws offers the seemingly improbable durability rate of 99 999999999 but this is a mathematical calculation based on independent failure rates and levels of replication — not a true probability estimate either way s3 has had a very good record of durability note this is much higher durability than ebs 💸s3 pricing depends on storage requests and transfer for transfer putting data into aws is free but youll pay on the way out transfer from s3 to ec2 in the same region is free transfer to other regions or the internet in general is not free deletes are free s3 reduced redundancy and infrequent access most people use the standard storage class in s3 but there are other storage classes with lower cost 🔸reduced redundancy storage rrs has been effectively deprecated and has lower durability 99 99 so just four nines than standard s3 note that it no longer participates in s3 price reductions so it offers worse redundancy for more money than standard s3 as a result theres no reason to use it infrequent access ia lets you get cheaper storage in exchange for more expensive access this is great for archives like logs you already processed but might want to look at later to get an idea of the cost savings when using infrequent access ia you can use this s3 infrequent access calculator glacier is a third alternative discussed as a separate product see the comparison table ⏱performance maximizing s3 performance means improving overall throughput in terms of bandwidth and number of operations per second s3 is highly scalable so in principle you can get arbitrarily high throughput a good example of this is s3distcp but usually you are constrained by the pipe between the source and s3 and or the level of concurrency of operations throughput is of course highest from within aws to s3 and between ec2 instances and s3 buckets that are in the same region bandwidth from ec2 depends on instance type see the “network performance” column at ec2instances info throughput of many objects is extremely high when data is accessed in a distributed way from many ec2 instances its possible to read or write objects from s3 from hundreds or thousands of instances at once however throughput is very limited when objects accessed sequentially from a single instance individual operations take many milliseconds and bandwidth to and from instances is limited therefore to perform large numbers of operations its necessary to use multiple worker threads and connections on individual instances and for larger jobs multiple ec2 instances as well multi part uploads for large objects you want to take advantage of the multi part uploading capabilities starting with minimum chunk sizes of 5 mb large downloads also you can download chunks of a single large object in parallel by exploiting the http get range header capability 🔸list pagination listing contents happens at 1000 responses per request so for buckets with many millions of objects listings will take time ❗key prefixes in addition latency on operations is highly dependent on prefix similarities among key names if you have need for high volumes of operations it is essential to consider naming schemes with more randomness early in the key name first 6 or 8 characters in order to avoid “hot spots” we list this as a major gotcha since its often painful to do large scale renames 🔸note that sadly the advice about random key names goes against having a consistent layout with common prefixes to manage data lifecycles in an automated way for data outside aws directconnect and s3 transfer acceleration can help for s3 transfer acceleration you pay about the equivalent of 1 2 months of storage for the transfer in either direction for using nearer endpoints command line applications there are a few ways to use s3 from the command line originally s3cmd was the best tool for the job its still used heavily by many the regular aws command line interface now supports s3 well and is useful for most situations s4cmd is a replacement with greater emphasis on performance via multi threading which is helpful for large files and large sets of files and also offers unix like globbing support gui applications you may prefer a gui or wish to support gui access for less technical users some options the aws console does offer a graphical way to use s3 use caution telling non technical people to use it however since without tight permissions it offers access to many other aws features transmit is a good option on macos for most use cases cyberduck is a good option on macos and windows with support for multipart uploads acls versioning lifecycle configuration storage classes and server side encryption sse s3 and sse kms s3 and cloudfront s3 is tightly integrated with the cloudfront cdn see the cloudfront section for more information as well as s3 transfer acceleration static website hosting s3 has a static website hosting option that is simply a setting that enables configurable http index and error pages and http redirect support to public content in s3 its a simple way to host static assets or a fully static website consider using cloudfront in front of most or all assets like any cdn cloudfront improves performance significantly 🔸ssl is only supported on the built in amazonaws com domain for s3 s3 supports serving these sites through a custom domain but not over ssl on a custom domain however cloudfront allows you to serve a custom domain over https amazon provides free sni ssl tls certificates via amazon certificate manager sni does not work on very outdated browsers operating systems alternatively you can provide your own certificate to use on cloudfront to support all browsers operating systems for a fee 🔸if you are including resources across domains such as fonts inside css files you may need to configure cors for the bucket serving those resources since pretty much everything is moving to ssl nowadays and you likely want control over the domain you probably want to set up cloudfront with your own certificate in front of s3 and to ignore the aws example on this as it is non ssl only that said if you do youll need to think through invalidation or updates on cloudfront you may wish to include versions or hashes in filenames so invalidation is not necessary data lifecycles when managing data the understanding the lifecycle of the data is as important as understanding the data itself when putting data into a bucket think about its lifecycle — its end of life not just its beginning 🔹in general data with different expiration policies should be stored under separate prefixes at the top level for example some voluminous logs might need to be deleted automatically monthly while other data is critical and should never be deleted having the former in a separate bucket or at least a separate folder is wise 🔸thinking about this up front will save you pain its very hard to clean up large collections of files created by many engineers with varying lifecycles and no coherent organization alternatively you can set a lifecycle policy to archive old data to glacier be careful with archiving large numbers of small objects to glacier since it may actually cost more there is also a storage class called infrequent access that has the same durability as standard s3 but is discounted per gb it is suitable for objects that are infrequently accessed data consistency understanding data consistency is critical for any use of s3 where there are multiple producers and consumers of data creation and updates to individual objects in s3 are atomic in that youll never upload a new object or change an object and have another client see only part half the change the uncertainty lies with when your clients and other clients see updates new objects if you create a new object youll be able to read it instantly which is called read after write consistency well with the additional caveat that if you do a read on an object before it exists then create it you get eventual consistency not read after write this does not apply to any list operations newly created objects are not guaranteed to appear in a list operation right away updates to objects if you overwrite or delete an object youre only guaranteed eventual consistency i e the change will happen but you have no guarantee of when 🔹for many use cases treating s3 objects as immutable i e deciding by convention they will be created or deleted but not updated can greatly simplify the code that uses them avoiding complex state management 🔹note that until 2015 us standard region had had a weaker eventual consistency model and the other newer regions were read after write this was finally corrected — but watch for many old blogs mentioning this slow updates in practice “eventual consistency” usually means within seconds but expect rare cases of minutes or hours s3 as a filesystem in general s3s apis have inherent limitations that make s3 hard to use directly as a posix style filesystem while still preserving s3s own object format for example appending to a file requires rewriting which cripples performance and atomic rename of directories mutual exclusion on opening files and hardlinks are impossible s3fs is a fuse filesystem that goes ahead and tries anyway but it has performance limitations and surprises for these reasons riofs c and goofys go are more recent efforts that attempt adopt a different data storage format to address those issues and so are likely improvements on s3fs s3ql discussion is a python implementation that offers data de duplication snap shotting and encryption but only one client at a time objectivefs discussion is a commercial solution that supports filesystem features and concurrent clients if you are primarily using a vpc consider setting up a vpc endpoint for s3 in order to allow your vpc hosted resources to easily access it without the need for extra network configuration or hops cross region replication s3 has a feature for replicating a bucket between one region and another note that s3 is already highly replicated within one region so usually this isnt necessary for durability but it could be useful for compliance geographically distributed data storage lower latency or as a strategy to reduce region to region bandwidth costs by mirroring heavily used data in a second region ipv4 vs ipv6 for a long time s3 only supported ipv4 at the default endpoint https bucket s3 amazonaws com however as of aug 11 2016 it now supports both ipv4 ipv6 to use both you have to enable dualstack either in your preferred api client or by directly using this url scheme https bucket s3 dualstack region amazonaws com this extends to s3 transfer acceleration as well s3 event notifications s3 can be configured to send an sns notification sqs message or aws lambda function on bucket events 💸limit your individual users or iam roles to the minimal required s3 locations and catalog the “approved” locations otherwise s3 tends to become the dumping ground where people put data to random locations that are not cleaned up for years costing you big bucks s3 gotchas and limitations ❗s3 buckets sit outside the vpc and can be accessed from anywhere in the world if bucket policies are not set to deny it read the permissions section above carefully there are countless cases of buckets exposed to the public 🔸for many years there was a notorious 100 bucket limit per account which could not be raised and caused many companies significant pain as of 2015 you can request increases you can ask to increase the limit but it will still be capped generally below 1000 per account 🔸be careful not to make implicit assumptions about transactionality or sequencing of updates to objects never assume that if you modify a sequence of objects the clients will see the same modifications in the same sequence or if you upload a whole bunch of files that they will all appear at once to all clients 🔸s3 has an sla with 99 9 uptime if you use s3 heavily youll inevitably see occasional error accessing or storing data as disks or other infrastructure fail availability is usually restored in seconds or minutes although availability is not extremely high as mentioned above durability is excellent 🔸after uploading any change that you make to the object causes a full rewrite of the object so avoid appending like behavior with regular files 🔸eventual data consistency as discussed above can be surprising sometimes if s3 suffers from internal replication issues an object may be visible from a subset of the machines depending on which s3 endpoint they hit those usually resolve within seconds however weve seen isolated cases when the issue lingered for 20 30 hours 🔸md5s and multi part uploads in s3 the etag header in s3 is a hash on the object and in many cases it is the md5 hash however this is not the case in general when you use multi part uploads one workaround is to compute md5s yourself and put them in a custom header such as is done by s4cmd 🔸incomplete multi part upload costs incomplete multi part uploads accrue storage charges even if the upload fails and no s3 object is created amazon and others recommend using a lifecycle policy to clean up incomplete uploads and save on storage costs note that if you have many of these it may be worth investigating whatevers failing regularly 🔸us standard region previously the us east 1 region also known as the us standard region was replicated across coasts which led to greater variability of latency effective jun 19 2015 this is no longer the case all amazon s3 regions now support read after write consistency amazon s3 also renamed the us standard region to the us east n virginia region to be consistent with aws regional naming conventions 🔸s3 authentication versions and regions in newer regions s3 only supports the latest authentication if an s3 file operation using cli or sdk doesnt work in one region but works correctly in another region make sure you are using the latest authentication signature storage durability availability and price as an illustration of comparative features and price the table below gives s3 standard rrs ia in comparison with glacier ebs efs and ec2 d2 xlarge instance store using virginia region as of sept 2017 durability per year availability “designed” availability sla storage per tb per month get or retrieve per million write or archive per million glacier eleven 9s sloooow – 4 50 50 s3 ia eleven 9s 99 9 99 12 50 1 10 s3 rrs 99 99 99 99 99 9 24 first tb 0 40 5 s3 standard eleven 9s 99 99 99 9 23 0 40 5 ebs 99 8 unstated 99 99 25 45 100 125 sc1 st1 gp2 io1 efs “high” “high” – 300 ec2 d2 xlarge instance store unstated unstated – 25 44 0 0 especially notable items are in boldface sources s3 pricing s3 sla s3 faq rrs info note that this is considered deprecated glacier pricing ebs availability and durability ebs pricing efs pricing ec2 sla ec2 ec2 basics 📒 homepage ∙ documentation ∙ faq ∙ pricing see also ec2instances info ec2 elastic compute cloud is aws offering of the most fundamental piece of cloud computing a virtual private server these “instances” can run most linux bsd and windows operating systems internally theyve used a heavily modified xen virtualization that said new instance classes are being introduced with a kvm derived hypervisor instead called nitro so far this is limited to the c5 and m5 instance types lastly theres a bare metal hypervisor available for i3 metal instances in a limited preview the term “ec2” is sometimes used to refer to the servers themselves but technically refers more broadly to a whole collection of supporting services too like load balancing clbs albs nlbs ip addresses eips bootable images amis security groups and network drives ebs which we discuss individually in this guide 💸ec2 pricing and cost management is a complicated topic it can range from free on the aws free tier to a lot depending on your usage pricing is by instance type by second or hour and changes depending on aws region and whether you are purchasing your instances on demand on the spot market or pre purchasing reserved instances network performance for some instance types aws uses general terms like low medium and high to refer to network performance users have done benchmarking to provide expectations for what these terms can mean ec2 alternatives and lock in running ec2 is akin to running a set of physical servers as long as you dont do automatic scaling or tooled cluster setup if you just run a set of static instances migrating to another vps or dedicated server provider should not be too hard 🚪alternatives to ec2 the direct alternatives are google cloud microsoft azure rackspace digitalocean awss own lightsail offering and other vps providers some of which offer similar apis for setting up and removing instances see the comparisons above should you use amazon linux aws encourages use of their own amazon linux which is evolved from red hat enterprise linux rhel and centos its used by many but others are skeptical whatever you do think this decision through carefully its true amazon linux is heavily tested and better supported in the unlikely event you have deeper issues with os and virtualization on ec2 but in general many companies do just fine using a standard non amazon linux distribution such as ubuntu or centos using a standard linux distribution means you have an exactly replicable environment should you use another hosting provider instead of or in addition to aws its also helpful if you wish to test deployments on local developer machines running the same standard linux distribution a practice thats getting more common with docker too amazon now supports an official amazon linux docker image aimed at assisting with local development on a comparable environment though this is new enough that it should be considered experimental note that the currently in testing amazon linux 2 supports on premise deployments explicitly ec2 costs see the section on this ec2 tips 🔹picking regions when you first set up consider which regions you want to use first many people in north america just automatically set up in the us east 1 n virginia region which is the default but its worth considering if this is best up front youll want to evaluate service availability some services are not available in all regions costing baseline costs also vary by region by up to 10 30 generally lowest in us east 1 for comparison purposes and compliance various countries have differing regulations with regard to data privacy for example instance types ec2 instances come in many types corresponding to the capabilities of the virtual machine in cpu architecture and speed ram disk sizes and types ssd or magnetic and network bandwidth selecting instance types is complex since there are so many types additionally there are different generations released over the years 🔹use the list at ec2instances info to review costs and features amazons own list of instance types is hard to use and doesnt list features and price together which makes it doubly difficult prices vary a lot so use ec2instances info to determine the set of machines that meet your needs and ec2price com to find the cheapest type in the region youre working in depending on the timing and region it might be much cheaper to rent an instance with more memory or cpu than the bare minimum turn off your instances when they arent in use for many situations such as testing or staging resources you may not need your instances on 24 7 and you wont need to pay ec2 running costs when they are suspended given that costs are calculated based on usage this is a simple mechanism for cost savings this can be achieved using lambda and cloudwatch an open source option like cloudcycler or a saas provider like gorillastack note if you turn off instances with an ephemeral root volume any state will be lost when the instance is turned off therefore for stateful applications it is safer to turn off ebs backed instances dedicated instances and dedicated hosts are assigned hardware instead of usual virtual instances they are more expensive than virtual instances but can be preferable for performance compliance financial modeling or licensing reasons 32 bit vs 64 bit a few micro small and medium instances are still available to use as 32 bit architecture youll be using 64 bit ec2 “amd64” instances nowadays though smaller instances still support 32 bit “i386” use 64 bit unless you have legacy constraints or other good reasons to use 32 hvm vs pv there are two kinds of virtualization technology used by ec2 hardware virtual machine hvm and paravirtual pv historically pv was the usual type but now hvm is becoming the standard if you want to use the newest instance types you must use hvm see the instance type matrix for details operating system to use ec2 youll need to pick a base operating system it can be windows or linux such as ubuntu or amazon linux you do this with amis which are covered in more detail in their own section below limits you cant create arbitrary numbers of instances default limits on numbers of ec2 instances per account vary by instance type as described in this list ❗use termination protection for any instances that are important and long lived in particular arent part of auto scaling enable termination protection this is an important line of defense against user mistakes such as accidentally terminating many instances instead of just one due to human error ssh key management when you start an instance you need to have at least one ssh key pair set up to bootstrap i e allow you to ssh in the first time aside from bootstrapping you should manage keys yourself on the instances assigning individual keys to individual users or services as appropriate avoid reusing the original boot keys except by administrators when creating new instances avoid sharing keys and add individual ssh keys for individual users gpu support you can rent gpu enabled instances on ec2 for use in machine learning or graphics rendering workloads there are three generations of gpu enabled instances available third generation p2 series offers nvidia k80 gpus in 1 8 and 16 gpu configurations targeting machine learning and scientific workloads second generation g2 series offers nvidia k520 gpus in 1 or 4 gpu configurations targeting graphics and video encoding first generation cg1 instances are still available in some regions in a single configuration with a nvidia m2050 gpu ⛓ aws offers an ami based on amazon linux with most nvidia drivers and ancillary software cuda cublas cudnn tensorflow installed to lower the barrier to usage note however that this leads to lock in due to amazon linux and the fact that you have no direct access to software configuration or versioning 🔹as with any expensive ec2 instance types spot instances can offer significant savings with gpu workloads when interruptions are tolerable all current ec2 instance types can take advantage of ipv6 addressing so long as they are launched in a subnet with an allocated cidr range in an ipv6 enabled vpc ec2 gotchas and limitations ❗never use ssh passwords just dont do it they are too insecure and consequences of compromise too severe use keys instead read up on this and fully disable ssh password access to your ssh server by making sure passwordauthentication no is in your etc ssh sshd config file if youre careful about managing ssh private keys everywhere they are stored it is a major improvement on security over password based authentication 🔸for all newer instance types when selecting the ami to use be sure you select the hvm ami or it just wont work ❗when creating an instance and using a new ssh key pair make sure the ssh key permissions are correct 🔸sometimes certain ec2 instances can get scheduled for retirement by aws due to “detected degradation of the underlying hardware ” in which case you are given a couple of weeks to migrate to a new instance if your instance root device is an ebs volume you can typically stop and then start the instance which moves it to healthy host hardware giving you control over timing of this event note however that you will lose any instance store volume data ephemeral drives if your instance type has instance store volumes the instance public ip if it has one will likely change unless youre using elastic ips this could be a problem if other systems depend on the ip address 🔸periodically you may find that your server or load balancer is receiving traffic for presumably a previous ec2 server that was running at the same ip address that you are handed out now this may not matter or it can be fixed by migrating to another new instance ❗if the ec2 api itself is a critical dependency of your infrastructure e g for automated server replacement custom scaling algorithms etc and you are running at a large scale or making many ec2 api calls make sure that you understand when they might fail calls to it are rate limited and the limits are not published and subject to change and code and test against that possibility ❗many newer ec2 instance types are ebs only make sure to factor in ebs performance and costs when planning to use them ❗if youre operating at significant scale you may wish to break apart api calls that enumerate all of your resources and instead operate either on individual resources or a subset of the entire list ec2 apis will time out consider using filters to restrict what gets returned ❗⏱ instances come in two types fixed performance instances e g m3 c3 and r3 and burstable performance instances e g t2 a t2 instance receives cpu credits continuously the rate of which depends on the instance size t2 instances accrue cpu credits when they are idle and use cpu credits when they are active however once an instance runs out of credits youll notice a severe degradation in performance if you need consistently high cpu performance for applications such as video encoding high volume websites or hpc applications it is recommended to use fixed performance instances instance user data is limited to 16 kb this limit applies to the data in raw form not base64 encoded form if more data is needed it can be downloaded from s3 by a user data script very new accounts may not be able to launch some instance types such as gpu instances because of an initially imposed “soft limit” of zero this limit can be raised by making a support request see aws service limits for the method to make the support request note that this limit of zero is not currently documented cloudwatch cloudwatch basics 📒 homepage ∙ documentation ∙ faq ∙ pricing cloudwatch monitors resources and applications captures logs and sends events cloudwatch monitoring is the standard mechanism for keeping tabs on aws resources a wide range of metrics and dimensions are available via cloudwatch allowing you to create time based graphs alarms and dashboards alarms are the most practical use of cloudwatch allowing you to trigger notifications from any given metric alarms can trigger sns notifications auto scaling actions or ec2 actions alarms also support alerting when any m out of n datapoints cross the alarm threshold publish and share graphs of metrics by creating customizable dashboard views monitor and report on ec2 instance system check failure alarms using cloudwatch events events create a mechanism to automate actions in various services on aws you can create event rules from instance states aws apis auto scaling run commands deployments or time based schedules think cron triggered events can invoke lambda functions send sns sqs kinesis messages or perform instance actions terminate restart stop or snapshot volumes custom payloads can be sent to targets in json format this is especially useful when triggering lambdas using cloudwatch logs cloudwatch logs is a streaming log storage system by storing logs within aws you have access to unlimited paid storage but you also have the option of streaming logs directly to elasticsearch or custom lambdas a log agent installed on your servers will process logs over time and send them to cloudwatch logs you can export logged data to s3 or stream results to other aws services cloudwatch logs can be encrypted using keys managed through kms detailed monitoring detailed monitoring for ec2 instances must be enabled to get granular metrics and is billed under cloudwatch cloudwatch alternatives and lock in cloudwatch offers fairly basic functionality that doesnt create significant additional aws lock in most of the metrics provided by the service can be obtained through apis that can be imported into other aggregation or visualization tools or services many specifically provide cloudwatch data import services 🚪 alternatives to cloudwatch monitoring services include newrelic datadog sumo logic zabbix nagios ruxit elastic stack open source options such as statsd or collectd with graphite and many others 🚪 cloudwatch log alternatives include splunk sumo logic loggly logstash papertrail elastic stack and other centralized logging solutions cloudwatch tips some very common use cases for cloudwatch are billing alarms instance or load balancer up down alarms and disk usage alerts you can use ec2config to monitor watch memory and disk metrics on windows platform instances for linux there are example scripts that do the same thing you can publish your own metrics using the aws api incurs additional cost you can stream directly from cloudwatch logs to a lambda or elasticsearch cluster by creating subscriptions on log groups dont forget to take advantage of the cloudwatch non expiring free tier cloudwatch gotchas and limitations 🔸metrics in cloudwatch originate on the hypervisor the hypervisor doesnt have access to os information so certain metrics most notably memory utilization are not available unless pushed to cloudwatch from inside the instance 🔸you can not use more than one metric for an alarm 🔸notifications you receive from alarms will not have any contextual detail they have only the specifics of the threshold alarm state and timing 🔸by default cloudwatch metric resolution is 1 minute if you send multiple values of a metric within the same minute they will be aggregated into minimum maximum average and total sum per minute 🐥in july 2017 a new high resolution option was added for cloudwatch metrics and alarms this feature allows you to record metrics with 1 second resolution and to evaluate cloudwatch alarms every 10 seconds the blog post introducing this feature describes how to publish a high resolution metric to cloudwatch note that when calling the putmetricdata api storageresolution is an attribute of each item you send in the metricdata array not a direct parameter of the putmetricdata api call 🔸data about metrics is kept in cloudwatch for 15 months starting november 2016 used to be 14 days minimum granularity increases after 15 days amis ami basics 📒 user guide amis amazon machine images are immutable images that are used to launch preconfigured ec2 instances they come in both public and private flavors access to public amis is either freely available shared community amis or bought and sold in the aws marketplace many operating system vendors publish ready to use base amis for ubuntu see the ubuntu ami finder amazon of course has amis for amazon linux ami tips amis are built independently based on how they will be deployed you must select amis that match your deployment when using them or creating them ebs or instance store pv or hvm virtualization types 32 bit “i386” vs 64 bit “amd64” architecture as discussed above modern deployments will usually be with 64 bit ebs backed hvm you can create your own custom ami by snapshotting the state of an ec2 instance that you have modified amis backed by ebs storage have the necessary image data loaded into the ebs volume itself and dont require an extra pull from s3 which results in ebs backed instances coming up much faster than instance storage backed ones amis are per region so you must look up amis in your region or copy your amis between regions with the ami copy feature as with other aws resources its wise to use tags to version amis and manage their lifecycle if you create your own amis there is always some tension in choosing how much installation and configuration you want to “bake” into them baking less into your amis for example just a configuration management client that downloads installs and configures software on new ec2 instances when they are launched allows you to minimize time spent automating ami creation and managing the ami lifecycle you will likely be able to use fewer amis and will probably not need to update them as frequently but results in longer waits before new instances are ready for use and results in a higher chance of launch time installation or configuration failures baking more into your amis for example pre installing but not fully configuring common software along with a configuration management client that loads configuration settings at launch time results in a faster launch time and fewer opportunities for your software installation and configuration to break at instance launch time but increases the need for you to create and manage a robust ami creation pipeline baking even more into your amis for example installing all required software as well and potentially also environment specific configuration information results in fast launch times and a much lower chance of instance launch time failures but without additional re deployment and re configuration considerations can require time consuming ami updates in order to update software or configuration as well as more complex ami creation automation processes which option you favor depends on how quickly you need to scale up capacity and size and maturity of your team and product when instances boot fast auto scaled services require less spare capacity built in and can more quickly scale up in response to sudden increases in load when setting up a service with autoscaling consider baking more into your amis and backing them with the ebs storage option as systems become larger it common to have more complex ami management such as a multi stage ami creation process in which few ideally one common base amis are infrequently regenerated when components that are common to all deployed services are updated and then a more frequently run “service level” ami generation process that includes installation and possibly configuration of application specific software more thinking on ami creation strategies here use tools like packer to simplify and automate ami creation if you use rhel instances and happen to have existing rhel on premise red hat subscriptions then you could leverage red hats cloud access program to migrate a portion of your subscriptions to aws and thereby not having aws charge you for rhel subscriptions a second time you can either use your own self created rhel amis or red hat provided gold images that will be added to your private amis once you sign up for red hat cloud access ami gotchas and limitations 🔸amazon linux package versions by default instances based on amazon linux amis are configured point to the latest versions of packages in amazons package repository this means that the package versions that get installed are not locked and it is possible for changes including breaking ones to appear when applying updates in the future if you bake your amis with updates already applied this is unlikely to cause problems in running services whose instances are based on those amis – breaks will appear at the earlier ami baking stage of your build process and will need to be fixed or worked around before new amis can be generated there is a “lock on launch” feature that allows you to configure amazon linux instances to target the repository of a particular major version of the amazon linux ami reducing the likelihood that breaks caused by amazon initiated package version changes will occur at package install time but at the cost of not having updated packages get automatically installed by future update runs pairing use of the “lock on launch” feature with a process to advance the amazon linux ami at your discretion can give you tighter control over update behaviors and timings cloud init defaults oftentimes users create amis after performing customizations albeit manually or via some tool such as packer or ansible if youre not careful to alter cloud init settings that correspond to the system service e g sshd etc youve customized you may find that your changes are no longer in effect after booting your new ami for the first time as cloud init has overwritten them some distros have different files than others but all are generally located in etc cloud regardless of distro you will want to review these files carefully for your chosen distro before rolling your own amis a complete reference to cloud init is available on the cloud init site this is an advanced configuration mechanism so test any changes made to these files in a sandbox prior to any serious usage auto scaling auto scaling basics 📒 homepage ∙ user guide ∙ faq ∙ pricing at no additional charge auto scaling groups asgs are used to control the number of instances in a service reducing manual effort to provision or deprovision ec2 instances they can be configured through scaling policies to automatically increase or decrease instance counts based on metrics like cpu utilization or based on a schedule there are three common ways of using asgs dynamic automatically adjust instance count based on metrics for things like cpu utilization static maintain a specific instance count at all times scheduled maintain different instance counts at different times of day or on days of the week 💸asgs have no additional charge themselves you pay for underlying ec2 and cloudwatch services auto scaling tips 💸 better matching your cluster size to your current resource requirements through use of asgs can result in significant cost savings for many types of workloads pairing asgs with clbs is a common pattern used to deal with changes in the amount of traffic a service receives dynamic auto scaling is easiest to use with stateless horizontally scalable services even if you are not using asgs to dynamically increase or decrease instance counts you should seriously consider maintaining all instances inside of asgs – given a target instance count the asg will work to ensure that number of instances running is equal to that target replacing instances for you if they die or are marked as being unhealthy this results in consistent capacity and better stability for your service autoscalers can be configured to terminate instances that a clb or alb has marked as being unhealthy auto scaling gotchas and limitations 🔸replaceunhealthy setting by default asgs will kill instances that the ec2 instance manager considers to be unresponsive it is possible for instances whose cpu is completely saturated for minutes at a time to appear to be unresponsive causing an asg with the default replaceunhealthy setting turned on to replace them when instances that are managed by asgs are expected to consistently run with very high cpu consider deactivating this setting if you do so however detecting and killing unhealthy nodes will become your responsibility ebs ebs basics 📒 homepage ∙ user guide ∙ faq ∙ pricing ebs elastic block store provides block level storage that is it offers storage volumes that can be attached as filesystems like traditional network drives ebs volumes can only be attached to one ec2 instance at a time in contrast efs can be shared but has a much higher price point a comparison ebs tips ⏱raid use raid drives for increased performance ⏱a worthy read is aws post on ebs io characteristics as well as their performance tips ⏱one can provision iops that is pay for a specific level of i o operations per second to ensure a particular level of performance for a disk ⏱a single ebs volume allows 10k iops max to get the maximum performance out of an ebs volume it has to be of a maximum size and attached to an ebs optimized ec2 instance 💸standard ebs volumes improve iops with size it may make sense for you to simply enlarge a volume instead of paying for better performance explicitly this can in many cases reduce costs by 2 3 a standard block size for an ebs volume is 16kb ebs gotchas and limitations ❗ebs durability is reasonably good for a regular hardware drive annual failure rate of between 0 1 0 2 on the other hand that is very poor if you dont have backups by contrast s3 durability is extremely high if you care about your data back it up to s3 with snapshots 🔸ebs has an sla with 99 99 uptime see notes on high availability below ❗ebs volumes have a volume type indicating the physical storage type the types called “standard” st1 or sc1 are actually old spinning platter disks which deliver only hundreds of iops — not what you want unless youre really trying to cut costs modern ssd based gp2 or io1 are typically the options you want ❗when restoring a snapshot to create an ebs volume blocks are lazily read from s3 the first time theyre referenced to avoid an initial period of high latency you may wish to use dd or fio as per the official documentation efs efs basics 📒 homepage ∙ user guide ∙ faq ∙ pricing 🐥efs is amazons network filesystem its presented as an nfsv4 1 server any compatible nfsv4 client can mount it it is designed to be highly available and durable and each efs file system object is redundantly stored across multiple availability zones efs is designed to be used as a shared network drive and it can automatically scale up to petabytes of stored data and thousands of instances attached to it efs can offer higher throughput multiple gigabytes per second and better durability and availability than ebs see the comparison table but with higher latency efs is priced based on the volume of data stored and costs much more than ebs its in the ballpark of three times as much compared to general purpose gp2 ebs volumes ⏱ performance is dependent on the volume of data stored as is the price like ebs efs uses a credit based system credits are earned at a rate of 50 kib s per gib of storage and consumed in bursts during reading writing files or metadata unlike ebs operations on metadata file size owner date etc also consume credits the burstcreditbalance metric in cloudwatch should be monitored to make sure the file system doesnt run out of credits throughput capacity during bursts is also dependent on size under 1 tib throughput can go up to 100 mib s above that 100 mib s is added for each stored tib for instance a file system storing 5 tib would be able to burst at a rate of 500 mib s maximum throughput per ec2 instance is 250 mib s efs has two performance modes that can only be set when a file system is created one is general purpose the other is max i o max i o scales higher but at the cost of higher latency when in doubt use general purpose which is also the default if the percentiolimit metric in cloudwatch hovers around 100 max i o is recommended changing performance mode means creating a new efs and migrating data high availability is achieved by having mount targets in different subnets availability zones efs tips with efs being based on nfsv4 1 any directory on the efs can be mounted directly it doesnt have to be the root directory one application could mount fs 12345678 prog1 another fs 12345678 prog2 user and group level permissions can be used to control access to certain directories on the efs file system ⏱ sharing efs filesystems one efs filesystem can be used for multiple applications or services but it should be considered carefully pros because performance is based on total size of stored files having everything on one drive will increase performance for everyone one application consuming credits faster than it can accumulate might be offset by another application that just stores files on efs and rarely accesses them cons since credits are shared if one application over consumes them it will affect the others a compromise is made with regards to security all clients will have to have network access to the drive someone with root access on one client instance can mount any directory on the efs and they have read write access to all files on the drive even if they dont have access to the applications hosted on other clients there isnt a no root squash equivalent for efs efs gotchas and limitations 🔸 a number of nfsv4 1 features are not supported and there are some limits to the service 🔸 as of 2017 08 efs offers disk level encryption for new drives for file systems created before that date encryption can only be achieved by moving the data to a new efs volume 🔸 an efs file system can be mounted on premises over direct connect 🔸 an efs file system can not be mounted over vpc peering or vpn even if the vpn is running on top of direct connect 🔸 using an efs volume on windows is not supported ⏱ when a file is uploaded to efs it can take hours for efs to update the details for billing and burst credit purposes 🔸⏱ metadata operations can be costly in terms of burst credit consumption recursively traversing a tree containing thousands of files can easily ramp up to tens or even hundreds of megabytes of burst credits being consumed even if no file is being touched commands like find or chown r can have an adverse impact on performance load balancers load balancer basics aws has 3 load balancing products “classic load balancers” clbs “application load balancers” albs and network load balancers nlb before the introduction of albs “classic load balancers” were known as “elastic load balancers” elbs so older documentation tooling and blog posts may still reference “elbs” clbs have been around since 2009 albs in 2016 nlbs were added in 2017 to aws clbs support tcp and http load balancing albs support http load balancing only nlbs support tcp layer 4 load balancing clbs and albs can optionally handle termination for a single ssl certificate all can optionally perform active health checks of instances and remove them from the destination pool if they become unhealthy clbs dont support complex rule based routing albs support a currently small set of rule based routing features nlbs have most extensive routing options clbs can only forward traffic to a single globally configured port on destination instances while albs can forward to ports that are configured on a per instance basis better supporting routing to services on shared clusters with dynamic port assignment like ecs or mesos nlbs support multiple ports on same ip registering targets by ip address including targets outside the vpc for the load balancer ecs can select unused port for scheduling a task then register a target group using this port clbs are supported in ec2 classic as well as in vpcs while albs are supported in vpcs only albs can target groups of instances and ip based targets in the rfc1918 ranges allowing you to use on premise destinations via vpn or direct connect load balancer tips if you dont have opinions on your load balancing up front and dont have complex load balancing needs like application specific routing of requests its reasonable just to use an clb or alb for load balancing instead even if you dont want to think about load balancing at all because your architecture is so simple say just one server put a load balancer in front of it anyway this gives you more flexibility when upgrading since you wont have to change any dns settings that will be slow to propagate and also it lets you do a few things like terminate ssl more easily clbs and albs have many ips internally an aws load balancer is simply a collection of individual software load balancers hosted within ec2 with dns load balancing traffic among them the pool can contain many ips at least one per availability zone and depending on traffic levels they also support ssl termination which is very convenient scaling clbs and albs can scale to very high throughput but scaling up is not instantaneous if youre expecting to be hit with a lot of traffic suddenly it can make sense to load test them so they scale up in advance you can also contact amazon and have them “pre warm” the load balancer client ips in general if servers want to know true client ip addresses load balancers must forward this information somehow clbs add the standard x forwarded for header when using an clb as an http load balancer its possible to get the clients ip address from this using load balancers when deploying one common pattern is to swap instances in the load balancer after spinning up a new stack with your latest version keep old stack running for one or two hours and either flip back to old stack in case of problems or tear it down rotating certificates while retaining arn rotating iam server certificates can be difficult as the standard practice is to upload a new one then update all resources with the new arn you can however retain the same arn using the update certificate call with the following process upload a new iam server certificate with a unique name e g fuzzy com new rename the existing iam server certificate e g fuzzy com to fuzzy com expired rename the new iam server certificate to the name of the previously existing certificate e g fuzzy com new to fuzzy com jiggle the clb alb listener to pick up the change alb invoke modify listener with the existing details for the alb listener clb invoke create load balancer listeners with the existing details for the clb listener load balancer gotchas and limitations ❗clbs and albs have no fixed external ip that all clients see for most consumer apps this doesnt matter but enterprise customers of yours may want this ips will be different for each user and will vary unpredictably for a single client over time within the standard ec2 ip ranges and similarly never resolve an clb name to an ip and put it as the value of an a record — it will work for a while then break ❗some web clients or reverse proxies cache dns lookups for a long time which is problematic for clbs and albs since they change their ips this means after a few minutes hours or days your client will stop working unless you disable dns caching watch out for javas settings and be sure to adjust them properly another example is nginx as a reverse proxy which normally resolves backends only at start up although there is a way to get around this ❗its not unheard of for ips to be recycled between customers without a long cool off period so as a client if you cache an ip and are not using ssl to verify the server you might get not just errors but responses from completely different services or companies 🔸as an operator of a service behind an clb or alb the latter phenomenon means you can also see puzzling or erroneous requests by clients of other companies this is most common with clients using back end apis since web browsers typically cache for a limited period ❗clbs and albs take time to scale up it does not handle sudden spikes in traffic well therefore if you anticipate a spike you need to “pre warm” the load balancer by gradually sending an increasing amount of traffic ❗tune your healthchecks carefully — if you are too aggressive about deciding when to remove an instance and conservative about adding it back into the pool the service that your load balancer is fronting may become inaccessible for seconds or minutes at a time be extra careful about this when an autoscaler is configured to terminate instances that are marked as being unhealthy by a managed load balancer ❗clb https listeners dont support server name indication sni if you need sni you can work around this limitation by either providing a certificate with subject alternative names sans or by using tcp listeners and terminating ssl at your backend 🔸 there is a limit on the number of albs clbs and nlbs per region separately as of late 2017 the default limit for each is 20 per region these limits can be easily raised for alb and clb but aws is quite reluctant to raise the limit on nlbs 🔸 if using a network load balancer nlb then ec2 clients cannot connect to an nlb that resides in another vpc vpc peering or aws managed vpn unless the ec2 client is a c5 i3 metal or m5 instance type for vpc peering both vpcs must be in the same region https docs aws amazon com elasticloadbalancing latest network network load balancers html clb clb basics 📒 homepage ∙ user guide ∙ faq ∙ pricing classic load balancers formerly known as elastic load balancers are http and tcp load balancers that are managed and scaled for you by amazon clb tips best practices this article is a must read if you use clbs heavily and has a lot more detail clb gotchas and limitations in general clbs are not as “smart” as some load balancers and dont have fancy features or fine grained control a traditional hardware load balancer would offer for most common cases involving sessionless apps or cookie based sessions over http or ssl termination they work well 🔸by default clbs will refuse to route traffic from a load balancer in one availability zone az to a backend instance in another this will cause 503s if the last instance in an az becomes unavailable even if there are healthy instances in other zones if youre running fewer than two backend instances per az you almost certainly want to enable cross zone load balancing 🔸complex rules for directing traffic are not supported for example you cant direct traffic based on a regular expression in the url like haproxy offers apex dns names once upon a time you couldnt assign an clb to an apex dns record i e example com instead of foo example com because it needed to be an a record instead of a cname this is now possible with a route 53 alias record directly pointing to the load balancer 🔸clbs use http keep alives on the internal side this can cause an unexpected side effect requests from different clients each in their own tcp connection on the external side can end up on the same tcp connection on the internal side never assume that multiple requests on the same tcp connection are from the same client 🔸 traffic between clbs and back end instances in the same subnet will have network acl rules evaluated ec2 to ec2 traffic in the same subnet would not have network acl rules evaluated if the default 0 0 0 0 0 allow rule is removed from the network acl applied to the subnet a rule that allows traffic on both the health check port and any listener port must be added as of december 2016 clbs launched in vpcs do not support ipv6 addressing clbs launched in ec2 classic support both ipv4 and ipv6 with the dualstack dns name alb alb basics 📒 homepage ∙ user guide ∙ faq ∙ pricing 🐥websockets and http 2 are now supported 🐥internet protocol version 6 ipv6 is now supported 🐥load balancing via ip is now supported prior to the application load balancer you were advised to use tcp instead of http as the protocol to make it work as described here and use the obscure but useful proxy protocol more on this to pass client ips over a tcp load balancer alb tips use albs to route to services that are hosted on shared clusters with dynamic port assignment like ecs or mesos albs support http host based routing send http requests for “api mydomain com” target group 1 “blog mydomain com” target group 2 as well as http path based routing send http requests for “ api ast ” target group 1 “ blog ast ” target group 2 alb gotchas and limitations 🔸albs only support http 2 over https no plain text http 2 🔸albs only support http 2 to external clients and not to internal resources instances containers albs support http routing but not port based tcp routing instances in the albs target groups have to either have a single fixed healthcheck port “ec2 instance” level healthcheck or the healthcheck port for a target has to be the same as its application port “application instance” level healthcheck you cant configure a per target healthcheck port that is different than the application port albs are vpc only they are not available in ec2 classic in a target group if there is no healthy target all requests are routed to all targets for example if you point a listener at a target group containing a single service that has a long initialization phase during which the health checks would fail requests will reach the service while it is still starting up 📜 although albs now support sni they only support 25 https certificates per load balancer this limitation is not described here so it might be subject to change elastic ips elastic ip basics 📒 documentation ∙ faq ∙ pricing elastic ips are static ip addresses you can rent from aws to assign to ec2 instances elastic ip tips 🔹prefer load balancers to elastic ips for single instance deployments you could just assign elastic ip to an instance give that ip a dns name and consider that your deployment most of the time you should provision a load balancer instead its easy to add and remove instances from load balancers its also quicker to add or remove instances from a load balancer than to reassign an elastic ip its more convenient to point dns records to load balancers instead of pointing them to specific ips you manage manually they can also be route 53 aliases which are easier to change and manage but in some situations you do need to manage and fix ip addresses of ec2 instances for example if a customer needs a fixed ip these situations require elastic ips elastic ips are limited to 5 per account its possible to request more if an elastic ip is not attached to an active resource there is a small hourly fee elastic ips are no extra charge as long as youre using them they have a small cost when not in use which is a mechanism to prevent people from squatting on excessive numbers of ip addresses elastic ip gotchas and limitations 🔸there is officially no way to allocate a contiguous block of ip addresses something you may desire when giving ips to external users though when allocating at once you may get lucky and have some be part of the same cidr block glacier glacier basics 📒 homepage ∙ developer guide ∙ faq ∙ pricing glacier is a lower cost alternative to s3 when data is infrequently accessed such as for archival purposes its only useful for data that is rarely accessed it generally takes 3 5 hours to fulfill a retrieval request aws has not officially revealed the storage media used by glacier it may be low spin hard drives or even tapes glacier tips you can physically ship your data to amazon to put on glacier on a usb or esata hdd glacier gotchas and limitations 🔸getting files off glacier is glacially slow typically 3 5 hours or more 🔸due to a fixed overhead per file you pay per put or get operation uploading and downloading many small files on to glacier might be very expensive there is also a 32k storage overhead per file hence its a good idea is to archive files before upload 🔸glaciers pricing policy is reportedly pretty complicated “glacier data retrievals are priced based on the peak hourly retrieval capacity used within a calendar month ” some more info can be found here and here 💸be aware of the per object costs of archiving s3 data to glacier it costs 0 05 per 1 000 requests if you have large numbers of s3 objects of relatively small size it will take time to reach a break even point initial archiving cost versus lower storage pricing rds rds basics 📒 homepage ∙ user guide ∙ faq ∙ pricing see also ec2instances info rds rds is a managed relational database service allowing you to deploy and scale databases more easily it supports oracle microsoft sql server postgresql mysql mariadb and amazons own aurora rds offers out of the box support for high availability and failover for your databases rds tips if youre looking for the managed convenience of rds for other data stores such as mongodb or cassandra you may wish to consider third party services from providers such as mlab compose or instaclustr 🔹make sure to create a new parameter group and option group for your database since the default parameter group does not allow dynamic configuration changes rds instances start with a default timezone of utc if necessary this can be changed to a different timezone rds gotchas and limitations ⏱rds instances run on ebs volumes either general purpose or provisioned iops and hence are constrained by ebs performance 🔸verify what database features you need as not everything you might want is available on rds for example if you are using postgres check the list of supported features and extensions if the features you need arent supported by rds youll have to deploy your database yourself 🔸if you use the failover support offered by rds keep in mind that it is based on dns changes and make sure that your client reacts to these changes appropriately this is particularly important for java given how its dns resolvers ttl is configured by default 🔸db migration to rds while importing your database into rds ensure you take into consideration the maintenance window settings if a backup is running at the same time your import can take a considerably longer time than you would have expected database sizes are limited to 6tb for all database engines except for sql server which has a 4tb limit and aurora which supports up to 64tb databases rds mysql and mariadb rds mysql and mariadb basics rds offers mysql versions 5 5 5 6 and 5 7 rds mysql and mariadb tips mysql rds allows access to binary logs multi az instances of mysql transparently replicate data across azs using drbd automated backups of multi az instances run off the backup instance to reduce latency spikes on the primary 🔸performance schema while performance schema is enabled by default in mysql 5 6 6 and later it is disabled by default in all versions of rds if you wish to enable performance schema a reboot of the rds instance will be required 🔸mysql vs mariadb vs aurora if you prefer a mysql style database but are starting something new you probably should consider aurora and mariadb as well aurora has increased availability and is the next generation solution that said aurora may not be that much faster than mysql for certain workloads mariadb the modern community fork of mysql likely now has the edge over mysql for many purposes and is supported by rds rds mysql and mariadb gotchas and limitations 🔸no super privileges rds provides some stored procedures to perform some tasks that require super privileges such as starting or stopping replication 🔸you can replicate to non rds instances of mysql but replication to these instances will break during az failovers 🔸there is no ability to manually change master on replicas so they must all be rebuilt after a failover of the master 🔸most global options are exposed only via db parameter groups some variables that were introduced in later mysql dot releases such as avoid temporal upgrade in mysql 5 6 24 are not made available in rdss 5 6 x parameter group and making use of them requires an upgrade to mysql 5 7 x 🔸rds features such as point in time restore and snapshot restore are not supported on myisam tables ensure you lock and flush each myisam table before executing a snapshot or backup operation to ensure consistency rds aurora rds aurora basics amazons proprietary fork of mysql intended to scale up for high concurrency workloads generally speaking individual query performance under aurora is not expected to improve significantly relative to mysql or mariadb but aurora is intended to maintain performance while executing many more queries concurrently than an equivalent mysql or mariadb server could handle notable new features include log structured storage instead of b trees to improve write performance out of process buffer pool so that databases instances can be restarted without clearing the buffer pool the underlying physical storage is a specialized ssd array that automatically maintains 6 copies of your data across 3 azs aurora read replicas share the storage layer with the write master which significantly reduces replica lag eliminates the need for the master to write and distribute the binary log for replication and allows for zero data loss failovers from the master to a replica the master and all the read replicas that share storage are known collectively as an aurora cluster rds aurora tips in order to take advantage of auroras higher concurrency applications should be configured with large database connection pools and should execute as many queries concurrently as possible for example aurora servers have been tested to produce increasing performance on some oltp workloads with up to 5 000 connections aurora scales well with multiple cpus and may require a large instance class for optimal performance because aurora is based on mysql 5 6 10 avoiding any mysql features from 5 7 or later will ease the transition from a mysql compatible database into aurora the easiest migration path to aurora is restoring a database snapshot from mysql 5 6 the next easiest method is restoring a dump from a mysql compatible database such as mariadb for low downtime migrations from other mysql compatible databases you can set up an aurora instance as a replica of your existing database if none of those methods are options amazon offers a fee based data migration service you can replicate from an aurora cluster to mysql or to another aurora cluster this requires binary logging to be enabled and is not as performant as native aurora replication because aurora read replicas are the equivalent of a multi az backup and they can be configured as zero data loss failover targets there are fewer scenarios in which the creation of a multi az aurora instance is required rds aurora gotchas and limitations 🔸aurora is based on mysql 5 6 10 with some cherry picking of later mysql features it is missing most 5 7 features as well as some online ddl features introduced in 5 6 17 rds sql server rds sql server basics rds offers sql server 2008 r2 2012 2014 2016 and 2017 including express web standard and enterprise rds sql server tips recently added support for backup and restore to from s3 which may make it an attractive dr option for on premises installations rds sql server gotchas and limitations 🔸the user is granted only db owner privileges for each database on the instance 🔸storage cannot be expanded for existing databases if you need more space you must restore your database on a new instance with larger storage 🔸there is a 16tb database size limit for non express editions there is also a minimum storage size 20gb for web and express 200gb for standard and enterprise 🔸limited to 30 databases per instance elasticache elasticache basics 📒 homepage ∙ user guide ∙ faq ∙ pricing elasticache is a managed in memory cache service that can be used to store temporary data in a fast in memory cache typically in order to avoid repeating the same computation multiple times when it could be reused it supports both the memcached and redis open source in memory cache software and exposes them both using their native access apis the main benefit is that aws takes care of running patching and optimizing the cache nodes for you so you just need to launch a cluster and configure its endpoint in your application while aws will take of most of the operational work of running the cache nodes elasticache tips choose the engine clustering configuration and instance type carefully based on your application needs the documentation explains in detail the pros cons and limitations of each engine in order to help you choose the best fit for your application in a nutshell redis is preferable for storing more complex data structures while memcached is just a plain key value store the simplicity of memcached allows it to be slightly faster and allows it to scale out if needed but redis has more features which you may use in your application for memcached aws provides enhanced sdks for certain programming languages which implement auto discovery a feature not available in the normal memcached client libraries elasticache gotchas and limitations since in some cases changing the cache clusters may have some restrictions like for scaling purposes it may become a problem if they were launched using cloudformation in a stack that also contains other resources and you really need to change the cache in order to avoid getting your cloudformation stacks in a non updateable state it is recommended to launch elasticache clusters just like any other resource with similar constraints in dedicated stacks which can be replaced entirely with new stacks having the desired configuration dynamodb dynamodb basics 📒 homepage ∙ developer guide ∙ faq ∙ pricing dynamodb is a nosql database with focuses on speed flexibility and scalability dynamodb is priced on a combination of throughput and storage dynamodb alternatives and lock in ⛓ unlike the technologies behind many other amazon products dynamodb is a proprietary aws product with no interface compatible alternative available as an open source project if you tightly couple your application to its api and featureset it will take significant effort to replace the most commonly used alternative to dynamodb is cassandra dynamodb tips there is a local version of dynamodb provided for developer use dynamodb streams provides an ordered stream of changes to a table use it to replicate back up or drive events off of data dynamodb can be used as a simple locking service dynamodb indexing can include primary keys which can either be a single attribute hash key or a composite hash key range you can also query non primary key attributes using secondary indexes data types dynamodb supports three data types – number string and binary – in both scalar and multi valued sets dynamodb can also support json as of late 2017 dynamodb supports both global tables and backup restore functionality dynamodb gotchas and limitations 🔸 dynamodb doesnt provide an easy way to bulk load data it is possible through data pipeline and this has some unfortunate consequences since you need to use the regular service apis to update existing or create new rows it is common to temporarily turn up a destination tables write throughput to speed import but when the tables write capacity is increased dynamodb may do an irreversible split of the partitions underlying the table spreading the total table capacity evenly across the new generation of tables later if the capacity is reduced the capacity for each partition is also reduced but the total number of partitions is not leaving less capacity for each partition this leaves the table in a state where it much easier for hotspots to overwhelm individual partitions 🔸 it is important to make sure that dynamodb resource limits are compatible with your dataset and workload for example the maximum size value that can be added to a dynamodb table is 400 kb larger items can be stored in s3 and a url stored in dynamodb 🔸 dealing with time series data in dynamodb can be challenging a global secondary index together with down sampling timestamps can be a possible solution as explained here 🔸 dynamodb does not allow an empty string as a valid attribute value the most common work around is to use a substitute value instead of leaving the field empty 🔸 when setting up fine grained policies for access to dynamodb tables be sure to include their secondary indices in the policy document as well ecs ecs basics 📒 homepage ∙ developer guide ∙ faq ∙ pricing ecs ec2 container service is a relatively new service launched end of 2014 that manages clusters of services deployed via docker see the containers and aws section for more context on containers ecs is growing in adoption especially for companies that embrace microservices deploying docker directly in ec2 yourself is another common approach to using docker on aws using ecs is not required and ecs does not yet seem to be the predominant way many companies are using docker on aws its also possible to use elastic beanstalk with docker which is reasonable if youre already using elastic beanstalk using docker may change the way your services are deployed within ec2 or elastic beanstalk but it does not radically change how most other services are used ecr ec2 container registry is amazons managed docker registry service while simpler than running your own registry it is missing some features that might be desired by some users doesnt support cross region replication of images if you want fast fleet wide pulls of large images youll need to push your image into a region local registry doesnt support custom domains certificates a containers health is monitored via clb or alb those can also be used to address a containerized service when using an alb you do not need to handle port contention i e services exposing the same port on the same host since an albs target groups can be associated with ecs based services directly ecs tips log drivers ecs supports multiple log drivers awslogs splunk fluentd syslog json use awslogs for cloudwatch make sure a group is made for the logs first drivers such as fluentd are not enabled by default you can install the agent and enable the driver by adding ecs available logging drivers awslogs fluentd to etc ecs ecs config this blog from convox and commentary lists a number of common challenges with ecs as of early 2016 it is possible to optimize disk clean up on ecs by default the unused containers are deleted after 3 hours and the unused images after 30 minutes these settings can be changed by adding ecs engine task cleanup wait duration 10m and ecs image cleanup interval 10m to etc ecs ecs config more information on optimizing ecs disk cleanup ecs alternatives and lock in kubernetes extensive container platform available as a hosted solution on google cloud https cloud google com container engine and aws https tectonic com aws have a kubernetes quickstart https aws amazon com quickstart architecture heptio kubernetes developed in collaboration with heptio nomad orchestrator scheduler tightly integrated in the hashicorp stack consul vault etc 🚧 please help expand this incomplete section fargate fargate basics 📒 homepage ∙ faq ∙ pricing fargate allows you to manage and deploy containers without having to worry about running the underlying compute infrastructure fargate serves as a new backend in addition to the legacy ec2 backend on which ecs and eks tasks can be run fargate and ec2 backends are called launch types fargate allows you to treat containers as fundamental building blocks of your infrastructure fargate tips fargate follows a similar mindset to lambda which lets you focus on applications instead of dealing with underlying infrastructure fargate is supported by cloudformation aws cli and ecs cli fargate tasks can be launched alongside tasks that use ec2 launch type 💸before creating a large fargate deployment make sure to estimate costs and compare them against alternative solution that uses traditional ec2 deployment fargate prices can be several times those of equivalently sized ec2 instances to evaluate both solutions based on potential costs refer to pricing for ec2 and fargate fargate alternatives and lock in 🚪azure container instances available on microsoft azure in preview version allows to run applications in containers without having to manage virtual machines fargate gotchas and limitations as of april 2018 fargate is available in multiple regions us east 1 us east 2 us west 2 and eu west 1 as of january 2018 fargate can only be used with ecs however there are plans to support eks later in 2018 the smallest resource values that can be configured for an ecs task that uses fargate is 0 25 vcpu and 0 5 gb of memory lambda lambda basics 📒 homepage ∙ developer guide ∙ faq ∙ pricing lambda is aws serverless compute offering allowing users to define lambda functions in a selection of runtimes that can be invoked via a variety of triggers including sns notifications and api gateway invocations lambda is the key service that enables serverless architecture on aws alongside aws api gateway aws batch and aws dynamodb lambda tips the idea behind serverless is that users dont manage provisioning scaling or maintenance of the physical machines that host their application code with lambda the machine that actually executes the user defined function is abstracted as a container when defining a lambda function users are able to declare the amount of memory available to the function which directly affects the physical hardware specification of the lambda container changing the amount of memory available to your lambda functions also affects the amount of cpu power available to it while aws does not offer hard guarantees around container reuse in general it can be expected that an unaltered lambda function will reuse a warm previously used container if called shortly after another invocation users can use this as a way to optimize their functions by smartly caching application data on initialization a lambda that hasnt been invoked in some time may not have any warm containers left in this case the lambda system will have to load and initialize the lambda code in a cold start scenario which can add significant latency to lambda invocations there are a few strategies to avoiding or mitigating cold starts including keeping containers warm by periodic triggering and favoring lightweight runtimes such as node as opposed to java lambda is integrated with aws cloudwatch and provides a logger at runtime that publishes cloudwatch events lambda offers out of the box opt in support for aws x ray x ray can help users diagnose lambda issues by offering in depth analysis of their lambdas execution flow this is especially useful when investigating issues calling other aws services as x ray gives you a detailed and easy to parse visualization of the call graph using timed cloudwatch events users can use lambda to run periodic jobs in a cron like manner events sent to lambda that fail processing can be managed using a dead letter queue dlq in sqs more on serverless martin fowlers thoughts aws serverless application model sam a simplification built on top of cloudformation that can help to define manage and deploy serverless applications using lambda serverless one of the most popular frameworks for building serverless applications using aws lambda and other serverless compute options other helpful frameworks lambda alternatives and lock in 🚪other clouds offer similar services with different names including google cloud functions azure functions and ibm openwhisk lambda gotchas and limitations 🔸testing lambdas locally and remotely can be a challenge several tools are available to make this easier including the officially supported sam local 🔸managing lots of lambda functions is a workflow challenge and tooling to manage lambda deployments is still immature 🔸aws official workflow around managing function versioning and aliases is painful one option is to avoid lambda versioning by abstracting your deployment workflow outside of lambda one way this can be accomplished is by deploying your application in successive stages with a distinct aws account per stage where each account only needs to be aware of the latest version and rollbacks and updates are handled by external tooling 🔸as of oct 2017 the minimum charge for a lambda invocation is 100ms so there is no cost benefit to reducing your run time below that 🔸while adding removing s3 buckets as triggers for lambda function this error may occur there was an error creating the trigger configuration is ambiguously defined cannot have overlapping suffixes in two rules if the prefixes are overlapping for the same event type in this case you can manually remove the lambda event in the events tab in the properties section of the s3 bucket 🔸managing the size of your deployment artifact can be a challenge especially if using java options to mitigate this include proguard and loading dependencies at runtime into tmp when using dynamodb as a trigger for your lambda functions this error may occur problem internal lambda error please contact lambda customer support this usually just means that lambda cant detect anything in the dynamodb stream within the last 48 hours if the issue persists deleting and recreating your trigger may help 🔸if your lambda needs access to resources in a vpc for example elasticache or rds it will need to be deployed within it this will increase cold start times as an elastic network interface eni will have to be registered within the vpc for each concurrent function aws also has a relatively low initial limit 350 on the number enis that can be created within an vpc however this can be increased to the 1000s if a good case is made to aws support 🔸 lambda has several resource limits as of 2017 06 a 6mb request or response payload size a 50 mb limit on the compressed zip jar file deployment package size a 250 mb limit on the code dependencies in the package before compression a 500 mb limit on local storage in tmp lambda code samples fan out is an example of using lambda to “fan out” or copy data from one service in this case kinesis to multiple other aws data services destinations for fan out data in the sample include iot sqs and more this aws limit monitor using lambdas shows use of multiple lambdas for monitoring this lambda ecs worker pattern shows use of lambda in a workflow where data from s3 is picked up by the lambda pushed to a queue then sent to ecs for more processing the secure pet store is a sample java application which uses lambda and api gateway with cognito for user identity aws lambda list is a list of hopefully useful aws lambdas and lambda related resources quite a few code samples here as usual not guaranteed tested caveat emptor 🚧 please help expand this incomplete section api gateway api gateway basics 📒 homepage ∙ developer guide ∙ faq ∙ pricing api gateway provides a scalable secured front end for service apis and can work with lambda elastic beanstalk or regular ec2 services it allows “serverless” deployment of applications built with lambda 🔸switching over deployments after upgrades can be tricky there are no built in mechanisms to have a single domain name migrate from one api gateway to another one so it may be necessary to build an additional layer in front even another api gateway to allow smooth migration from one deployment to another api gateway alternatives and lock in kong is an open source on premises api and microservices gateway built on nginx with lua kong is extensible through “plugins” tyk is an open source api gateway implemented in go and available in the cloud on premises or hybrid api gateway tips 🔹prior to 2016 11 you could only send and receive plain text data so people would base64 encode binary data but binary data is now supported api gateway supports the openapi specification aka swagger this allows you to describe your api in a language agnostic way and use various tools to generate code supporting your api generating clients is extremely easy either through the aws console or using the get sdk api api gateway integrates with cloudwatch out of the box allowing for easy logging of requests and responses note that if your request or response are too large cloudwatch will truncate the log for full request reply logging make sure to do so in your integration e g lambda a good practice when calling api gateway apis is to log the request id on the client you can later refer to these request ids in cloudwatch for easier tracing and debugging there are multiple ways to secure your api including built in support for aws cognito for most use cases cognito is the easiest and simplest way to authenticate users although you can roll your own solution using a custom authorizer which is basically a lambda you define that determines if a request is acceptable or not while api gateway lends itself well to rest style development its perfectly reasonable to implement an rpc style api in api gateway as well depending on your use case this can often lead to a much simpler api structure and smoother client experience rpc style apis are particularly useful when designing services that sit deeper in the stack and dont serve content directly to users api gateway gotchas and limitations 🔸api gateway only supports encrypted https endpoints and does not support unencrypted http this is probably a good thing 🔸api gateway doesnt support multi region deployments for high availability it is a service that is deployed in a single region but comes with a global endpoint that is served from aws edge locations similar to a cloudfront distribution you cannot have multiple api gateways with the same hostname in different aws regions and use route 53 to distribute the traffic more in this forum post 🔸integration timeout all of the various integration types eg lambda http for api gateway have timeouts as described here unlike some limits these timeouts cant be increased 🔸api gateway returns a 504 status code for any network or low level transport related issue when this happens you may see a message in the cloudwatch logs for the request that includes the message execution failed due to an internal error one possible reason for this error is that even though your backend server is up and running it may be doing something outside of the http specification like not sending well formed chunked messages you can test by hitting your backend directly with the curl raw s i backend endpoint url and seeing if it complains 🔸api gateway does not support gzip compression of responses see aws forum 🔸aws x ray support exists but cumbersome to use if you have other aws services calling api gateway your trace will seemingly end there api gateway will also not appear as a node in your service map more here 🔸be careful using the export feature the resulting swagger template is often incomplete and doesnt integrate well with the swagger extensions for things such as cors 🔸many changes to api gateway resources need to be deployed via console or api call unfortunately api gateway is terrible about notifying the user when changes are staged for deployment and what changes require deployment if youve changed something about your api and its not taking effect theres a decent chance you just need to deploy it in particular when deploying an api gateway as part of a cloudformation stack changes will not automatically deploy unless the deployment resource itself was changed you can change work around this by always changing the deployment resource on a cloudformation update or running a custom resource that ensures the deployment is made alternatively by using the serverless application model definition for an api gateway resource you can always expect the api to be deployed on a stack update since sam will generate a new deployment every time 🔸api gateway does not support nested query parameters on method requests 🚧 please help expand this incomplete section step functions step functions basics 📒 homepage ∙ developer guide ∙ faq ∙ pricing step functions is aws way to create state machines that manage a serverless workflow step functions tips a variety of structures are supported including branching parallel operations and waits tasks represent the real work nodes and are frequently lambda functions but can be activities which are externally driven tasks implemented any way you like state machines have data that flows through the steps and can be modified and added to as the state machine executes its best if your tasks are idempotent in part because you may want to re run the state machine with the same input data during debugging the aws console facilitates your examining the execution state at various steps the console lets you do this with a few steps select the input tab from the failed execution copy the input data json select the state machine name in the breadcrumbs start a new execution pasting the input data you copied previously step functions gotchas and limitations step functions are free tier eligible up to an initial 4000 transitions per month thereafter the charge is 0 025 per 1000 state transitions you can have many simultaneous executions but be aware of lambda throttling limits this has been per account pre region but recently became settable per lambda route 53 route 53 basics 📒 homepage ∙ developer guide ∙ faq ∙ pricing route 53 is aws dns service route 53 alternatives and lock in historically aws was slow to penetrate the dns market as it is often driven by perceived reliability and long term vendor relationships but route 53 has matured and is becoming the standard option for many companies route 53 is cheap by historic dns standards as it has a fairly large global network with geographic dns and other formerly “premium” features its convenient if you are already using aws ⛓generally you dont get locked into a dns provider for simple use cases but increasingly become tied in once you use specific features like geographic routing or route 53s alias records 🚪many alternative dns providers exist ranging from long standing premium brands like ultradns and dyn to less well known more modestly priced brands like dnsmadeeasy most dns experts will tell you that the market is opaque enough that reliability and performance dont really correlate well with price ⏱route 53 is usually somewhere in the middle of the pack on performance tests e g the solvedns reports route 53 tips 🔹know about route 53s “alias” records route 53 supports all the standard dns record types but note that alias resource record sets are not standard part of dns but a specific route 53 feature its available from other dns providers too but each provider has a different name for it aliases are like an internal name a bit like a cname that is resolved internally on the server side for example traditionally you could have a cname to the dns name of a clb or alb but its often better to make an alias to the same load balancer the effect is the same but in the latter case externally all a client sees is the target the record points to its often wise to use alias record as an alternative to cnames since they can be updated instantly with an api call without worrying about dns propagation you can use them for clbs albs or any other resource where aws supports it somewhat confusingly you can have cname and a aliases depending on the type of the target because aliases are extensions to regular dns records if exported the output zone file will have additional non standard “alias” lines in it latency based routing allows users around the globe to be automatically directed to the nearest aws region where you are running so that latency is reduced understand that domain registration and dns management hosted zones are two separate route 53 services when you buy transfer a domain route 53 automatically assigns four name servers to it e g ns 2 awsdns 00 com route 53 also offers to automatically create a hosted zone for dns management but you are not required do your dns management in the same account or even in route 53 you just need to create an ns record pointing to the servers assigned to your domain in route 53 one use case would be to put your domain registration very mission critical in a bastion account while managing the hosted zones within another account which is accessible by your applications cloudformation cloudformation basics 📒 homepage ∙ developer guide ∙ faq ∙ pricing at no additional charge cloudformation allows you to manage sets of resources from other aws services grouped into stacks cloudformation allows you to define these stacks in a template using json or yaml cloudformation is one of the major services underpinning aws infrastructure as code capabilities and is crucial in enabling repeatable and consistent deployments of infrastructure 💸cloudformation itself has no additional charge itself you pay for the underlying resources cloudformation alternatives and lock in hashicorps terraform is a third party alternative that can support other cloud platforms providers including azure and openstack 🔸some aws features may not be available in terraform e g multi az elasticache using redis and you may have to resort to embedded cloudformation templates cloudformation tips validate your stack in a different aws account cloudformation truly shines when making multiple deployments of the same stack to different accounts and regions a common practice is to deploy stacks in successive stages ending in a production rollout avoid potentially time consuming syntax errors from eating into your deployment time by running validate template cloudformation is sometimes slow to update what resources and new features on old services a user is able to define in the template if you need to deploy a resource or feature that isnt supported by the template cloudformation allows running arbitrary code using lambda on a stack create or update via custom resources custom resources make cloudformation into a truly powerful tool as you can do all sorts of neat things quite easily such as sanity tests initial configuration of dynamo tables or s3 buckets cleaning up old cloudwatch logs etc for writing custom resources in java cfnresponse comes in very handy for writing custom resources in javascript aws provides a good reference in the documentation cloudformation offers a visual template designer that can be useful when getting up to speed with the template syntax by using stacksets users can define and deploy an entire production application consisting of multiple stacks one service per stack in a single cloudformation template if youre developing a serverless application i e using lambda api gateway cloudformation offers a simplified template format called sam ❗use a restrictive stack policy without one you can inadvertently delete live production resources probably causing a severe outage ❗turn on termination protection on all of your stacks to avoid costly accidents the cloudformation template reference is indispensable when discovering what is and isnt possible in a cloudformation template troposphere is a python library that makes it much easier to create cloudformation templates currently supports aws and openstack resource types troposphere attempts to support all resources types that can be described in cloudformation templates built in error checking a recommended soft dependency is awacs which allows you to generate aws access policy in json by writing python code stacker is a python application that makes it easy to define configure orchestrate and manage dependencies for cloudformation stacks across multiple user defined environments if you are building different stacks with similar layers it may be useful to build separate templates for each layer that you can reuse using aws cloudformation stack 🔸avoid hardcoding resource parameters that can potentially change use stack parameters as much as you can and resort to default parameter values 🔹until 2016 cloudformation used only an awkward json format that makes both reading and debugging difficult to use it effectively typically involved building additional tooling including converting it to yaml but now this is supported directly wherever possible export relevant physical ids from your stacks by defining outputs in your cloudformation templates these are the actual names assigned to the resources being created outputs can be returned from describestack api calls and get imported to other stacks as part of the recent addition of cross stack references note that importing outputs in a stack from another stack creates a hard dependency that is tracked by cloudformation you will not be able to delete the stack with the outputs until there are no importing stacks cloudformation can be set up to send sns notifications upon state changes enabling programmatic handling of situations where stacks fail to build or simple email alerts so the appropriate people are informed cloudformation allows the use of conditionals when creating a stack one common way to leverage this capability is in support of multi environment cloudformation templates – by configuring them to use ‘if else statements on the value of a parameter passed in e g “env” environment specific values for things like vpc ids securitygroup ids and ami names can be passed into reusable generic templates version control your cloudformation templates in the cloud an application is the combination of the code written and the infrastructure it runs on by version controlling both it is easy to roll back to known good states avoid naming your resources explicitly e g dynamodb tables when deploying multiple stacks to the same aws account these names can come into conflict potentially slowing down your testing prefer using resource references instead for things that shouldnt ever be deleted you can set an explicit deletionpolicy on the resource that will prevent the resource from being deleted even if the cloudformation stack itself is deleted this is useful for anything that can maintain expensive to rebuild state such as dynamodb tables and things that are exposed to the outside world such as api gateway apis cloudformation gotchas and limitations 🔸a given cloudformation stack can end up in a wide variety of states error reporting is generally weak and often times multiple observe tweak redeploy cycles are needed to get a working template the internal state machine for all the varying states is extremely opaque 🔸some cross region operations are not possible in cloudformation without using a custom resource such as cross region sns subscriptions 🔸while having hand made resources live alongside cloudformation created resources is inadvisable its sometimes unavoidable if at all possible leave all resource management up to a cloudformation template and only provide read only access to the console ❗modifications to stack resources made outside cloudformation can potentially lead to stacks stuck in update rollback failed mode stacks in this state can be recovered using the continue update rollback command this command can be initiated in the console or in the cli the resources to skip parameter usable in the cli can be useful if the continue update rollback command fails 🔸cloudformation is useful but complex and with a variety of pain points many companies find alternate solutions and many companies use it but only with significant additional tooling 🔸cloudformation can be very slow especially for items like cloudfront distributions and route53 cname entries 🔸its hard to assemble good cloudformation configurations from existing state aws does offer a trick to do this but its very clumsy cloudformer also hasnt been updated in ages as of oct 2017 doesnt support templatizing many new services and wont fully define even existing services that have since been updated for example dynamo tables defined through cloudformer wont contain ttl definitions or auto scaling configuration 🔸many users dont use cloudformation at all because of its limitations or because they find other solutions preferable often there are other ways to accomplish the same goals such as local scripts boto bash ansible etc you manage yourself that build infrastructure or docker based solutions convox etc 🔸deploying large stacks i e many resources can be problematic due to unintuitive api limits for instance api gateways createdeployment api has a default limit of 3 requests per minute as of 1 12 2018 this limit is readily exceeded even in moderately sized cloudformation stacks creating cw alarms is another commonly seen limit putmetricalarm 3 tps as of 1 12 2018 especially when creating many autoscaling policies for dynamodb one way to work around this limit is to include cloudformation dependson clauses to artificially chain resource creation 🔸creating deleting stacks can be a little less clean than ideal some resources will leave behind traces in your aws account even after deletion e g lambda will leave behind cloudwatch log groups that never expire vpcs network security and security groups vpc basics 📒 homepage ∙ user guide ∙ faq ∙ security groups ∙ pricing vpc virtual private cloud is the virtualized networking layer of your aws systems most aws users should have a basic understanding of vpc concepts but few need to get into all the details vpc configurations can be trivial or extremely complex depending on the extent of your network and security needs all modern aws accounts those created after 2013 12 04 are “ec2 vpc” accounts that support vpcs and all instances will be in a default vpc older accounts may still be using “ec2 classic” mode some features dont work without vpcs so you probably will want to migrate vpc and network security tips ❗security groups are your first line of defense for your servers be extremely restrictive of what ports are open to all incoming connections in general if you use clbs albs or other load balancing the only ports that need to be open to incoming traffic would be port 22 and whatever port your application uses security groups access policy is deny by default port hygiene a good habit is to pick unique ports within an unusual range for each different kind of production service for example your web frontend might use 3010 your backend services 3020 and 3021 and your postgres instances the usual 5432 then make sure you have fine grained security groups for each set of servers this makes you disciplined about listing out your services but also is more error proof for example should you accidentally have an extra apache server running on the default port 80 on a backend server it will not be exposed migrating from classic for migrating from older ec2 classic deployments to modern ec2 vpc setup this article may be of help you can migrate elastic ips between ec2 classic and ec2 vpc for basic aws use one default vpc may be sufficient but as you scale up you should consider mapping out network topology more thoroughly a good overview of best practices is here consider controlling access to you private aws resources through a vpn you get better visibility into and control of connection and connection attempts you expose a smaller surface area for attack compared to exposing separate potentially authenticated services over the public internet e g a bug in the yaml parser used by the ruby on rails admin site is much less serious when the admin site is only visible to the private network and accessed through vpn another common pattern especially as deployments get larger security or regulatory requirements get more stringent or team sizes increase is to provide a bastion host behind a vpn through which all ssh connections need to transit for a cheap vpn to access private aws resources consider using a point to site software vpn such as openvpn it can either be installed using the official ami though you are limited to 2 concurrent users on the free license or it can be installed using the openvpn package on linux the linux package allows for unlimited concurrent users but the installation is less straightforward this openvpn installer script can help you install it and add client keys easily 🔹consider using other security groups as sources for security group rules instead of using cidrs — that way all hosts in the source security group and only hosts in that security group are allowed access this is a much more dynamic and secure way of managing security group rules vpc flow logs allow you to monitor the network traffic to from and within your vpc logs are stored in cloudwatch logs groups and can be used for security monitoring with third party tools performance evaluation and forensic investigation see the vpc flow logs user guide for basic information see the flowlogs reader cli tool and python library to retrieve and work with vpc flow logs ipv6 is available in vpc along with this announcement came the introduction of the egress only internet gateway in cases where one would use nat gateways to enable egress only traffic for their vpc in ipv4 one can use an egress only internet gateway for the same purpose in ipv6 amazon provides an ipv6 cidr block for your vpc at your request at present you cannot implement your own ipv6 block if you happen to own one already new and existing vpcs can both use ipv6 existing vpcs will need to be configured to have an ipv6 cidr block associated with them just as new vpcs do vpc and network security gotchas and limitations 🔸vpcs are tied to one region in one account subnets are tied to one vpc and limited to one availability zone 🔸security groups are tied to one vpc if you are utilizing infrastructure in multiple vpcs you should make sure your configuration deployment tools take that into account 🔸vpc endpoints are currently only available for s3 and dynamodb if you have a security requirement to lockdown outbound traffic from your vpc you may want to use dns filtering to control outbound traffic to other services ❗be careful when choosing your vpc ip cidr block if you are going to need to make use of classiclink make sure that your private ip range doesnt overlap with that of ec2 classic ❗if you are going to peer vpcs carefully consider the cost of data transfer between vpcs since for some workloads and integrations this can be prohibitively expensive ❗new rds instances require a subnet group within your vpc if youre using the default vpc this isnt a concern it will contain a subnet for each availability zone in your region however if youre creating your own vpc and plan on using rds make sure you have at least two subnets within the vpc to act as the subnet group ❗if you delete the default vpc you can recreate it via the cli or the console ❗be careful with vpc vpn credentials if lost or compromised the vpn endpoint must be deleted and recreated see the instructions for replacing compromised credentials ❗security groups and route tables apply entries separately for ipv4 and ipv6 so one must ensure they add entries for both protocols accordingly 💸managed nat gateways are a convenient alternative to manually managing nat instances but they do come at a cost per gigabyte consider alternatives if youre transferring many terabytes from private subnets to the internet kms kms basics 📒 homepage ∙ developer guide ∙ faq ∙ pricing kms key management service is a secure service for creating storing and auditing usage of cryptographic keys service integration kms integrates with other aws services ebs elastic transcoder emr redshift rds ses s3 workmail and workspaces encryption apis the encrypt and decrypt api allow you to encrypt and decrypt data on the kms service side never exposing the master key contents data keys the generatedatakey api generates a new key off of a master key the data key contents are exposed to you so you can use it to encrypt and decrypt any size of data in your application layer kms does not store manage or track data keys you are responsible for this in your application 🔹auditing turn on cloudtrail to audit all kms api events access use key policies and iam policies to grant different levels of kms access for example you create an iam policy that only allows a user to encrypt and decrypt with a specific key kms tips 🔹its very common for companies to manage keys completely via home grown mechanisms but its far preferable to use a service such as kms from the beginning as it encourages more secure design and improves policies and processes around managing keys a good motivation and overview is in this aws presentation the cryptographic details are in this aws whitepaper this blog from convox demonstrates why and how to use kms for encryption at rest kms gotchas and limitations 🔸the encrypt api only works with 4kb of data larger data requires generating and managing a data key in your application layer 🔸kms audit events are not available in the cloudtrail lookup events api you need to look find them in the raw json gz files that cloudtrail saves in s3 🔸in order to encrypt a multi part upload to s3 the kms key policy needs to allow “kms decrypt” and “kms generatedatakey ” in addition to “kms encrypt” otherwise the upload will fail with an “accessdenied” error 🔸kms keys are region specific — they are stored and can only be used in the region in which they are created they cant be transferred to other regions 🔸kms keys have a key policy that must grant access to something to manage the key if you dont grant anything access to the key on creation then you have to reach out to support to have the key policy reset reduce the risk of the key becoming unmanagable 🔸if you use a key policy to grant access to iam roles or users and then delete the user role recreating the user or role wont grant them permission to the key again cloudfront cloudfront basics 📒 homepage ∙ developer guide ∙ faq ∙ pricing cloudfront is aws content delivery network cdn its primary use is improving latency for end users through accessing cacheable content by hosting it at over 60 global edge locations cloudfront alternatives and lock in 🚪cdns are a highly fragmented market cloudfront has grown to be a leader but there are many alternatives that might better suit specific needs cloudfront tips 🐥ipv6 is supported this is a configurable setting and is enabled by default on new cloudfront distributions ipv6 support extends to the use of waf with cloudfront 🐥http 2 is now supported clients must support tls 1 2 and sni while the most common use is for users to browse and download content get or head methods requests cloudfront also supports since 2013 uploaded data post put delete options and patch you must enable this by specifying the allowed http methods when you create the distribution interestingly the cost of accepting uploaded data is usually less than for sending downloaded data in its basic version cloudfront supports ssl via the sni extension to tls which is supported by all modern web browsers if you need to support older browsers you need to pay a few hundred dollars a month for dedicated ips 💸⏱consider invalidation needs carefully cloudfront does support invalidation of objects from edge locations but this typically takes many minutes to propagate to edge locations and costs 0 005 per request after the first 1000 requests some other cdns support this better everyone should use tls nowadays if possible ilya grigoriks table offers a good summary of features regarding tls performance features of cloudfront an alternative to invalidation that is often easier to manage and instant is to configure the distribution to cache with query strings and then append unique query strings with versions onto assets that are updated frequently ⏱for good web performance it is recommended to enable compression on cloudfront distributions if the origin is s3 or another source that does not already compress cloudfront gotchas and limitations 🔸if using s3 as a backing store remember that the endpoints for website hosting and for general s3 are different example “bucketname s3 amazonaws com” is a standard s3 serving endpoint but to have redirect and error page support you need to use the website hosting endpoint listed for that bucket e g “bucketname s3 website us east 1 amazonaws com” or the appropriate region 🔸by default cloudfront will not forward http host headers through to your origin servers this can be problematic for your origin if you run multiple sites switched with host headers you can enable host header forwarding in the default cache behavior settings 🔸4096 bit ssl certificates cloudfront do not support 4096 bit ssl certificates as of late 2016 if you are using an externally issued ssl certificate youll need to make sure its 2048 bits see ongoing discussion although connections from clients to cloudfront edge servers can make use of ipv6 connections to the origin server will continue to use ipv4 directconnect directconnect basics 📒 homepage ∙ user guide ∙ faq ∙ pricing direct connect is a private dedicated connection from your network s to aws directconnect tips if your data center has a partnering relationship with aws setup is streamlined use for more consistent predictable network performance guarantees 1 gbps or 10 gbps per link use to peer your colocation corporate or physical datacenter network with your vpc s example extend corporate ldap and or kerberos to ec2 instances running in a vpc example make services that are hosted outside of aws for financial regulatory or legacy reasons callable from within a vpc redshift redshift basics 📒 homepage ∙ developer guide ∙ faq ∙ pricing redshift is aws managed data warehouse solution which is massively parallel scalable and columnar it is very widely used it was built using paraccel technology and exposes postgres compatible interfaces redshift alternatives and lock in ⛓🚪whatever data warehouse you select your business will likely be locked in for a long time also and not coincidentally the data warehouse market is highly fragmented selecting a data warehouse is a choice to be made carefully with research and awareness of the market landscape and what business intelligence tools youll be using redshift tips although redshift is mostly postgres compatible its sql dialect and performance profile are different redshift supports only 12 primitive data types list of unsupported postgres types it has a leader node and computation nodes the leader node distributes queries to the computation ones note that some functions can be executed only on the lead node 🔹make sure to create a new cluster parameter group and option group for your database since the default parameter group does not allow dynamic configuration changes major third party bi tools support redshift integration see quora top 10 performance tuning techniques for amazon redshift provides an excellent list of performance tuning techniques amazon redshift utils contains useful utilities scripts and views to simplify redshift ops vacuum regularly following a significant number of deletes or updates to reclaim space and improve query performance avoid performing blanket vacuum or analyze operations at a cluster level the checks on each table to determine whether vacuum or analyze action needs to be taken is wasteful only perform analyze and vacuum commands on the objects that require it utilize the analyze vacuum schema utility to perform this work the sql to determine whether a table needs to be vacuumed or analyzed can be found in the schema utility readme if you wish to create your own maintenance process redshift provides various column compression options to optimize the stored data size aws strongly encourages users to use automatic compression at the copy stage when redshift uses a sample of the data being ingested to analyze the column compression options however automatic compression can only be applied to an empty table with no data therefore make sure the initial load batch is big enough to provide redshift with a representative sample of the data the default sample size is 100 000 rows redshift uses columnar storage hence it does not have indexing capabilities you can however use distribution key and sortkey to improve performance redshift has two types of sort keys compounding sort key and interleaved sort key a compound sort key is made up of all columns listed in the sort key definition it is most useful when you have queries with operations using the prefix of the sortkey an interleaved sort key on the other hand gives equal weight to each column or a subset of columns in the sort key so if you dont know ahead of time which column s you want to choose for sorting and filtering this is a much better choice than the compound key here is an example using interleaved sort key 🔸⏱ distribution strategies since data in redshift is physically distributed among nodes choosing the right data distribution key and distribution style is crucial for adequate query performance there are three possible distribution style settings — even the default key or all use key to collocate join key columns for tables which are joined in queries use all to place the data in small sized tables on all cluster nodes redshift gotchas and limitations ❗⏱while redshift can handle heavy queries well it does not scale horizontally i e does not handle multiple queries in parallel therefore if you expect a high parallel load consider replicating or if possible sharding your data across multiple clusters 🔸 the leader node which manages communications with client programs and all communication with compute nodes is the single point of failure ⏱although most redshift queries parallelize well at the compute node level certain stages are executed on the leader node which can become the bottleneck 🔹redshift data commit transactions are very expensive and serialized at the cluster level therefore consider grouping multiple mutation commands copy insert update commands into a single transaction whenever possible 🔹redshift does not support multi az deployments building multi az clusters is not trivial here is an example using kinesis 🔸beware of storing multiple small tables in redshift the way redshift tables are laid out on disk makes it impractical the minimum space required to store a table in mb is nodes slices node columns for example on a 16 node cluster an empty table with 20 columns will occupy 640mb on disk ⏱ query performance degrades significantly during data ingestion wlm workload management tweaks help to some extent however if you need consistent read performance consider having replica clusters at the extra cost and swap them during update ❗ never resize a live cluster the resize operation can take hours depending on the dataset size in rare cases the operation may also get stuck and youll end up having a non functional cluster the safer approach is to create a new cluster from a snapshot resize the new cluster and shut down the old one 🔸redshift has reserved keywords that are not present in postgres see full list here watch out for delta delta encodings 🔸redshift does not support many postgres functions most notably several date time related and aggregation functions see the full list here 🔸 uniqueness primary key and foreign key constraints on redshift tables are informational only and are not enforced they are however used by the query optimizer to generate query plans not null column constraints are enforced see here for more information on defining constraints 🔸compression on sort key can result in significant performance impact so if your redshift queries involving sort key s are slow you might want to consider removing compression on a sort key 🔹 choosing a sort key is very important since you can not change a tables sort key after it is created if you need to change the sort or distribution key of a table you need to create a new table with the new key and move your data into it with a query like “insert into new table select from old table” ❗🚪 when moving data with a query that looks like “insert into x select from y” you need to have twice as much disk space available as table “y” takes up on the clusters disks redshift first copies the data to disk and then to the new table here is a good article on how to this for big tables emr emr basics 📒 homepage ∙ release guide ∙ faq ∙ pricing emr which used to stand for elastic map reduce but not anymore since it now extends beyond map reduce is a service that offers managed deployment of hadoop hbase and spark it reduces the management burden of setting up and maintaining these services yourself emr alternatives and lock in ⛓most of emr is based on open source technology that you can in principle deploy yourself however the job workflows and much other tooling is aws specific migrating from emr to your own clusters is possible but not always trivial emr tips emr relies on many versions of hadoop and other supporting software be sure to check which versions are in use ⏱off the shelf emr and hadoop can have significant overhead when compared with efficient processing on a single machine if your data is small and performance matters you may wish to consider alternatives as this post illustrates python programmers may want to take a look at yelps mrjob it takes time to tune performance of emr jobs which is why third party services such as quboles data service are gaining popularity as ways to improve performance or reduce costs emr gotchas and limitations 💸❗emr costs can pile up quickly since it involves lots of instances efficiency can be poor depending on cluster configuration and choice of workload and accidents like hung jobs are costly see the section on ec2 cost management especially the tips there about spot instances this blog post has additional tips but was written prior to the shift to per second billing 💸 beware of “double dipping” with emr you pay for the ec2 capacity and the service fees in addition emr syncs task logs to s3 which means you pay for the storage and put requests at s3 standard rates while the log files tend to be relatively small every hadoop job depending on the size generates thousands of log files that can quickly add up to thousands of dollars on the aws bill yarns log aggregation is not available on emr kinesis streams kinesis streams basics 📒 homepage ∙ developer guide ∙ faq ∙ pricing kinesis streams which used to be only called kinesis before kinesis firehose and kinesis analytics were launched is a service that allows you to ingest high throughput data streams for immediate or delayed processing by other aws services kinesis streams subcomponents are called shards each shard provides 1mb s of write capacity and 2mb s of read capacity at a maximum of 5 reads per second a stream can have its shards programmatically increased or decreased based on a variety of metrics all records entered into a kinesis stream are assigned a unique sequence number as they are captured the records in a stream are ordered by this number so any time ordering is preserved this page summarizes key terms and concepts for kinesis streams kinesis streams alternatives and lock in 🚪 kinesis is most closely compared to apache kafka an open source data ingestion solution it is possible to set up a kafka cluster hosted on ec2 instances or any other vps however you are responsible for managing and maintaining both zookeeper and the kafka brokers in a highly available configuration confluent has a good blog post with their recommendations on how to do this here which has links on the bottom to several other blogs they have written on the subject ⛓ kinesis uses very aws specific apis so you should be aware of the potential future costs of migrating away from it should you choose to use it an application that efficiently uses kinesis streams will scale the number of shards up and down based on the required streaming capacity note there is no direct equivalent to this with apache kafka kinesis streams tips the kcl kinesis client library provides a skeleton interface for java node python ruby and net programs to easily consume data from a kinesis stream in order to start consuming data from a stream you only need to provide a config file to point at the correct kinesis stream and functions for initialising the consumer processing the records and shutting down the consumer within the skeletons provided the kcl uses a dynamodb table to keep track of which records have been processed by the kcl this ensures that all records are processed “at least once” it is up to the developer to ensure that the program can handle doubly processed records the kcl also uses dynamodb to keep track of other kcl “workers” it automatically shares the available kinesis shards across all the workers as equally as possible kinesis streams gotchas and limitations 🔸⏱ kinesis streams shards each only permit 5 reads per second if you are evenly distributing data across many shards your read limit for the stream will remain at 5 reads per second on aggregate as each consuming application will need to check every single shard for new records this puts a hard limit on the number of different consuming applications possible per stream for a given maximum read latency for example if you have 5 consuming applications reading data from one stream with any number of shards they cannot read with a latency of less than one second as each of the 5 consumers will need to poll each shard every second reaching the cap of 5 reads per second per shard this blog post further discusses the performance and limitations of kinesis in production 💸 kinesis streams are not included in the free tier make sure if you do any experimentation with it on a personal account you shut down the stream or it may run up unexpected costs 11 per shard month kinesis firehose kinesis firehose gotchas and limitations 🔸 📜 when delivering from firehose to elasticsearch the json document cannot contain an “ id” property firehose will not attempt to deliver those documents and wont log any error device farm device farm basics 📒 homepage ∙ developer guide ∙ faq ∙ pricing device farm is an aws service that enables mobile app testing on real devices supports ios and android including kindle fire devices as well as the mobile web supports remote device access in order to allow for interactive testing debugging device farm tips aws mobile blog contains several examples of device farm usage for testing device farm offers a free trial for users who want to evaluate their service device farm offers two pricing models paying per device minute is useful for small usage levels or for situations where it‘s hard to predict usage amount unmetered plans are useful in situations where active usage is expected from the beginning to minimize waiting time for device availability one approach is to create several device pools with different devices then randomly choose one of the unused device pools on every run device farm gotchas and limitations ❗devices dont have a sim card and therefore can‘t be used for testing sim card related features 🔸device farm supports testing for most popular languages frameworks but not for all an actual list of supported frameworks and languages is presented on this page 🔸the api and cli for device farm is quite a low level and may require developing additional tools or scripts on top of it 🔸aws provide several tools and plugins for device farm however it doesn‘t cover all cases or platforms it may require developing specific tools or plugins to support specific requirements ❗in general device farm doesn‘t have android devices from chinese companies like huawei meizu lenovo etc an actual list of supported devices located here 🔸device availibility is uneven it depends on several factors including device popularity usually more modern devices see higher demand thus the waiting time for them will be higher compared to relatively old devices mobile hub mobile hub basics 📒 homepage ∙ user guide ∙ faq ∙ pricing mobile hub orchestrates multiple services to create an aws backend for mobile and web applications each project in mobile hub has one backend made up of configurable features plus one or more applications features include analytics cloud logic conversational bots hosting and streaming nosql database user data storage and user sign in each feature uses one or two services to deliver a chunk of functionality services used include api gateway cloudfront cognito device farm dynamodb lambda lex pinpoint and s3 application sdks exist for android java ios swift web js and react native js there is also a cli for javascript applications mobile hub tips the mobile hub console has starter kits and tutorials for various app platforms the cli allows local development of lambda code js by default with awsmobile pull push commands to sync from cloud to folder and back again mobile hub itself is free but each of the services has its own pricing model mobile hub gotchas and limitations 🔸the cloud api feature allows importing an existing lambda function instead of defining a new one but there are some rough edges with the cli check the github issues ❗mobile hub uses cloudformation under the covers and gets confused when a service is changed outside of the mobile hub console iot iot basics 📒 homepage ∙ user guide ∙ faq ∙ pricing iot is a platform for allowing clients such as iot devices or software applications examples to communicate with the aws cloud clients are also called devices or things and include a wide variety of device types roughly there are three categories of device types that interact with iot services by sending message over an iot protocol to the iot pub sub style message broker which is called the iot device gateway send messages only for example the aws iot button on an eddystone beacon send and receive messages for example the phillips home safe medical alert device send receive and process messages for example a simple processing board such as a raspberry pi quick start guide or an aws device such as echo or echo dot which are designed to work with the aws alexa skills kit a programmable voice enabled service from aws aws has a useful quick start using the console and a slide presentation on core topics iot terms aws iot things metadata for devices in a registry and can store device state in a json document which is called a device shadow device metadata can also be stored in iot thing types this aids in device metadata management by allowing for reuse of device description and configuration for more than one device note that iot thing types can be deprecated but not changed — they are immutable aws iot certificates device authentication are the logical association of a unique certificate to the logical representation of a device this association can be done in the console in addition the public key of the certificate must be copied to the physical device this covers the authentication of devices to a particular aws device gateway or message broker you can associate an aws iot certificate with an iot device or you can register your own ca certificate authority with aws generate your own certificate s and associate those certificates with your devices via the aws console or cli aws iot policies device topic authorization are json files that are associated to one or more aws iot certificates this authorizes associated devices to publish and or subscribe to messages from one or more mqtt topics aws iot rules are sql like queries which allows for reuse of some or all device message data as described in this presentation which summarizes design patterns with for iot rules shown below is a diagram which summarizes the flow of messages between the aws iot services iot greengrass 📒 homepage 🐥greengrass is a software platform that extends aws iot capabilities allowing lambda functions to be run directly on local devices it also enables iot devices to be able to securely communicate on a local network without having to connect to the cloud greengrass includes a local pub sub message manager that can buffer messages if connectivity is lost so that inbound and outbound messages to the cloud are preserved locally deployed lambda functions can be triggered by local events messages from the cloud or other sources greengrass includes secure authentication and authorization of devices within the local network and also between the local network and the aws cloud it also provides secure over the air software updates of lambda functions greengrass core software includes a message manager object lambda runtime local copy service for iot thing or device shadows and a deployment agent to manage greengrass group configuration greengrass groups are containers for selected iot devices settings subscriptions and associated lambda functions in a greengrass group a device is either a greengrass core or an iot device which will be connected that particular greengrass core the greengrass core sdk enables lambda functions to interact with the aws greengrass core on which they run in order to publish messages interact with the local thing shadows service or invoke other deployed lambda functions the aws greengrass core sdk only supports sending mqtt messages with qos 0 shown below is a diagram which shows the architecture of aws iot greengrass services iot alternatives and lock in aws microsoft and google have all introduced iot specific sets of cloud services since late 2015 aws was first moving their iot services to general availability in dec 2015 microsoft released their set of iot services for azure in feb 2016 google has only previewed but not released their iot services android things and weave issues of lock in center around your devices — protocols for example mqtt amqp message formats such as json vs hex and security certificates iot tips getting started with buttons one way to start is to use an aws iot button aws provides a number of code samples for use with their iot button you can use the aws iot console click the “connect aws iot button” link and youll be taken to the aws lambda console there you fill out your buttons serial number to associate it with a lambda as of this writing aws iot buttons are only available for sale in the us connections and protocols it is important to understand the details of about the devices you wish to connect to the aws iot service including how you will secure the device connections the device protocols and more cloud vendors differ significantly in their support for common iot protocols such as mqtt amqp xmpp aws iot supports secure mqtt websockets and https support for device security via certificate processing is a key differentiator in this space in august 2016 aws added just in time registrations for iot devices to their services combining with other services its common to use other aws services such as aws lambda kinesis and dynamodb although this is by no means required sample iot application reference architectures are in this screencast testing tools to get started aws includes a lightweight mqtt client in the aws iot console here you can create and test sending and receiving messages to and from various mqtt topics when testing locally if using mqtt it may be helpful to download and use the open source mosquitto broker tool for local testing with devices and or device simulators use this mqtt load simulator to test device message load throughout your iot solution iot gotchas and limitations 🔸iot protocols it is important to verify the exact type of support for your particular iot device message protocol for example one commonly used iot protocol is mqtt within mqtt there are three possible levels of qos in mqtt aws iot supports mqtt qos 0 fire and forget or at most once and qos 1 at least once or includes confirmation but not qos 2 exactly once requires 4 step confirmation this is important in understanding how much code youll need to write for your particular application message resolution needs here is a presentation about the nuances of connecting 🔸the ecosystems to match iam users or roles to iot policies and their associated authorized aws iot devices are immature custom coding to enforce your security requirements is common ❗a common mistake is to misunderstand the importance of iot device security it is imperative to associate each device with a unique certificate public key you can generate your own certificates and upload them to aws or you can use aws generated iot device certificates its best to read and understand awss own guidance on this topic 🔸there is only one aws iot gateway endpoint per aws account for production scenarios youll probably need to set up multiple aws accounts in order to separate device traffic for development test and production its interesting to note that the azure iot gateway supports configuration of multiple endpoints so that a single azure account can be used with separate pub sub endpoints for development testing and production 🔸limits be aware of limits including device message size type frequency and number of aws iot rules iot code samples simple beer service is a surprisingly useful code example using aws iot lambda etc iot elf offers clean python sample using the aws iot sdk iot button projects on hackster include many different code samples for projects 5 iot code examples a device simulator mqtt sample just in time registration truck simulator prediction data simulator aws alexa trivia voice example is a quick start using alexa voice capability and lambda some raspberry pi examples include the beacon project danbo and gopigo ses ses basics 📒 homepage ∙ documentation ∙ faq ∙ pricing ses or simple email service is a service that exposes smtp endpoints for your application to directly integrate with ses tips 🔹bounce handling make sure you handle this early enough your ability to send emails can be removed if ses sees too many bounces 🔹credentials many developers get confused between ses credentials and aws api keys make sure to enter smtp credentials while using the smtp apis ses gotchas and limitations 🔸internet access ses smtp endpoints are on the internet and will not be accessible from a location without internet access e g a private subnet without nat gateway route in the routing table in such a case set up an smtp relay instance in a subnet with internet access and configure your application to send emails to this smtp relay instance rather than ses the relay should have a forwarding rule to send all emails to ses ❗if you are using a proxy instead of a nat confirm that your proxy service supports smtp certificate manager certificate manager basics 📒 homepage ∙ user guide ∙ faq ∙ pricing use the certificate manager to manage ssl tls certificates in other aws services supports importing existing certificates as well as issuing new ones provides domain validated dv certificates validation is done by sending an email to 3 contact addresses in whois and 5 common addresses for the domain for each domain name present in the request as of late 2017 this can also be done via dns instead acm will attempt to automatically renew a certificate issued by amazon it will first attempt to connect to the domain on https and check that the certificate used by the domain is the same with the certificate that it intends to renew failing that it will check the dns record used previously for validation failing that acm will attempt manual validation by sending emails to all domains in the certificate certificate manager alternatives and lock in ⛓certificates issued by the certificate manager cant be used outside of the services that support it imported certificates however can still be used elsewhere certificate manager tips 🔹supported services managed load balancers cloudfront api gateway and elastic beanstalk 🔸during the domain validation process if dns validation is unsuccessful certificate manager will send an email to every contact address specified in the domains whois record and up to five common administrative addresses some anti spam filters can mark emails as spam because of this you should check the spam folder of your email if you dont receive a confirmation email 🔹 setting up a certificate for a test domain you dont have email set up on you can now use dns validation instead 🔹remember when requesting a wildcard domain that the request will not be valid for the level just below the wildcard or any subdomains preceding the wildcard take for example an approved issued certificate for bar example com this would be valid for foo bar example com but not bar example com likewise it would also not be valid for www bar foo example com you would need to add each of these domains to the certificate request certificate manager gotchas and limitations 🔸in order to use certificate manager for cloudfront distributions the certificate must be issued or imported from us east 1 n virginia region 🔸certificates used with elastic load balancers must be issued in the same region as the load balancer certificates can not be moved or copied between regions as of july 2017 if a domain uses load balancers present in multiple regions a different certificate must be requested for each region 🔸iot has its own way of setting up certificates 🔸by default the maximum number of domains per certificate is 10 you can get this limit increased to a maximum of 100 by contacting aws support note for every different domain you have on the requested cert youll need to press accept on an email sent to that domain for example if you request a cert with 42 different domains or sub domains youll need to press accept on 42 different links 🔹if you request a limit increase to aws support for this they will respond to you asking to confirm this bypass this by saying in the body of your initial request i acknowledge at the moment there is no method to add or remove a name from a certificate instead you must request a new certificate with the revised namelist and you must then re approve all of the names in the certificate even if theyd been previously approved 🔸there is no way at the moment to add or remove a domain to from an existing certificate you must request a new certificate and re approve it from each of the domains requested waf waf basics 📒 homepage ∙ documentation ∙ faq ∙ pricing waf web application firewall is used in conjunction with the cloudfront and alb services to inspect and block allow web requests based on user configurable conditions https and http requests are supported with this service wafs strength is in detecting malicious activity based on pattern matching inputs for attacks such as sql injections xss etc waf supports inspection of requests received through both ipv6 and ipv4 waf tips getting a waf api call history can be done through cloudtrail this is enabled through the cloudtrail console waf gotchas and limitations as of february 2018 waf is available in the us east northern virginia us west oregon and northern california asia pacific tokyo and eu ireland regions opsworks opsworks basics 📒 homepage ∙ documentation ∙ faq ∙ pricing stacks chef automate puppet enterprise opsworks is a configuration management service that relies heavy on chef or puppet for configuration as code deployment automation the service lets you configure and launch stacks specific to your applications needs there are numerous options in and out of aws that let you automate application deployments the separating factor between opsworks and other configuration management services elastic beanstalk for example is that opsworks specializes in letting you control the details of the systems your application runs on where a service like elastic beanstalk simplifies this to focus on application configuration opsworks stacks allows you to run your deployment stacks both in the aws cloud as well as on your own hardware on premises opsworks for chefautomate provides a managed chef configuration management server for your deployment pipeline this server stores configuration tasks and provides them to your deployment nodes without manual intervention in addition to providing other management and monitoring features as of december 2016 opsworks stacks supports chef versions 12 11 10 4 11 4 4 and 0 9 15 5 as of december 2016 opsworks for chefautomate uses chef server version 12 11 1 this is the current stable version of chef berkshelf can be used with chef stacks of version 11 10 and later for managing cookbooks and their respective dependencies running your own chef environment may be an alternative to consider some considerations are listed in this bitlancer article a key difference between running opsworkss chef variant and rolling your own chef environment is that the latter allows the scheduling of chef runs while the former has chef runs occur according to lifecycle hooks opsworks alternatives and lock in major competitors to chef include puppet and ansible opsworks tips opsworks relies heavily on chef cookbooks and recipes for customization so familiarity with reading their syntax will help greatly with getting up and running opsworks gotchas and limitations although opsworks will let you work with common chef recipes or puppet modules when creating your stacks creating custom recipes will require familiarity with chef or puppet syntax opsworks stacks is not available in the canada govcloud and beijing regions opsworks for chef automate puppet enterprise is only available in the north virginia ohio oregon northern california frankfurt singapore tokyo sydney and ireland regions batch batch basics 📒 homepage ∙ documentation ∙ faq ∙ pricing aws batch is a service that offers an environment to run batch computing jobs the service dynamically provisions the optimal compute resources needed by the jobs based on their resource requirements and can scale up to hundreds of thousands of jobs these batch workloads have access to all other aws services and features aws batch coupled with spot instances can help run the jobs when appropriate capacity is available providing for optimal utilization of compute resources the batch workloads are built as a docker image these images can then pushed to the ec2 container registry ecr or any private repository that can be accessed from aws a job definition has the workloads docker image uri and also lets the users specify the environment details like vcpus memory volume mappings environment variables parameters retry strategy container properties and the jobs iam role the compute environments are ec2 clusters that provide the runtime for the batch workloads to execute in aws batch provides managed as well as unmanaged compute environments the managed environments are provisioned and managed by aws while the unmanaged environments are managed by the customers the job definitions are submitted to job queue s for execution each queue has a priority and has at least one compute environment associated with it aws batch uses ecs to execute the containerized jobs batch tips aws batch supports prioritization of jobs via the job queue priority higher the number higher the priority aws batch supports launching the compute environment into specific vpc and subnets a compute environment is same as an ecs cluster there is no additional cost for aws batch you only pay the cost associated with the aws services being used like ec2 instances and any resources consumed by the batch jobs associate iam roles and policies with the compute environment to enable the containers access to other aws resources 🔹 use unmanaged compute environments if you need specialized resources like dedicated hosts or efs sqs sqs basics 📒 homepage ∙ documentation ∙ faq ∙ pricing sqs is a highly scalable fully managed message queuing service from aws sqs supports the pull model where the producers queue the messages and the consumers pull messages off the queue sqs provides a message visibility timeout during which the message being processed will not be delivered to other consumers if the consumer does not delete the message after processing the message becomes available to other consumers upon reaching the message visibility timeout this parameter is called visibilitytimeout each message can have up to 10 custom fields or attributes sqs allows producers to set up to 15 minutes of delay before the messages are delivered to the consumers this parameter is called delayseconds there are two types of queues supported by sqs standard queues guarantee at least once delivery of the messages do not retain the order of delivery of the messages fifo queues guarantee only once delivery of the messages guarantee the order of the delivery of the messages sqs supports fine grained access to various api calls and queues via iam policies the messages that fail to process can be put in a dead letter queue sqs alternatives and lock in alternatives to sqs include kafka rabbitmq activemq and others google cloud platform has pub sub and azure has azure queue service sqs vs sns sqs tips sns can be used in combination of sqs to build a “fan out” mechanism by having an sqs queue subscribe to the sns topic sqs supports encryption using aws kms cloudwatch alarms can be creating using various sqs metrics to trigger autoscaling actions and or notifications sqs gotchas and limitations 🔸 sqs does not have a vpc endpoint unlike s3 and dynamodb so sqs will need to be accessed using public sqs api endpoints 🔸 fifo queues are limited to 300 api calls per second 🔸 fifo queues cannot subscribe to an sns topic 🔸 standard queues can deliver duplicate messages regardless of the visibility window if only once delivery is your only choice then use fifo queues or build an additional layer to de dupe the messages 🔸 you can send receive messages in batch however there can only be maximum of 10 messages in a batch 🔸 sqs cannot be directly used as a trigger for lambda functions sns sns basics 📒 homepage ∙ documentation ∙ faq ∙ pricing sns simple notification service is a pub sub based highly scalable and fully managed messaging service that can also be used for mobile notifications sns can push the messages down to the subscribers via sms email sqs and http s transport protocols producers publish messages to a sns topics which can have many subscribers each subscription has an associated protocol which is used to notify the subscriber a copy of the message is sent to each subscriber using the associated protocol sns can also invoke lambda functions sns alternatives and lock in popular alternatives to sns are kafka notification hubs on azure and pub sub on google cloud sns vs sqs both sns and sqs are highly scalable fully managed messaging services provided by aws sqs supports a pull model while sns supports a push model consumers have to pull messages from an sqs queue while theyre pushed the message from an sns topic an sqs message is intended to be processed by only one subscriber while sns topics can have many subscribers after processing the sqs message is deleted from the queue by the subscriber to avoid being re processed an sns message is pushed to all subscribers of the topic at the same time and is not available for deletion at the topic sns supports multiple transport protocols of delivery of the messages to the subscribers while sqs subscribers have to pull the messages off the queue over https sns can be used to trigger lambda functions while sqs cannot sns tips fan out architecture can be achieved by having multiple subscribers for a topic this is particularly useful when events have to be fanned out to multiple isolated systems sns topics can be used to power webhooks with backoff support to subscribers over http s sqs queues can subscribe to sns topics sns is used to manage notifications for other aws services like autoscaling groups notifications cloudwatch alarms etc sns is frequently used as “glue” between disparate systems— such as github and aws services sns gotchas and limitations 🔸 http s subscribers of sns topics need to have public endpoints as sns does not support calling private endpoints like those in a private subnet within a vpc 📜 in a fan out scenario sse enabled sqs subscribers of an sns topic will not receive the messages sent to the topic high availability this section covers tips and information on achieving high availability high availability tips aws offers two levels of redundancy regions and availability zones azs when used correctly regions and zones do allow for high availability you may want to use non aws providers for larger business risk mitigation i e not tying your company to one vendor but reliability of aws across regions is very high multiple regions using multiple regions is complex since its essentially like managing completely separate infrastructures it is necessary for business critical services with the highest levels of redundancy however for many applications like your average consumer startup deploying extensive redundancy across regions may be overkill the high scalability blog has a good guide to help you understand when you need to scale an application to multiple regions 🔹multiple azs using azs wisely is the primary tool for high availability a typical single region high availability architecture would be to deploy in two or more availability zones with load balancing in front as in this aws diagram the bulk of outages in aws services affect one zone only there have been rare outages affecting multiple zones simultaneously for example the great ebs failure of 2011 but in general most customers outages are due to using only a single az for some infrastructure consequently design your architecture to minimize the impact of az outages especially single zone outages deploy key infrastructure across at least two or three azs replicating a single resource across more than three zones often wont make sense if you have other backup mechanisms in place like s3 snapshots a second or third az should significantly improve availability but additional reliability of 4 or more azs may not justify the costs or complexity unless you have other reasons like capacity or spot market prices 💸watch out for cross az traffic costs this can be an unpleasant surprise in architectures with large volume of traffic crossing az boundaries deploy instances evenly across all available azs so that only a minimal fraction of your capacity is lost in case of an az outage if your architecture has single points of failure put all of them into a single az this may seem counter intuitive but it minimizes the likelihood of any one spof to go down on an outage of a single az ebs vs instance storage for a number of years ebss had a poorer track record for availability than instance storage for systems where individual instances can be killed and restarted easily instance storage with sufficient redundancy could give higher availability overall ebs has improved and modern instance types since 2015 are now ebs only so this approach while helpful at one time may be increasingly archaic be sure to use and understand clbs albs appropriately many outages are due to not using load balancers or misunderstanding or misconfiguring them high availability gotchas and limitations 🔸az naming differs from one customer account to the next your “us west 1a” is not the same as another customers “us west 1a” — the letters are assigned to physical azs randomly per account this can also be a gotcha if you have multiple aws accounts 🔸💸cross az traffic is not free at large scale the costs add up to a significant amount of money if possible optimize your traffic to stay within the same az as much as possible billing and cost management billing and cost visibility aws offers a free tier of service that allows very limited usage of resources at no cost for example a micro instance and small amount of storage is available for no charge many services are only eligible for the free tier for the first twelve months that an account exists but other services offer a free usage tier indefinitely if you have an old account but starting fresh sign up for a new one to qualify for the free tier aws activate extends this to tens of thousands of dollars of free credits to startups in certain funds or accelerators you can set billing alerts to be notified of unexpected costs such as costs exceeding the free tier you can set these in a granular way aws offers cost explorer a tool to get better visibility into costs unfortunately the aws console and billing tools are rarely enough to give good visibility into costs for large accounts the aws billing console can time out or be too slow to use tools 🔹enable billing reports and install an open source tool to help manage or monitor aws resource utilization teevity ice originally written by netflix is probably the first one you should try check out docker ice for a dockerized version that eases installation 🔸one challenge with ice is that it doesnt cover amortized cost of reserved instances other tools include security monkey and cloud custodian third party services several companies offer services designed to help you gain insights into expenses or lower your aws bill such as opsclarity cloudability cloudhealth technologies and parkmycloud some of these charge a percentage of your bill which may be expensive see the market landscape awss trusted advisor is another service that can help with cost concerns dont be shy about asking your account manager for guidance in reducing your bill its their job to keep you happily using aws tagging for cost visibility as the infrastructure grows a key part of managing costs is understanding where they lie its strongly advisable to tag resources and as complexity grows group them effectively if you set up billing allocation appropriately you can then get visibility into expenses according to organization product individual engineer or any other way that is helpful if you need to do custom analysis of raw billing data or want to feed it to a third party cost analysis service enable the detailed billing report feature multiple amazon accounts can be linked for billing purposes using the consolidated billing feature large enterprises may need complex billing structures depending on ownership and approval processes multiple amazon accounts can be managed centrally using aws organizations 🔸 be aware that if an aws account has been created through the aws organizations console api or cli it can never leave that organization aws data transfer costs for deployments that involve significant network traffic a large fraction of aws expenses are around data transfer furthermore costs of data transfer within azs within regions between regions and into and out of aws and the internet vary significantly depending on deployment choices some of the most common gotchas 🔸az to az traffic note ec2 traffic between azs is effectively the same as between regions for example deploying a cassandra cluster across azs is helpful for high availability but can hurt on network costs 🔸using public ips when not necessary if you use an elastic ip or public ip address of an ec2 instance you will incur network costs even if it is accessed locally within the az this figure gives an overview ec2 cost management with ec2 there is a trade off between engineering effort more analysis more tools more complex architectures and spend rate on aws if your ec2 costs are small many of the efforts here are not worth the engineering time required to make them work but once you know your costs will be growing in excess of an engineers salary serious investment is often worthwhile larger instances arent necessarily priced higher in the spot market – therefore you should look at the available options and determine which instances will be most cost effective for your jobs see bid advisor 🔹spot instances ec2 spot instances are a way to get ec2 resources at significant discount — often many times cheaper than standard on demand prices — if youre willing to accept the possibility that they be terminated with little to no warning use spot instances for potentially very significant discounts whenever you can use resources that may be restarted and dont maintain long term state the huge savings that you can get with spot come at the cost of a significant increase in complexity when provisioning and reasoning about the availability of compute capacity amazon maintains spot prices at a market driven fluctuating level based on their inventory of unused capacity prices are typically low but can spike very high see the price history to get a sense for this you set a bid price high to indicate how high youre willing to pay but you only pay the going rate not the bid rate if the market rate exceeds the bid your instance may be terminated prices are per instance type and per availability zone the same instance type may have wildly different price in different zones at the same time different instance types can have very different prices even for similarly powered instance types in the same zone compare prices across instance types for better deals use spot instances whenever possible setting a high bid price will assure your machines stay up the vast majority of the time at a fraction of the price of normal instances get notified up to two minutes before price triggered shutdown by polling your spot instances metadata make sure your usage profile works well for spot before investing heavily in tools to manage a particular configuration spot fleet you can realize even bigger cost reductions at the same time as improvements to fleet stability relative to regular spot usage by using spot fleet to bid on instances across instance types availability zones and through multiple spot fleet requests regions spot fleet targets maintaining a specified and weighted by instance type total capacity across a cluster of servers if the spot price of one instance type and availability zone combination rises above the weighted bid it will rotate running instances out and bring up new ones of another type and location up in order to maintain the target capacity without going over target cluster cost spot usage best practices application profiling profile your application to figure out its runtime characteristics that would help give an understanding of the minimum cpu memory disk required having this information is critical before you try to optimize spot costs once you know the minimum application requirements instead of resorting to fixed instance types you can bid across a variety of instance types that gives you higher chances of getting a spot instance to run your application e g if you know that 4 cpu cores are enough for your job you can choose any instance type that is equal or above 4 cores and that has the least spot price based on history this helps you bid for instances with greater discount less demand at that point spot price monitoring and intelligence spot instance prices fluctuate depending on instance types time of day region and availability zone the aws cli tools and api allow you to describe spot price metadata given time instance type and region az based on history of spot instance prices you could potentially build a myriad of algorithms that would help you to pick an instance type in a way that optimizes cost maximizes availability or offers predictable performance you can also track the number of times an instance of certain type got taken away out bid and plot that in graphite to improve your algorithm based on time of day spot machine resource utilization for running spiky workloads spark map reduce jobs that are schedule based and where failure is non critical spot instances become the perfect candidates the time it takes to satisfy a spot instance could vary between 2 10 mins depending on the type of instance and availability of machines in that az if you are running an infrastructure with hundreds of jobs of spiky nature it is advisable to start pooling instances to optimize for cost performance and most importantly time to acquire an instance pooling implies creating and maintaining spot instances so that they do not get terminated after use this promotes re use of spot instances across jobs this of course comes with the overhead of lifecycle management pooling has its own set of metrics that can be tracked to optimize resource utilization efficiency and cost typical pooling implementations give anywhere between 45 60 cost optimizations and 40 reduction in spot instance creation time an excellent example of pooling implementation described by netflix part1 part2 spot management gotchas 🔸lifetime there is no guarantee for the lifetime of a spot instance it is purely based on bidding if anyone outbids your price the instance is taken away spot is not suitable for time sensitive jobs that have strong sla instances will fail based on demand for spot at that time aws provides a two minute warning before amazon ec2 must terminate your spot instance 🔹api return data the spot price api returns spot prices of varying granularity depending on the time range specified in the api call e g if the last 10 min worth of history is requested the data is more fine grained if the last 2 day worth of history is requested the data is more coarser do not assume you will get all the data points there will be skipped intervals ❗lifecycle management do not attempt any fancy spot management unless absolutely necessary if your entire usage is only a few machines and your cost is acceptable and your failure rate is lower do not attempt to optimize the pain for building maintaining it is not worth just a few hundred dollar savings reserved instances allow you to get significant discounts on ec2 compute hours in return for a commitment to pay for instance hours of a specific instance type in a specific aws region and availability zone for a pre established time frame 1 or 3 years further discounts can be realized through “partial” or “all upfront” payment options consider using reserved instances when you can predict your longer term compute needs and need a stronger guarantee of compute availability and continuity than the typically cheaper spot market can provide however be aware that if your architecture changes your computing needs may change as well so long term contracts can seem attractive but may turn out to be cumbersome there are two types of reserved instances standard and convertible if you purchase excess standard reserved instances you may offer to “sell back” unused reserved instances via the reserved instance marketplace this allows you to potentially recoup the cost of unused ec2 compute instance hours by selling them to other aws customers instance reservations are not tied to specific ec2 instances they are applied at the billing level to eligible compute hours as they are consumed across all of the instances in an account 📜there have been scattered reports of convertible ri purchases needing to be exercised in a block namely if you buy five convertible ris in one purchase you cant convert just two of them reach out to your account manager for clarification if this may impact you if you have multiple aws accounts and have configured them to roll charges up to one account using the “consolidated billing” feature you can expect unused reserved instance hours from one account to be applied to matching region availability zone instance type compute hours from another account if you have multiple aws accounts that are linked with consolidated billing plan on using reservations and want unused reservation capacity to be able to apply to compute hours from other accounts youll need to create your instances in the availability zone with the same name across accounts keep in mind that when you have done this your instances may not end up in the same physical data center across accounts amazon shuffles availability zones names across accounts in order to equalize resource utilization make use of dynamic auto scaling where possible in order to better match your cluster size and cost to the current resource requirements of your service if you use rhel instances and happen to have existing rhel on premise red hat subscriptions then you can leverage red hats cloud access program to migrate a portion of your on premise subscriptions to aws and thereby saving on aws charges for rhel subscriptions you can either use your own self created rhel amis or red hat provided gold images that will be added to your private amis once you sign up for red hat cloud access further reading this section covers a few unusually useful or “must know about” resources or lists aws aws in plain english a readable overview of all the aws services awesome aws a curated list of aws tools and software aws tips i wish id known before i started a list of tips from rich adams aws whitepapers a list of technical aws whitepapers covering topics such as architecture security and economics last week in aws a weekly email newsletter covering the latest happenings in the aws ecosystem aws geek a blog by jerry hargrove with notes and hand drawn diagrams about various aws services books amazon web services in action aws lambda in action serverless architectures on aws serverless single page apps the terraform book aws scripted 2 book series amazon web services for dummies aws system administration python and aws cookbook resilience and reliability on aws aws documentation as kindle ebooks general references aws well architected framework guide amazons own 56 page guide to operational excellence guidelines and checklists to validate baseline security reliability performance including high availability and cost optimization practices awesome microservices a curated list of tools and technologies for microservice architectures worth browsing to learn about popular open source projects is it fast yet ilya grigoriks tls performance overview high performance browser networking a full modern book on web network performance a presentation on the http 2 portion is here disclaimer the authors and contributors to this content cannot guarantee the validity of the information found here please make sure that you understand that the information provided here is being provided freely and that no kind of agreement or contract is created between you and any persons associated with this content or project the authors and contributors do not assume and hereby disclaim any liability to any party for any loss damage or disruption caused by errors or omissions in the information contained in associated with or linked from this content whether such errors or omissions result from negligence accident or any other cause license this work is licensed under a creative commons attribution sharealike 4 0 international license