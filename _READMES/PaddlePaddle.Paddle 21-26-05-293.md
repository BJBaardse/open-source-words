paddlepaddle welcome to the paddlepaddle github paddlepaddle parallel distributed deep learning is an easy to use efficient flexible and scalable deep learning platform which is originally developed by baidu scientists and engineers for the purpose of applying deep learning to many products at baidu our vision is to enable deep learning for everyone via paddlepaddle please refer to our release announcement to track the latest feature of paddlepaddle features flexibility paddlepaddle supports a wide range of neural network architectures and optimization algorithms it is easy to configure complex models such as neural machine translation model with attention mechanism or complex memory connection efficiency in order to unleash the power of heterogeneous computing resource optimization occurs at different levels of paddlepaddle including computing memory architecture and communication the following are some examples optimized math operations through sse avx intrinsics blas libraries e g mkl openblas cublas or customized cpu gpu kernels optimized cnn networks through mkl dnn library highly optimized recurrent networks which can handle variable length sequence without padding optimized local and distributed training for models with high dimensional sparse data scalability with paddlepaddle it is easy to use many cpus gpus and machines to speed up your training paddlepaddle can achieve high throughput and performance via optimized communication connected to products in addition paddlepaddle is also designed to be easily deployable at baidu paddlepaddle has been deployed into products and services with a vast number of users including ad click through rate ctr prediction large scale image classification optical character recognition ocr search ranking computer virus detection recommendation etc it is widely utilized in products at baidu and it has achieved a significant impact we hope you can also explore the capability of paddlepaddle to make an impact on your product installation it is recommended to check out the docker installation guide before looking into the build from source guide documentation we provide english and chinese documentation deep learning 101 you might want to start from this online interactive book that can run in a jupyter notebook distributed training you can run distributed training jobs on mpi clusters distributed training on kubernetes you can also run distributed training jobs on kubernetes clusters python api our new api enables much shorter programs how to contribute we appreciate your contributions ask questions you are welcome to submit questions and bug reports as github issues copyright and license paddlepaddle is provided under the apache 2 0 license