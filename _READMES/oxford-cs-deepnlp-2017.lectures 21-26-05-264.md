preamble this repository contains the lecture slides and course description for the deep natural language processing course offered in hilary term 2017 at the university of oxford this is an advanced course on natural language processing automatically processing natural language inputs and producing language outputs is a key component of artificial general intelligence the ambiguities and noise inherent in human communication render traditional symbolic ai techniques ineffective for representing and analysing language data recently statistical techniques based on neural networks have achieved a number of remarkable successes in natural language processing leading to a great deal of commercial and academic interest in the field this is an applied course focussing on recent advances in analysing and generating speech and text using recurrent neural networks we introduce the mathematical definitions of the relevant machine learning models and derive their associated optimisation algorithms the course covers a range of applications of neural networks in nlp including analysing latent dimensions in text transcribing speech to text translating between languages and answering questions these topics are organised into three high level themes forming a progression from understanding the use of neural networks for sequential language modelling to understanding their use as conditional language models for transduction tasks and finally to approaches employing these techniques in combination with other mechanisms for advanced applications throughout the course the practical implementation of such models on cpu and gpu hardware is also discussed this course is organised by phil blunsom and delivered in partnership with the deepmind natural language research group lecturers phil blunsom oxford university and deepmind chris dyer carnegie mellon university and deepmind edward grefenstette deepmind karl moritz hermann deepmind andrew senior deepmind wang ling deepmind jeremy appleyard nvidia tas yannis assael yishu miao brendan shillingford jan buys timetable practicals group 1 monday 9 00 11 00 weeks 2 8 60 05 thom building group 2 friday 16 00 18 00 weeks 2 8 room 379 practical 1 word2vec practical 2 text classification practical 3 recurrent neural networks for text classification and language modelling practical 4 open practical lectures public lectures are held in lecture theatre 1 of the maths institute on tuesdays and thursdays except week 8 16 00 18 00 hilary term weeks 1 3 8 lecture materials 1 lecture 1a introduction phil blunsom this lecture introduces the course and motivates why it is interesting to study language processing using deep learning techniques slides video 2 lecture 1b deep neural networks are our friends wang ling this lecture revises basic machine learning concepts that students should know before embarking on this course slides video 3 lecture 2a word level semantics ed grefenstette words are the core meaning bearing units in language representing and learning the meanings of words is a fundamental task in nlp and in this lecture the concept of a word embedding is introduced as a practical and scalable solution slides video reading embeddings basics firth john r a synopsis of linguistic theory 1930 1955 1957 1 32 curran james richard from distributional to semantic similarity 2004 collobert ronan et al natural language processing almost from scratch journal of machine learning research 12 aug 2011 2493 2537 mikolov tomas et al distributed representations of words and phrases and their compositionality advances in neural information processing systems 2013 datasets and visualisation finkelstein lev et al placing search in context the concept revisited proceedings of the 10th international conference on world wide web acm 2001 hill felix roi reichart and anna korhonen simlex 999 evaluating semantic models with genuine similarity estimation computational linguistics 2016 maaten laurens van der and geoffrey hinton visualizing data using t sne journal of machine learning research 9 nov 2008 2579 2605 blog posts deep learning nlp and representations christopher olah visualizing top tweeps with t sne in javascript andrej karpathy further reading hermann karl moritz and phil blunsom multilingual models for compositional distributed semantics arxiv preprint arxiv 1404 4641 2014 levy omer and yoav goldberg neural word embedding as implicit matrix factorization advances in neural information processing systems 2014 levy omer yoav goldberg and ido dagan improving distributional similarity with lessons learned from word embeddings transactions of the association for computational linguistics 3 2015 211 225 ling wang et al two too simple adaptations of word2vec for syntax problems hlt naacl 2015 4 lecture 2b overview of the practicals chris dyer this lecture motivates the practical segment of the course slides video 5 lecture 3 language modelling and rnns part 1 phil blunsom language modelling is important task of great practical use in many nlp applications this lecture introduces language modelling including traditional n gram based approaches and more contemporary neural approaches in particular the popular recurrent neural network rnn language model is introduced and its basic training and evaluation algorithms described slides video reading textbook deep learning chapter 10 blogs the unreasonable effectiveness of recurrent neural networks andrej karpathy the unreasonable effectiveness of character level language models yoav goldberg explaining and illustrating orthogonal initialization for recurrent neural networks stephen merity 6 lecture 4 language modelling and rnns part 2 phil blunsom this lecture continues on from the previous one and considers some of the issues involved in producing an effective implementation of an rnn language model the vanishing and exploding gradient problem is described and architectural solutions such as long short term memory lstm are introduced slides video reading textbook deep learning chapter 10 vanishing gradients lstms etc on the difficulty of training recurrent neural networks pascanu et al icml 2013 long short term memory hochreiter and schmidhuber neural computation 1997 learning phrase representations using rnn encoderdecoder for statistical machine translation cho et al emnlp 2014 blog understanding lstm networks christopher olah dealing with large vocabularies a scalable hierarchical distributed language model mnih and hinton nips 2009 a fast and simple algorithm for training neural probabilistic language models mnih and teh icml 2012 on using very large target vocabulary for neural machine translation jean et al acl 2015 exploring the limits of language modeling jozefowicz et al arxiv 2016 efficient softmax approximation for gpus grave et al arxiv 2016 notes on noise contrastive estimation and negative sampling dyer arxiv 2014 pragmatic neural language modelling in machine translation baltescu and blunsom naacl 2015 regularisation and dropout a theoretically grounded application of dropout in recurrent neural networks gal and ghahramani nips 2016 blog uncertainty in deep learning yarin gal other stuff recurrent highway networks zilly et al arxiv 2016 capacity and trainability in recurrent neural networks collins et al arxiv 2016 7 lecture 5 text classification karl moritz hermann this lecture discusses text classification beginning with basic classifiers such as naive bayes and progressing through to rnns and convolution networks slides video reading recurrent convolutional neural networks for text classification lai et al aaai 2015 a convolutional neural network for modelling sentences kalchbrenner et al acl 2014 semantic compositionality through recursive matrix vector socher et al emnlp 2012 blog understanding convolution neural networks for nlp denny britz thesis distributional representations for compositional semantics hermann 2014 8 lecture 6 deep nlp on nvidia gpus jeremy appleyard this lecture introduces graphical processing units gpus as an alternative to cpus for executing deep learning algorithms the strengths and weaknesses of gpus are discussed as well as the importance of understanding how memory bandwidth and computation impact throughput for rnns slides video reading optimizing performance of recurrent neural networks on gpus appleyard et al arxiv 2016 persistent rnns stashing recurrent weights on chip diamos et al icml 2016 efficient softmax approximation for gpus grave et al arxiv 2016 9 lecture 7 conditional language models chris dyer in this lecture we extend the concept of language modelling to incorporate prior information by conditioning an rnn language model on an input representation we can generate contextually relevant language this very general idea can be applied to transduce sequences into new sequences for tasks such as translation and summarisation or images into captions describing their content slides video reading recurrent continuous translation models kalchbrenner and blunsom emnlp 2013 sequence to sequence learning with neural networks sutskever et al nips 2014 multimodal neural language models kiros et al icml 2014 show and tell a neural image caption generator vinyals et al cvpr 2015 10 lecture 8 generating language with attention chris dyer this lecture introduces one of the most important and influencial mechanisms employed in deep neural networks attention attention augments recurrent networks with the ability to condition on specific parts of the input and is key to achieving high performance in tasks such as machine translation and image captioning slides video reading neural machine translation by jointly learning to align and translate bahdanau et al iclr 2015 show attend and tell neural image caption generation with visual attention xu et al icml 2015 incorporating structural alignment biases into an attentional neural translation model cohn et al naacl 2016 bleu a method for automatic evaluation of machine translation papineni et al acl 2002 11 lecture 9 speech recognition asr andrew senior automatic speech recognition asr is the task of transducing raw audio signals of spoken language into text transcriptions this talk covers the history of asr models from gaussian mixtures to attention augmented rnns the basic linguistics of speech and the various input and output representations frequently employed slides video 12 lecture 10 text to speech tts andrew senior this lecture introduces algorithms for converting written language into spoken language text to speech tts is the inverse process to asr but there are some important differences in the models applied here we review traditional tts models and then cover more recent neural approaches such as deepminds wavenet model slides video 13 lecture 11 question answering karl moritz hermann slides video reading teaching machines to read and comprehend hermann et al nips 2015 deep learning for answer sentence selection yu et al nips deep learning workshop 2014 14 lecture 12 memory ed grefenstette slides video reading hybrid computing using a neural network with dynamic external memory graves et al nature 2016 reasoning about entailment with neural attention rocktäschel et al iclr 2016 learning to transduce with unbounded memory grefenstette et al nips 2015 end to end memory networks sukhbaatar et al nips 2015 15 lecture 13 linguistic knowledge in neural networks slides video piazza we will be using piazza to facilitate class discussion during the course rather than emailing questions directly i encourage you to post your questions on piazza to be answered by your fellow students instructors and lecturers however do please do note that all the lecturers for this course are volunteering their time and may not always be available to give a response find our class page at https piazza com ox ac uk winter2017 dnlpht2017 home assessment the primary assessment for this course will be a take home assignment issued at the end of the term this assignment will ask questions drawing on the concepts and models discussed in the course as well as from selected research publications the nature of the questions will include analysing mathematical descriptions of models and proposing extensions improvements or evaluations to such models the assignment may also ask students to read specific research publications and discuss their proposed algorithms in the context of the course in answering questions students will be expected to both present coherent written arguments and use appropriate mathematical formulae and possibly pseudo code to illustrate answers the practical component of the course will be assessed in the usual way acknowledgements this course would not have been possible without the support of deepmind the university of oxford department of computer science nvidia and the generous donation of gpu resources from microsoft azure