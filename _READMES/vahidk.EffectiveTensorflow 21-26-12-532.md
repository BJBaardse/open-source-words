effective tensorflow table of contents tensorflow basics understanding static and dynamic shapes scopes and when to use them broadcasting the good and the ugly feeding data to tensorflow take advantage of the overloaded operators understanding order of execution and control dependencies control flow operations conditionals and loops prototyping kernels and advanced visualization with python ops multi gpu processing with data parallelism debugging tensorflow models numerical stability in tensorflow building a neural network training framework with learn api tensorflow cookbook get shape batch gather beam search merge entropy kl divergence make parallel leaky relu batch normalization we aim to gradually expand this series by adding new articles and keep the content up to date with the latest releases of tensorflow api if you have suggestions on how to improve this series or find the explanations ambiguous feel free to create an issue send patches or reach out by email we encourage you to also check out the accompanied neural network training framework built on top of tf contrib learn api the framework can be downloaded separately git clone https github com vahidk tensorflowframework git tensorflow basics the most striking difference between tensorflow and other numerical computation libraries such as numpy is that operations in tensorflow are symbolic this is a powerful concept that allows tensorflow to do all sort of things e g automatic differentiation that are not possible with imperative libraries such as numpy but it also comes at the cost of making it harder to grasp our attempt here is to demystify tensorflow and provide some guidelines and best practices for more effective use of tensorflow lets start with a simple example we want to multiply two random matrices first we look at an implementation done in numpy python import numpy as np x np random normal size 10 10 y np random normal size 10 10 z np dot x y print z now we perform the exact same computation this time in tensorflow python import tensorflow as tf x tf random normal 10 10 y tf random normal 10 10 z tf matmul x y sess tf session z val sess run z print z val unlike numpy that immediately performs the computation and produces the result tensorflow only gives us a handle of type tensor to a node in the graph that represents the result if we try printing the value of z directly we get something like this tensor matmul 0 shape 10 10 dtype float32 since both the inputs have a fully defined shape tensorflow is able to infer the shape of the tensor as well as its type in order to compute the value of the tensor we need to create a session and evaluate it using session run method tip when using jupyter notebook make sure to call tf reset default graph at the beginning to clear the symbolic graph before defining new nodes to understand how powerful symbolic computation can be lets have a look at another example assume that we have samples from a curve say f x 5x 2 3 and we want to estimate f x based on these samples we define a parametric function g x w w0 x 2 w1 x w2 which is a function of the input x and latent parameters w our goal is then to find the latent parameters such that g x w ≈ f x this can be done by minimizing the following loss function l w ∑ f x g x w 2 although theres a closed form solution for this simple problem we opt to use a more general approach that can be applied to any arbitrary differentiable function and that is using stochastic gradient descent we simply compute the average gradient of l w with respect to w over a set of sample points and move in the opposite direction heres how it can be done in tensorflow python import numpy as np import tensorflow as tf placeholders are used to feed values from python to tensorflow ops we define two placeholders one for input feature x and one for output y x tf placeholder tf float32 y tf placeholder tf float32 assuming we know that the desired function is a polynomial of 2nd degree we allocate a vector of size 3 to hold the coefficients the variable will be automatically initialized with random noise w tf get variable w shape 3 1 we define yhat to be our estimate of y f tf stack tf square x x tf ones like x 1 yhat tf squeeze tf matmul f w 1 the loss is defined to be the l2 distance between our estimate of y and its true value we also added a shrinkage term to ensure the resulting weights would be small loss tf nn l2 loss yhat y 0 1 tf nn l2 loss w we use the adam optimizer with learning rate set to 0 1 to minimize the loss train op tf train adamoptimizer 0 1 minimize loss def generate data x val np random uniform 10 0 10 0 size 100 y val 5 np square x val 3 return x val y val sess tf session since we are using variables we first need to initialize them sess run tf global variables initializer for in range 1000 x val y val generate data loss val sess run train op loss x x val y y val print loss val print sess run w by running this piece of code you should see a result close to this 4 9924135 0 00040895029 3 4504161 which is a relatively close approximation to our parameters this is just tip of the iceberg for what tensorflow can do many problems such as optimizing large neural networks with millions of parameters can be implemented efficiently in tensorflow in just a few lines of code tensorflow takes care of scaling across multiple devices and threads and supports a variety of platforms understanding static and dynamic shapes tensors in tensorflow have a static shape attribute which is determined during graph construction the static shape may be underspecified for example we might define a tensor of shape none 128 python import tensorflow as tf a tf placeholder tf float32 none 128 this means that the first dimension can be of any size and will be determined dynamically during session run you can query the static shape of a tensor as follows python static shape a shape as list returns none 128 to get the dynamic shape of the tensor you can call tf shape op which returns a tensor representing the shape of the given tensor python dynamic shape tf shape a the static shape of a tensor can be set with tensor set shape method python a set shape 32 128 static shape of a is 32 128 a set shape none 128 first dimension of a is determined dynamically you can reshape a given tensor dynamically using tf reshape function python a tf reshape a 32 128 it can be convenient to have a function that returns the static shape when available and dynamic shape when its not the following utility function does just that python def get shape tensor static shape tensor shape as list dynamic shape tf unstack tf shape tensor dims s 1 if s 0 is none else s 0 for s in zip static shape dynamic shape return dims now imagine we want to convert a tensor of rank 3 to a tensor of rank 2 by collapsing the second and third dimensions into one we can use our get shape function to do that python b tf placeholder tf float32 none 10 32 shape get shape b b tf reshape b shape 0 shape 1 shape 2 note that this works whether the shapes are statically specified or not in fact we can write a general purpose reshape function to collapse any list of dimensions python import tensorflow as tf import numpy as np def reshape tensor dims list shape get shape tensor dims prod for dims in dims list if isinstance dims int dims prod append shape dims elif all isinstance shape d int for d in dims dims prod append np prod shape d for d in dims else dims prod append tf prod shape d for d in dims tensor tf reshape tensor dims prod return tensor then collapsing the second dimension becomes very easy python b tf placeholder tf float32 none 10 32 b reshape b 0 1 2 scopes and when to use them variables and tensors in tensorflow have a name attribute that is used to identify them in the symbolic graph if you dont specify a name when creating a variable or a tensor tensorflow automatically assigns a name for you python a tf constant 1 print a name prints const 0 b tf variable 1 print b name prints variable 0 you can overwrite the default name by explicitly specifying it python a tf constant 1 name a print a name prints a 0 b tf variable 1 name b print b name prints b 0 tensorflow introduces two different context managers to alter the name of tensors and variables the first is tf name scope python with tf name scope scope a tf constant 1 name a print a name prints scope a 0 b tf variable 1 name b print b name prints scope b 0 c tf get variable name c shape print c name prints c 0 note that there are two ways to define new variables in tensorflow by creating a tf variable object or by calling tf get variable calling tf get variable with a new name results in creating a new variable but if a variable with the same name exists it will raise a valueerror exception telling us that re declaring a variable is not allowed tf name scope affects the name of tensors and variables created with tf variable but doesnt impact the variables created with tf get variable unlike tf name scope tf variable scope modifies the name of variables created with tf get variable as well python with tf variable scope scope a tf constant 1 name a print a name prints scope a 0 b tf variable 1 name b print b name prints scope b 0 c tf get variable name c shape print c name prints scope c 0 python with tf variable scope scope a1 tf get variable name a shape a2 tf get variable name a shape disallowed but what if we actually want to reuse a previously declared variable variable scopes also provide the functionality to do that python with tf variable scope scope a1 tf get variable name a shape with tf variable scope scope reuse true a2 tf get variable name a shape ok this becomes handy for example when using built in neural network layers python with tf variable scope my scope features1 tf layers conv2d image1 filters 32 kernel size 3 use the same convolution weights to process the second image with tf variable scope my scope reuse true features2 tf layers conv2d image2 filters 32 kernel size 3 alternatively you can set reuse to tf auto reuse which tells tensorflow to create a new variable if a variable with the same name doesnt exist and reuse otherwise python with tf variable scope scope reuse tf auto reuse features1 tf layers conv2d image1 filters 32 kernel size 3 with tf variable scope scope reuse tf auto reuse features2 tf layers conv2d image2 filters 32 kernel size 3 if you want to do lots of variable sharing keeping track of when to define new variables and when to reuse them can be cumbersome and error prone tf auto reuse simplifies this task but adds the risk of sharing variables that werent supposed to be shared tensorflow templates are another way of tackling the same problem without this risk python conv3x32 tf make template conv3x32 lambda x tf layers conv2d x 32 3 features1 conv3x32 image1 features2 conv3x32 image2 will reuse the convolution weights you can turn any function to a tensorflow template upon the first call to a template the variables defined inside the function would be declared and in the consecutive invocations they would automatically get reused broadcasting the good and the ugly tensorflow supports broadcasting elementwise operations normally when you want to perform operations like addition and multiplication you need to make sure that shapes of the operands match e g you cant add a tensor of shape 3 2 to a tensor of shape 3 4 but theres a special case and thats when you have a singular dimension tensorflow implicitly tiles the tensor across its singular dimensions to match the shape of the other operand so its valid to add a tensor of shape 3 2 to a tensor of shape 3 1 python import tensorflow as tf a tf constant 1 2 3 4 b tf constant 1 2 c a tf tile b 1 2 c a b broadcasting allows us to perform implicit tiling which makes the code shorter and more memory efficient since we dont need to store the result of the tiling operation one neat place that this can be used is when combining features of varying length in order to concatenate features of varying length we commonly tile the input tensors concatenate the result and apply some nonlinearity this is a common pattern across a variety of neural network architectures python a tf random uniform 5 3 5 b tf random uniform 5 1 6 concat a and b and apply nonlinearity tiled b tf tile b 1 3 1 c tf concat a tiled b 2 d tf layers dense c 10 activation tf nn relu but this can be done more efficiently with broadcasting we use the fact that f m x y is equal to f mx my so we can do the linear operations separately and use broadcasting to do implicit concatenation python pa tf layers dense a 10 activation none pb tf layers dense b 10 activation none d tf nn relu pa pb in fact this piece of code is pretty general and can be applied to tensors of arbitrary shape as long as broadcasting between tensors is possible python def merge a b units activation tf nn relu pa tf layers dense a units activation none pb tf layers dense b units activation none c pa pb if activation is not none c activation c return c a slightly more general form of this function is included in the cookbook so far we discussed the good part of broadcasting but whats the ugly part you may ask implicit assumptions almost always make debugging harder to do consider the following example python a tf constant 1 2 b tf constant 1 2 c tf reduce sum a b what do you think the value of c would be after evaluation if you guessed 6 thats wrong its going to be 12 this is because when rank of two tensors dont match tensorflow automatically expands the first dimension of the tensor with lower rank before the elementwise operation so the result of addition would be 2 3 3 4 and the reducing over all parameters would give us 12 the way to avoid this problem is to be as explicit as possible had we specified which dimension we would want to reduce across catching this bug would have been much easier python a tf constant 1 2 b tf constant 1 2 c tf reduce sum a b 0 here the value of c would be 5 7 and we immediately would guess based on the shape of the result that theres something wrong a general rule of thumb is to always specify the dimensions in reduction operations and when using tf squeeze feeding data to tensorflow tensorflow is designed to work efficiently with large amount of data so its important not to starve your tensorflow model in order to maximize its performance there are various ways that you can feed your data to tensorflow constants the simplest approach is to embed the data in your graph as a constant python import tensorflow as tf import numpy as np actual data np random normal size 100 data tf constant actual data this approach can be very efficient but its not very flexible one problem with this approach is that in order to use your model with another dataset you have to rewrite the graph also you have to load all of your data at once and keep it in memory which would only work with small datasets placeholders using placeholders solves both of these problems python import tensorflow as tf import numpy as np data tf placeholder tf float32 prediction tf square data 1 actual data np random normal size 100 tf session run prediction feed dict data actual data placeholder operator returns a tensor whose value is fetched through the feed dict argument in session run function note that running session run without feeding the value of data in this case will result in an error python ops another approach to feed the data to tensorflow is by using python ops python def py input fn actual data np random normal size 100 return actual data data tf py func py input fn tf float32 python ops allow you to convert a regular python function to a tensorflow operation dataset api the recommended way of reading the data in tensorflow however is through the dataset api python actual data np random normal size 100 dataset tf contrib data dataset from tensor slices actual data data dataset make one shot iterator get next if you need to read your data from file it may be more efficient to write it in tfrecord format and use tfrecorddataset to read it python dataset tf contrib data tfrecorddataset path to data see the official docs for an example of how to write your dataset in tfrecord format dataset api allows you to make efficient data processing pipelines easily for example this is how we process our data in the accompanied framework see trainer py python dataset dataset dataset cache if mode tf estimator modekeys train dataset dataset repeat dataset dataset shuffle batch size 5 dataset dataset map parse num threads 8 dataset dataset batch batch size after reading the data we use dataset cache method to cache it into memory for improved efficiency during the training mode we repeat the dataset indefinitely this allows us to process the whole dataset many times we also shuffle the dataset to get batches with different sample distributions next we use the dataset map function to perform preprocessing on raw records and convert the data to a usable format for the model we then create batches of samples by calling dataset batch take advantage of the overloaded operators just like numpy tensorflow overloads a number of python operators to make building graphs easier and the code more readable the slicing op is one of the overloaded operators that can make indexing tensors very easy python z x begin end z tf slice x begin end begin be very careful when using this op though the slicing op is very inefficient and often better avoided especially when the number of slices is high to understand how inefficient this op can be lets look at an example we want to manually perform reduction across the rows of a matrix python import tensorflow as tf import time x tf random uniform 500 10 z tf zeros 10 for i in range 500 z x i sess tf session start time time sess run z print took f seconds time time start on my macbook pro this took 2 67 seconds to run the reason is that we are calling the slice op 500 times which is going to be very slow to run a better choice would have been to use tf unstack op to slice the matrix into a list of vectors all at once python z tf zeros 10 for x i in tf unstack x z x i this took 0 18 seconds of course the right way to do this simple reduction is to use tf reduce sum op python z tf reduce sum x axis 0 this took 0 008 seconds which is 300x faster than the original implementation tensorflow also overloads a range of arithmetic and logical operators python z x z tf negative x z x y z tf add x y z x y z tf subtract x y z x y z tf mul x y z x y z tf div x y z x y z tf floordiv x y z x y z tf mod x y z x y z tf pow x y z x y z tf matmul x y z x y z tf greater x y z x y z tf greater equal x y z x y z tf less x y z x y z tf less equal x y z abs x z tf abs x z x y z tf logical and x y z x y z tf logical or x y z x y z tf logical xor x y z x z tf logical not x you can also use the augmented version of these ops for example x y and x 2 are also valid note that python doesnt allow overloading and or and not keywords tensorflow also doesnt allow using tensors as booleans as it may be error prone python x tf constant 1 if x this will raise a typeerror error you can either use tf cond x if you want to check the value of the tensor or use if x is none to check the value of the variable other operators that arent supported are equal and not equal operators which are overloaded in numpy but not in tensorflow use the function versions instead which are tf equal and tf not equal understanding order of execution and control dependencies as we discussed in the first item tensorflow doesnt immediately run the operations that are defined but rather creates corresponding nodes in a graph that can be evaluated with session run method this also enables tensorflow to do optimizations at run time to determine the optimal order of execution and possible trimming of unused nodes if you only have tf tensors in your graph you dont need to worry about dependencies but you most probably have tf variables too and tf variables make things much more difficult my advice to is to only use variables if tensors dont do the job this might not make a lot of sense to you now so lets start with an example python import tensorflow as tf a tf constant 1 b tf constant 2 a a b tf session run a evaluating a will return the value 3 as expected note that here we are creating 3 tensors two constant tensors and another tensor that stores the result of the addition note that you cant overwrite the value of a tensor if you want to modify it you have to create a new tensor as we did here tip if you dont define a new graph tensorflow automatically creates a graph for you by default you can use tf get default graph to get a handle to the graph you can then inspect the graph for example by printing all its tensors python print tf contrib graph editor get tensors tf get default graph unlike tensors variables can be updated so lets see how we may use variables to do the same thing python a tf variable 1 b tf constant 2 assign tf assign a a b sess tf session sess run tf global variables initializer print sess run assign again we get 3 as expected note that tf assign returns a tensor representing the value of the assignment so far everything seemed to be fine but lets look at a slightly more complicated example python a tf variable 1 b tf constant 2 c a b assign tf assign a 5 sess tf session for i in range 10 sess run tf global variables initializer print sess run assign c note that the tensor c here wont have a deterministic value this value might be 3 or 7 depending on whether addition or assignment gets executed first you should note that the order that you define ops in your code doesnt matter to tensorflow runtime the only thing that matters is the control dependencies control dependencies for tensors are straightforward every time you use a tensor in an operation that op will define an implicit dependency to that tensor but things get complicated with variables because they can take many values when dealing with variables you may need to explicitly define dependencies using tf control dependencies as follows python a tf variable 1 b tf constant 2 c a b with tf control dependencies c assign tf assign a 5 sess tf session for i in range 10 sess run tf global variables initializer print sess run assign c this will make sure that the assign op will be called after the addition control flow operations conditionals and loops when building complex models such as recurrent neural networks you may need to control the flow of operations through conditionals and loops in this section we introduce a number of commonly used control flow ops lets assume you want to decide whether to multiply to or add two given tensors based on a predicate this can be simply implemented with tf cond which acts as a python if function python a tf constant 1 b tf constant 2 p tf constant true x tf cond p lambda a b lambda a b print tf session run x since the predicate is true in this case the output would be the result of the addition which is 3 most of the times when using tensorflow you are using large tensors and want to perform operations in batch a related conditional operation is tf where which like tf cond takes a predicate but selects the output based on the condition in batch python a tf constant 1 1 b tf constant 2 2 p tf constant true false x tf where p a b a b print tf session run x this will return 3 2 another widely used control flow operation is tf while loop it allows building dynamic loops in tensorflow that operate on sequences of variable length lets see how we can generate fibonacci sequence with tf while loops python n tf constant 5 def cond i a b return i n def body i a b return i 1 b a b i a b tf while loop cond body 2 1 1 print tf session run b this will print 5 tf while loops takes a condition function and a loop body function in addition to initial values for loop variables these loop variables are then updated by multiple calls to the body function until the condition returns false now imagine we want to keep the whole series of fibonacci sequence we may update our body to keep a record of the history of current values python n tf constant 5 def cond i a b c return i n def body i a b c return i 1 b a b tf concat c a b 0 i a b c tf while loop cond body 2 1 1 tf constant 1 1 print tf session run c now if you try running this tensorflow will complain that the shape of the the fourth loop variable is changing so you must make that explicit that its intentional i a b c tf while loop cond body 2 1 1 tf constant 1 1 shape invariants tf tensorshape tf tensorshape tf tensorshape tf tensorshape none this is not only getting ugly but is also somewhat inefficient note that we are building a lot of intermediary tensors that we dont use tensorflow has a better solution for this kind of growing arrays meet tf tensorarray lets do the same thing this time with tensor arrays python n tf constant 5 c tf tensorarray tf int32 n c c write 0 1 c c write 1 1 def cond i a b c return i n def body i a b c c c write i a b return i 1 b a b c i a b c tf while loop cond body 2 1 1 c c c stack print tf session run c tensorflow while loops and tensor arrays are essential tools for building complex recurrent neural networks as an exercise try implementing beam search using tf while loops can you make it more efficient with tensor arrays prototyping kernels and advanced visualization with python ops operation kernels in tensorflow are entirely written in c for efficiency but writing a tensorflow kernel in c can be quite a pain so before spending hours implementing your kernel you may want to prototype something quickly however inefficient with tf py func you can turn any piece of python code to a tensorflow operation for example this is how you can implement a simple relu nonlinearity kernel in tensorflow as a python op python import numpy as np import tensorflow as tf import uuid def relu inputs define the op in python def relu x return np maximum x 0 define the ops gradient in python def relu grad x return np float32 x 0 an adapter that defines a gradient op compatible with tensorflow def relu grad op op grad x op inputs 0 x grad grad tf py func relu grad x tf float32 return x grad register the gradient with a unique id grad name myrelugrad str uuid uuid4 tf registergradient grad name relu grad op override the gradient of the custom op g tf get default graph with g gradient override map pyfunc grad name output tf py func relu inputs tf float32 return output to verify that the gradients are correct you can use tensorflows gradient checker python x tf random normal 10 y relu x x with tf session diff tf test compute gradient error x 10 y 10 print diff compute gradient error computes the gradient numerically and returns the difference with the provided gradient what we want is a very low difference note that this implementation is pretty inefficient and is only useful for prototyping since the python code is not parallelizable and wont run on gpu once you verified your idea you definitely would want to write it as a c kernel in practice we commonly use python ops to do visualization on tensorboard consider the case that you are building an image classification model and want to visualize your model predictions during training tensorflow allows visualizing images with tf summary image function python image tf placeholder tf float32 tf summary image image image but this only visualizes the input image in order to visualize the predictions you have to find a way to add annotations to the image which may be almost impossible with existing ops an easier way to do this is to do the drawing in python and wrap it in a python op python import io import matplotlib pyplot as plt import numpy as np import pil import tensorflow as tf def visualize labeled images images labels max outputs 3 name image def visualize image image label do the actual drawing in python fig plt figure figsize 3 3 dpi 80 ax fig add subplot 111 ax imshow image 1 ax text 0 0 str label horizontalalignment left verticalalignment top fig canvas draw write the plot as a memory file buf io bytesio data fig savefig buf format png buf seek 0 read the image and convert to numpy array img pil image open buf return np array img getdata reshape img size 0 img size 1 1 def visualize images images labels only display the given number of examples in the batch outputs for i in range max outputs output visualize image images i labels i outputs append output return np array outputs dtype np uint8 run the python op figs tf py func visualize images images labels tf uint8 return tf summary image name figs note that since summaries are usually only evaluated once in a while not per step this implementation may be used in practice without worrying about efficiency multi gpu processing with data parallelism if you write your software in a language like c for a single cpu core making it run on multiple gpus in parallel would require rewriting the software from scratch but this is not the case with tensorflow because of its symbolic nature tensorflow can hide all that complexity making it effortless to scale your program across many cpus and gpus lets start with the simple example of adding two vectors on cpu python import tensorflow as tf with tf device tf devicespec device type cpu device index 0 a tf random uniform 1000 100 b tf random uniform 1000 100 c a b tf session run c the same thing can as simply be done on gpu python with tf device tf devicespec device type gpu device index 0 a tf random uniform 1000 100 b tf random uniform 1000 100 c a b but what if we have two gpus and want to utilize both to do that we can split the data and use a separate gpu for processing each half python split a tf split a 2 split b tf split b 2 split c for i in range 2 with tf device tf devicespec device type gpu device index i split c append split a i split b i c tf concat split c axis 0 lets rewrite this in a more general form so that we can replace addition with any other set of operations python def make parallel fn num gpus kwargs in splits for k v in kwargs items in splits k tf split v num gpus out split for i in range num gpus with tf device tf devicespec device type gpu device index i with tf variable scope tf get variable scope reuse tf auto reuse out split append fn k v i for k v in in splits items return tf concat out split axis 0 def model a b return a b c make parallel model 2 a a b b you can replace the model with any function that takes a set of tensors as input and returns a tensor as result with the condition that both the input and output are in batch note that we also added a variable scope and set the reuse to true this makes sure that we use the same variables for processing both splits this is something that will become handy in our next example lets look at a slightly more practical example we want to train a neural network on multiple gpus during training we not only need to compute the forward pass but also need to compute the backward pass the gradients but how can we parallelize the gradient computation this turns out to be pretty easy recall from the first item that we wanted to fit a second degree polynomial to a set of samples we reorganized the code a bit to have the bulk of the operations in the model function python import numpy as np import tensorflow as tf def model x y w tf get variable w shape 3 1 f tf stack tf square x x tf ones like x 1 yhat tf squeeze tf matmul f w 1 loss tf square yhat y return loss x tf placeholder tf float32 y tf placeholder tf float32 loss model x y train op tf train adamoptimizer 0 1 minimize tf reduce mean loss def generate data x val np random uniform 10 0 10 0 size 100 y val 5 np square x val 3 return x val y val sess tf session sess run tf global variables initializer for in range 1000 x val y val generate data loss val sess run train op loss x x val y y val loss val sess run train op loss x x val y y val print sess run tf contrib framework get variables by name w now lets use make parallel that we just wrote to parallelize this we only need to change two lines of code from the above code python loss make parallel model 2 x x y y train op tf train adamoptimizer 0 1 minimize tf reduce mean loss colocate gradients with ops true the only thing that we need to change to parallelize backpropagation of gradients is to set the colocate gradients with ops flag to true this ensures that gradient ops run on the same device as the original op debugging tensorflow models symbolic nature of tensorflow makes it relatively more difficult to debug tensorflow code compared to regular python code here we introduce a number of tools included with tensorflow that make debugging much easier probably the most common error one can make when using tensorflow is passing tensors of wrong shape to ops many tensorflow ops can operate on tensors of different ranks and shapes this can be convenient when using the api but may lead to extra headache when things go wrong for example consider the tf matmul op it can multiply two matrices python a tf random uniform 2 3 b tf random uniform 3 4 c tf matmul a b c is a tensor of shape 2 4 but the same function also does batch matrix multiplication python a tf random uniform 10 2 3 b tf random uniform 10 3 4 tf matmul a b c is a tensor of shape 10 2 4 another example that we talked about before in the broadcasting section is add operation which supports broadcasting python a tf constant 1 2 b tf constant 1 2 c a b c is a tensor of shape 2 2 validating your tensors with tf assert ops one way to reduce the chance of unwanted behavior is to explicitly verify the rank or shape of intermediate tensors with tf assert ops python a tf constant 1 2 b tf constant 1 2 check a tf assert rank a 1 this will raise an invalidargumenterror exception check b tf assert rank b 1 with tf control dependencies check a check b c a b c is a tensor of shape 2 2 remember that assertion nodes like other operations are part of the graph and if not evaluated would get pruned during session run so make sure to create explicit dependencies to assertion ops to force tensorflow to execute them you can also use assertions to validate the value of tensors at runtime python check pos tf assert positive a see the official docs for a full list of assertion ops logging tensor values with tf print another useful built in function for debugging is tf print which logs the given tensors to the standard error python input copy tf print input tensors to print list note that tf print returns a copy of its first argument as output one way to force tf print to run is to pass its output to another op that gets executed for example if we want to print the value of tensors a and b before adding them we could do something like this python a b a tf print a a b c a b alternatively we could manually define a control dependency check your gradients with tf compute gradient error not all the operations in tensorflow come with gradients and its easy to unintentionally build graphs for which tensorflow can not compute the gradients lets look at an example python import tensorflow as tf def non differentiable softmax entropy logits probs tf nn softmax logits return tf nn softmax cross entropy with logits labels probs logits logits w tf get variable w shape 5 y non differentiable softmax entropy w opt tf train adamoptimizer train op opt minimize y sess tf session sess run tf global variables initializer for i in range 10000 sess run train op print sess run tf nn softmax w we are using tf nn softmax cross entropy with logits to define entropy over a categorical distribution we then use adam optimizer to find the weights with maximum entropy if you have passed a course on information theory you would know that uniform distribution contains maximum entropy so you would expect for the result to be 0 2 0 2 0 2 0 2 0 2 but if you run this you may get unexpected results like this 0 34081486 0 24287023 0 23465775 0 08935683 0 09230034 it turns out tf nn softmax cross entropy with logits has undefined gradients with respect to labels but how may we spot this if we didnt know fortunately for us tensorflow comes with a numerical differentiator that can be used to find symbolic gradient errors lets see how we can use it python with tf session diff tf test compute gradient error w 5 y print diff if you run this you would see that the difference between the numerical and symbolic gradients are pretty high 0 06 0 1 in my tries now lets fix our function with a differentiable version of the entropy and check again python import tensorflow as tf import numpy as np def softmax entropy logits dim 1 plogp tf nn softmax logits dim tf nn log softmax logits dim return tf reduce sum plogp dim w tf get variable w shape 5 y softmax entropy w print w get shape print y get shape with tf session as sess diff tf test compute gradient error w 5 y print diff the difference should be 0 0001 which looks much better now if you run the optimizer again with the correct version you can see the final weights would be 0 2 0 2 0 2 0 2 0 2 which are exactly what we wanted tensorflow summaries and tfdbg tensorflow debugger are other tools that can be used for debugging please refer to the official docs to learn more numerical stability in tensorflow when using any numerical computation library such as numpy or tensorflow its important to note that writing mathematically correct code doesnt necessarily lead to correct results you also need to make sure that the computations are stable lets start with a simple example from primary school we know that x y y is equal to x for any non zero value of x but lets see if thats always true in practice python import numpy as np x np float32 1 y np float32 1e 50 y would be stored as zero z x y y print z prints nan the reason for the incorrect result is that y is simply too small for float32 type a similar problem occurs when y is too large python y np float32 1e39 y would be stored as inf z x y y print z prints 0 the smallest positive value that float32 type can represent is 1 4013e 45 and anything below that would be stored as zero also any number beyond 3 40282e 38 would be stored as inf python print np nextafter np float32 0 np float32 1 prints 1 4013e 45 print np finfo np float32 max print 3 40282e 38 to make sure that your computations are stable you want to avoid values with small or very large absolute value this may sound very obvious but these kind of problems can become extremely hard to debug especially when doing gradient descent in tensorflow this is because you not only need to make sure that all the values in the forward pass are within the valid range of your data types but also you need to make sure of the same for the backward pass during gradient computation lets look at a real example we want to compute the softmax over a vector of logits a naive implementation would look something like this python import tensorflow as tf def unstable softmax logits exp tf exp logits return exp tf reduce sum exp tf session run unstable softmax 1000 0 prints nan 0 note that computing the exponential of logits for relatively small numbers results to gigantic results that are out of float32 range the largest valid logit for our naive softmax implementation is ln 3 40282e 38 88 7 anything beyond that leads to a nan outcome but how can we make this more stable the solution is rather simple its easy to see that exp x c ∑ exp x c exp x ∑ exp x therefore we can subtract any constant from the logits and the result would remain the same we choose this constant to be the maximum of logits this way the domain of the exponential function would be limited to inf 0 and consequently its range would be 0 0 1 0 which is desirable python import tensorflow as tf def softmax logits exp tf exp logits tf reduce max logits return exp tf reduce sum exp tf session run softmax 1000 0 prints 1 0 lets look at a more complicated case consider we have a classification problem we use the softmax function to produce probabilities from our logits we then define our loss function to be the cross entropy between our predictions and the labels recall that cross entropy for a categorical distribution can be simply defined as xe p q ∑ p i log q i so a naive implementation of the cross entropy would look like this python def unstable softmax cross entropy labels logits logits tf log softmax logits return tf reduce sum labels logits labels tf constant 0 5 0 5 logits tf constant 1000 0 xe unstable softmax cross entropy labels logits print tf session run xe prints inf note that in this implementation as the softmax output approaches zero the logs output approaches infinity which causes instability in our computation we can rewrite this by expanding the softmax and doing some simplifications python def softmax cross entropy labels logits scaled logits logits tf reduce max logits normalized logits scaled logits tf reduce logsumexp scaled logits return tf reduce sum labels normalized logits labels tf constant 0 5 0 5 logits tf constant 1000 0 xe softmax cross entropy labels logits print tf session run xe prints 500 0 we can also verify that the gradients are also computed correctly python g tf gradients xe logits print tf session run g prints 0 5 0 5 which is correct let me remind again that extra care must be taken when doing gradient descent to make sure that the range of your functions as well as the gradients for each layer are within a valid range exponential and logarithmic functions when used naively are especially problematic because they can map small numbers to enormous ones and the other way around building a neural network training framework with learn api for simplicity in most of the examples here we manually create sessions and we dont care about saving and loading checkpoints but this is not how we usually do things in practice you most probably want to use the learn api to take care of session management and logging we provide a simple but practical framework for training neural networks using tensorflow in this item we explain how this framework works when experimenting with neural network models you usually have a training test split you want to train your model on the training set and once in a while evaluate it on test set and compute some metrics you also need to store the model parameters as a checkpoint and ideally you want to be able to stop and resume training tensorflows learn api is designed to make this job easier letting us focus on developing the actual model the most basic way of using tf learn api is to use tf estimator object directly you need to define a model function that defines a loss function a train op one or a set of predictions and optionally a set of metric ops for evaluation python import tensorflow as tf def model fn features labels mode params predictions loss train op metric ops return tf estimator estimatorspec mode mode predictions predictions loss loss train op train op eval metric ops metric ops params run config tf contrib learn runconfig model dir flags output dir estimator tf estimator estimator model fn model fn config run config params params to train the model you would then simply call estimator train function while providing an input function to read the data python def input fn features labels return features labels estimator train input fn input fn max steps and to evaluate the model simply call estimator evaluate estimator evaluate input fn input fn estimator object might be good enough for simple cases but tensorflow provides a higher level object called experiment which provides some additional useful functionality creating an experiment object is very easy python experiment tf contrib learn experiment estimator estimator train input fn train input fn eval input fn eval input fn now we can call train and evaluate function to compute the metrics while training experiment train and evaluate an even higher level way of running experiments is by using learn runner run function heres how our main function looks like in the provided framework python import tensorflow as tf tf flags define string output dir optional output dir tf flags define string schedule train and evaluate schedule tf flags define string hparams hyper parameters flags tf flags flags def experiment fn run config hparams estimator tf estimator estimator model fn make model fn config run config params hparams return tf contrib learn experiment estimator estimator train input fn make input fn tf estimator modekeys train hparams eval input fn make input fn tf estimator modekeys eval hparams def main unused argv run config tf contrib learn runconfig model dir flags output dir hparams tf contrib training hparams hparams parse flags hparams estimator tf contrib learn learn runner run experiment fn experiment fn run config run config schedule flags schedule hparams hparams if name main tf app run the schedule flag decides which member function of the experiment object gets called so if you for example set schedule to train and evaluate experiment train and evaluate would be called the input function returns two tensors or dictionaries of tensors providing the features and labels to be passed to the model python def input fn features labels return features labels see mnist py for an example of how to read your data with the dataset api to learn about various ways of reading your data in tensorflow refer to this item the framework also comes with a simple convolutional network classifier in alexnet py that includes an example model and thats it this is all you need to get started with tensorflow learn api i recommend to have a look at the framework source code and see the official python api to learn more about the learn api tensorflow cookbook this section includes implementation of a set of common operations in tensorflow get shape python def get shape tensor returns static shape if available and dynamic shape otherwise static shape tensor shape as list dynamic shape tf unstack tf shape tensor dims s 1 if s 0 is none else s 0 for s in zip static shape dynamic shape return dims batch gather python def batch gather tensor indices gather in batch from a tensor of arbitrary size in pseudocode this module will produce the following output i tf gather tensor i indices i args tensor tensor of arbitrary size indices vector of indices returns output a tensor of gathered values shape get shape tensor flat first tf reshape tensor shape 0 shape 1 shape 2 indices tf convert to tensor indices offset shape shape 0 1 indices shape ndims 1 offset tf reshape tf range shape 0 shape 1 offset shape output tf gather flat first indices offset return output beam search python import tensorflow as tf def rnn beam search update fn initial state sequence length beam width begin token id end token id name rnn beam search decoder for recurrent models args update fn function to compute the next state and logits given the current state and ids initial state recurrent model states sequence length length of the generated sequence beam width beam width begin token id begin token id end token id end token id name scope of the variables returns ids output indices logprobs output log probabilities probabilities batch size initial state shape as list 0 state tf tile tf expand dims initial state axis 1 1 beam width 1 sel sum logprobs tf log 1 0 beam width 1 ids tf tile begin token id batch size beam width sel ids tf zeros batch size beam width 0 dtype ids dtype mask tf ones batch size beam width dtype tf float32 for i in range sequence length with tf variable scope name reuse true if i 0 else none state logits update fn state ids logits tf nn log softmax logits sum logprobs tf expand dims sel sum logprobs axis 2 logits tf expand dims mask axis 2 num classes logits shape as list 1 sel sum logprobs indices tf nn top k tf reshape sum logprobs batch size num classes beam width k beam width ids indices num classes beam ids indices num classes state batch gather state beam ids sel ids tf concat batch gather sel ids beam ids tf expand dims ids axis 2 axis 2 mask batch gather mask beam ids tf to float tf not equal ids end token id return sel ids sel sum logprobs merge python import tensorflow as tf def merge tensors units activation tf nn relu name none kwargs merge features with broadcasting support this operation concatenates multiple features of varying length and applies non linear transformation to the outcome example a tf zeros m 1 d1 b tf zeros 1 n d2 c merge a b d3 shape of c would be m n d3 args tensors a list of tensor with the same rank units number of units in the projection function with tf variable scope name default name merge apply linear projection to input tensors projs for i tensor in enumerate tensors proj tf layers dense tensor units activation none name proj d i kwargs projs append proj compute sum of tensors result projs pop for proj in projs result result proj apply nonlinearity if activation result activation result return result entropy python import tensorflow as tf def softmax entropy logits dim 1 compute entropy over specified dimensions plogp tf nn softmax logits dim tf nn log softmax logits dim return tf reduce sum plogp dim kl divergence python def gaussian kl q p 0 0 computes kl divergence between two isotropic gaussian distributions to ensure numerical stability this op uses mu log sigma 2 to represent the distribution if q is not provided its assumed to be unit gaussian args q a tuple mu log sigma 2 representing a multi variatie gaussian p a tuple mu log sigma 2 representing a multi variatie gaussian returns a tensor representing kl q p mu1 log sigma1 sq q mu2 log sigma2 sq p return tf reduce sum 0 5 log sigma2 sq log sigma1 sq tf exp log sigma1 sq log sigma2 sq tf square mu1 mu2 tf exp log sigma2 sq 1 axis 1 make parallel python def make parallel fn num gpus kwargs parallelize given model on multiple gpu devices args fn arbitrary function that takes a set of input tensors and outputs a single tensor first dimension of inputs and output tensor are assumed to be batch dimension num gpus number of gpu devices kwargs keyword arguments to be passed to the model returns a tensor corresponding to the model output in splits for k v in kwargs items in splits k tf split v num gpus out split for i in range num gpus with tf device tf devicespec device type gpu device index i with tf variable scope tf get variable scope reuse tf auto reuse out split append fn k v i for k v in in splits items return tf concat out split axis 0 leaky relu python def leaky relu tensor alpha 0 1 computes the leaky rectified linear activation return tf maximum tensor alpha tensor batch normalization python def batch normalization tensor training false epsilon 0 001 momentum 0 9 fused batch norm false name none performs batch normalization on given 4 d tensor the features are assumed to be in nhwc format noe that you need to run update ops in order for this function to perform correctly e g with tf control dependencies tf get collection tf graphkeys update ops train op optimizer minimize loss based on https arxiv org abs 1502 03167 with tf variable scope name default name batch normalization channels tensor shape as list 1 axes list range tensor shape ndims 1 beta tf get variable beta channels initializer tf zeros initializer gamma tf get variable gamma channels initializer tf ones initializer avg mean tf get variable avg mean channels initializer tf zeros initializer trainable false avg variance tf get variable avg variance channels initializer tf ones initializer trainable false if training if fused batch norm mean variance none none else mean variance tf nn moments tensor axes axes else mean variance avg mean avg variance if fused batch norm tensor mean variance tf nn fused batch norm tensor scale gamma offset beta mean mean variance variance epsilon epsilon is training training else tensor tf nn batch normalization tensor mean variance beta gamma epsilon if training update mean tf assign avg mean avg mean momentum mean 1 0 momentum update variance tf assign avg variance avg variance momentum variance 1 0 momentum tf add to collection tf graphkeys update ops update mean tf add to collection tf graphkeys update ops update variance return tensor