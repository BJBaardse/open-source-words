table of contents introduction resources models supplementary data faq cheatsheet requirements building fasttext getting the source code building fasttext using make preferred building fasttext using cmake building fasttext for python example use cases word representation learning obtaining word vectors for out of vocabulary words text classification full documentation references enriching word vectors with subword information bag of tricks for efficient text classification fasttext zip compressing text classification models join the fasttext community license introduction fasttext is a library for efficient learning of word representations and sentence classification resources models recent state of the art english word vectors word vectors for 157 languages trained on wikipedia and crawl models for language identification and various supervised tasks supplementary data the preprocessed yfcc100m data used in 2 faq you can find answers to frequently asked questions on our website cheatsheet we also provide a cheatsheet full of useful one liners requirements we are continously building and testing our library cli and python bindings under various docker images using circleci generally fasttext builds on modern mac os and linux distributions since it uses some c 11 features it requires a compiler with good c 11 support these include g 4 7 2 or newer or clang 3 3 or newer compilation is carried out using a makefile so you will need to have a working make if you want to use cmake you need at least version 2 8 9 one of the oldest distributions we successfully built and tested the cli under is debian wheezy for the word similarity evaluation script you will need python 2 6 or newer numpy scipy for the python bindings see the subdirectory python you will need python version 2 7 or 3 4 numpy scipy pybind11 one of the oldest distributions we successfully built and tested the python bindings under is debian jessie if these requirements make it impossible for you to use fasttext please open an issue and we will try to accommodate you building fasttext we discuss building the latest stable version of fasttext getting the source code you can find our latest stable release in the usual place there is also the master branch that contains all of our most recent work but comes along with all the usual caveats of an unstable branch you might want to use this if you are a developer or power user building fasttext using make preferred wget https github com facebookresearch fasttext archive v0 1 0 zip unzip v0 1 0 zip cd fasttext 0 1 0 make this will produce object files for all the classes as well as the main binary fasttext if you do not plan on using the default system wide compiler update the two macros defined at the beginning of the makefile cc and includes building fasttext using cmake for now this is not part of a release so you will need to clone the master branch git clone https github com facebookresearch fasttext git cd fasttext mkdir build cd build cmake make make install this will create the fasttext binary and also all relevant libraries shared static pic building fasttext for python for now this is not part of a release so you will need to clone the master branch git clone https github com facebookresearch fasttext git cd fasttext pip install for further information and introduction see python readme md example use cases this library has two main use cases word representation learning and text classification these were described in the two papers 1 and 2 word representation learning in order to learn word vectors as described in 1 do fasttext skipgram input data txt output model where data txt is a training file containing utf 8 encoded text by default the word vectors will take into account character n grams from 3 to 6 characters at the end of optimization the program will save two files model bin and model vec model vec is a text file containing the word vectors one per line model bin is a binary file containing the parameters of the model along with the dictionary and all hyper parameters the binary file can be used later to compute word vectors or to restart the optimization obtaining word vectors for out of vocabulary words the previously trained model can be used to compute word vectors for out of vocabulary words provided you have a text file queries txt containing words for which you want to compute vectors use the following command fasttext print word vectors model bin queries txt this will output word vectors to the standard output one vector per line this can also be used with pipes cat queries txt fasttext print word vectors model bin see the provided scripts for an example for instance running word vector example sh will compile the code download data compute word vectors and evaluate them on the rare words similarity dataset rw thang et al 2013 text classification this library can also be used to train supervised text classifiers for instance for sentiment analysis in order to train a text classifier using the method described in 2 use fasttext supervised input train txt output model where train txt is a text file containing a training sentence per line along with the labels by default we assume that labels are words that are prefixed by the string label this will output two files model bin and model vec once the model was trained you can evaluate it by computing the precision and recall at k p k and r k on a test set using fasttext test model bin test txt k the argument k is optional and is equal to 1 by default in order to obtain the k most likely labels for a piece of text use fasttext predict model bin test txt k or use predict prob to also get the probability for each label fasttext predict prob model bin test txt k where test txt contains a piece of text to classify per line doing so will print to the standard output the k most likely labels for each line the argument k is optional and equal to 1 by default see classification example sh for an example use case in order to reproduce results from the paper 2 run classification results sh this will download all the datasets and reproduce the results from table 1 if you want to compute vector representations of sentences or paragraphs please use fasttext print sentence vectors model bin text txt this assumes that the text txt file contains the paragraphs that you want to get vectors for the program will output one vector representation per line in the file you can also quantize a supervised model to reduce its memory usage with the following command fasttext quantize output model this will create a ftz file with a smaller memory footprint all the standard functionality like test or predict work the same way on the quantized models fasttext test model ftz test txt the quantization procedure follows the steps described in 3 you can run the script quantization example sh for an example full documentation invoke a command without arguments to list available arguments and their default values fasttext supervised empty input or output path the following arguments are mandatory input training file path output output file path the following arguments are optional verbose verbosity level 2 the following arguments for the dictionary are optional mincount minimal number of word occurences 1 mincountlabel minimal number of label occurences 0 wordngrams max length of word ngram 1 bucket number of buckets 2000000 minn min length of char ngram 0 maxn max length of char ngram 0 t sampling threshold 0 0001 label labels prefix label the following arguments for training are optional lr learning rate 0 1 lrupdaterate change the rate of updates for the learning rate 100 dim size of word vectors 100 ws size of the context window 5 epoch number of epochs 5 neg number of negatives sampled 5 loss loss function ns hs softmax softmax thread number of threads 12 pretrainedvectors pretrained word vectors for supervised learning saveoutput whether output params should be saved 0 the following arguments for quantization are optional cutoff number of words and ngrams to retain 0 retrain finetune embeddings if a cutoff is applied 0 qnorm quantizing the norm separately 0 qout quantizing the classifier 0 dsub size of each sub vector 2 defaults may vary by mode word representation modes skipgram and cbow use a default mincount of 5 references please cite 1 if using this code for learning word representations or 2 if using for text classification enriching word vectors with subword information 1 p bojanowski e grave a joulin t mikolov enriching word vectors with subword information article bojanowski2017enriching title enriching word vectors with subword information author bojanowski piotr and grave edouard and joulin armand and mikolov tomas journal transactions of the association for computational linguistics volume 5 year 2017 issn 2307 387x pages 135 146 bag of tricks for efficient text classification 2 a joulin e grave p bojanowski t mikolov bag of tricks for efficient text classification inproceedings joulin2017bag title bag of tricks for efficient text classification author joulin armand and grave edouard and bojanowski piotr and mikolov tomas booktitle proceedings of the 15th conference of the european chapter of the association for computational linguistics volume 2 short papers month april year 2017 publisher association for computational linguistics pages 427 431 fasttext zip compressing text classification models 3 a joulin e grave p bojanowski m douze h j√©gou t mikolov fasttext zip compressing text classification models article joulin2016fasttext title fasttext zip compressing text classification models author joulin armand and grave edouard and bojanowski piotr and douze matthijs and j \e gou h \e rve and mikolov tomas journal arxiv preprint arxiv 1612 03651 year 2016 these authors contributed equally join the fasttext community facebook page https www facebook com groups 1174547215919768 google group https groups google com forum forum fasttext library contact egrave fb com bojanowski fb com ajoulin fb com tmikolov fb com see the contributing file for information about how to help out license fasttext is bsd licensed we also provide an additional patent grant