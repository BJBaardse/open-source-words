char rnn this code implements multi layer recurrent neural network rnn lstm and gru for training sampling from character level language models in other words the model takes one text file as input and trains a recurrent neural network that learns to predict the next character in a sequence the rnn can then be used to generate text character by character that will look like the original training data the context of this code base is described in detail in my blog post if you are new to torch lua neural nets it might be helpful to know that this code is really just a slightly more fancy version of this 100 line gist that i wrote in python numpy the code in this repo additionally allows for multiple layers uses an lstm instead of a vanilla rnn has more supporting code for model checkpointing and is of course much more efficient since it uses mini batches and can run on a gpu update torch rnn justin johnson jcjohnson recently re implemented char rnn from scratch with a much nicer smaller cleaner faster torch code base its under the name torch rnn it uses adam for optimization and hard codes the rnn lstm forward backward passes for space time efficiency this also avoids headaches with cloning models in this repo in other words torch rnn should be the default char rnn implemention to use now instead of the one in this code base requirements this code is written in lua and requires torch if youre on ubuntu installing torch in your home directory may look something like bash curl s https raw githubusercontent com torch ezinstall master install deps bash git clone https github com torch distro git torch recursive cd torch install sh and enter yes at the end to modify your bashrc source bashrc see the torch installation documentation for more details after torch is installed we need to get a few more packages using luarocks which already came with the torch install in particular bash luarocks install nngraph luarocks install optim luarocks install nn if youd like to train on an nvidia gpu using cuda this can be to about 15x faster youll of course need the gpu and you will have to install the cuda toolkit then get the cutorch and cunn packages bash luarocks install cutorch luarocks install cunn if youd like to use opencl gpu instead e g ati cards you will instead need to install the cltorch and clnn packages and then use the option opencl 1 during training cltorch issues bash luarocks install cltorch luarocks install clnn usage data all input data is stored inside the data directory youll notice that there is an example dataset included in the repo in folder data tinyshakespeare which consists of a subset of works of shakespeare im providing a few more datasets on this page your own data if youd like to use your own data then create a single file input txt and place it into a folder in the data directory for example data some folder input txt the first time you run the training script it will do some preprocessing and write two more convenience cache files into data some folder dataset sizes note that if your data is too small 1mb is already considered very small the rnn wont learn very effectively remember that it has to learn everything completely from scratch conversely if your data is large more than about 2mb feel confident to increase rnn size and train a bigger model see details of training below it will work significantly better for example with 6mb you can easily go up to rnn size 300 or even more the biggest that fits on my gpu and that ive trained with this code is rnn size 700 with num layers 3 2 is default training start training the model using train lua as a sanity check to run on the included example dataset simply try th train lua gpuid 1 notice that here we are setting the flag gpuid to 1 which tells the code to train using cpu otherwise it defaults to gpu 0 there are many other flags for various options consult th train lua help for comprehensive settings heres another example that trains a bigger network and also shows how you can run on your own custom dataset this already assumes that data some folder input txt exists th train lua data dir data some folder rnn size 512 num layers 2 dropout 0 5 checkpoints while the model is training it will periodically write checkpoint files to the cv folder the frequency with which these checkpoints are written is controlled with number of iterations as specified with the eval val every option e g if this is 1 then a checkpoint is written every iteration the filename of these checkpoints contains a very important number the loss for example a checkpoint with filename lm lstm epoch0 95 2 0681 t7 indicates that at this point the model was on epoch 0 95 i e it has almost done one full pass over the training data and the loss on validation data was 2 0681 this number is very important because the lower it is the better the checkpoint works once you start to generate data discussed below you will want to use the model checkpoint that reports the lowest validation loss notice that this might not necessarily be the last checkpoint at the end of training due to possible overfitting another important quantities to be aware of are batch size call it b seq length call it s and the train frac and val frac settings the batch size specifies how many streams of data are processed in parallel at one time the sequence length specifies the length of each stream which is also the limit at which the gradients can propagate backwards in time for example if seq length is 20 then the gradient signal will never backpropagate more than 20 time steps and the model might not find dependencies longer than this length in number of characters thus if you have a very difficult dataset where there are a lot of long term dependencies you will want to increase this setting now if at runtime your input text file has n characters these first all get split into chunks of size bxs these chunks then get allocated across three splits train val test according to the frac settings by default train frac is 0 95 and val frac is 0 05 which means that 95 of our data chunks will be trained on and 5 of the chunks will be used to estimate the validation loss and hence the generalization if your data is small its possible that with the default settings youll only have very few chunks in total for example 100 this is bad in these cases you may want to decrease batch size or sequence length note that you can also initialize parameters from a previously saved checkpoint using init from sampling given a checkpoint file such as those written to cv we can generate new text for example th sample lua cv some checkpoint t7 gpuid 1 make sure that if your checkpoint was trained with gpu it is also sampled from with gpu or vice versa otherwise the code will currently complain as with the train script see th sample lua help for full options one important one is for example length 10000 which would generate 10 000 characters default 2000 temperature an important parameter you may want to play with is temperature which takes a number in range 0 1 0 not included default 1 the temperature is dividing the predicted log probabilities before the softmax so lower temperature will cause the model to make more likely but also more boring and conservative predictions higher temperatures cause the model to take more chances and increase diversity of results but at a cost of more mistakes priming its also possible to prime the model with some starting text using primetext this starts out the rnn with some hardcoded characters to warm it up with some context before it starts generating text e g a fun primetext might be primetext the meaning of life is training with gpu but sampling on cpu right now the solution is to use the convert gpu cpu checkpoint lua script to convert your gpu checkpoint to a cpu checkpoint in near future you will not have to do this explicitly e g th convert gpu cpu checkpoint lua cv lm lstm epoch30 00 1 3950 t7 will create a new file cv lm lstm epoch30 00 1 3950 t7 cpu t7 that you can use with the sample script and with gpuid 1 for cpu mode happy sampling tips and tricks monitoring validation loss vs training loss if youre somewhat new to machine learning or neural networks it can take a bit of expertise to get good models the most important quantity to keep track of is the difference between your training loss printed during training and the validation loss printed once in a while when the rnn is run on the validation data by default every 1000 iterations in particular if your training loss is much lower than validation loss then this means the network might be overfitting solutions to this are to decrease your network size or to increase dropout for example you could try dropout of 0 5 and so on if your training validation loss are about equal then your model is underfitting increase the size of your model either number of layers or the raw number of neurons per layer approximate number of parameters the two most important parameters that control the model are rnn size and num layers i would advise that you always use num layers of either 2 3 the rnn size can be adjusted based on how much data you have the two important quantities to keep track of here are the number of parameters in your model this is printed when you start training the size of your dataset 1mb file is approximately 1 million characters these two should be about the same order of magnitude its a little tricky to tell here are some examples i have a 100mb dataset and im using the default parameter settings which currently print 150k parameters my data size is significantly larger 100 mil 0 15 mil so i expect to heavily underfit i am thinking i can comfortably afford to make rnn size larger i have a 10mb dataset and running a 10 million parameter model im slightly nervous and im carefully monitoring my validation loss if its larger than my training loss then i may want to try to increase dropout a bit and see if that heps the validation loss best models strategy the winning strategy to obtaining very good models if you have the compute time is to always err on making the network larger as large as youre willing to wait for it to compute and then try different dropout values between 0 1 whatever model has the best validation performance the loss written in the checkpoint filename low is good is the one you should use in the end it is very common in deep learning to run many different models with many different hyperparameter settings and in the end take whatever checkpoint gave the best validation performance by the way the size of your training and validation splits are also parameters make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative additional pointers and acknowledgements this code was originally based on oxford university machine learning class practical 6 which is in turn based on learning to execute code from wojciech zaremba chunks of it were also developed in collaboration with my labmate justin johnson to learn more about rnn language models i recommend looking at my recent talk on char rnn generating sequences with recurrent neural networks by alex graves generating text with recurrent neural networks by ilya sutskever tomas mikolovs thesis license mit