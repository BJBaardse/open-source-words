stream handbook this document covers the basics of how to write node js programs with streams you also could read a chinese edition node packaged manuscript you can install this handbook with npm just do npm install g stream handbook now you will have a stream handbook command that will open this readme file in your pager otherwise you may continue reading this document as you are presently doing introduction we should have some ways of connecting programs like garden hose screw in another segment when it becomes necessary to massage data in another way this is the way of io also doug mcilroy october 11 1964 streams come to us from the earliest days of unix and have proven themselves over the decades as a dependable way to compose large systems out of small components that do one thing well in unix streams are implemented by the shell with pipes in node the built in stream module is used by the core libraries and can also be used by user space modules similar to unix the node stream modules primary composition operator is called pipe and you get a backpressure mechanism for free to throttle writes for slow consumers streams can help to separate your concerns because they restrict the implementation surface area into a consistent interface that can be reused you can then plug the output of one stream to the input of another and use libraries that operate abstractly on streams to institute higher level flow control streams are an important component of small program design and unix philosophy but there are many other important abstractions worth considering just remember that technical debt is the enemy and to seek the best abstractions for the problem at hand why you should use streams i o in node is asynchronous so interacting with the disk and network involves passing callbacks to functions you might be tempted to write code that serves up a file from disk like this js var http require http var fs require fs var server http createserver function req res fs readfile dirname data txt function err data res end data server listen 8000 this code works but its bulky and buffers up the entire data txt file into memory for every request before writing the result back to clients if data txt is very large your program could start eating a lot of memory as it serves lots of users concurrently particularly for users on slow connections the user experience is poor too because users will need to wait for the whole file to be buffered into memory on your server before they can start receiving any contents luckily both of the req res arguments are streams which means we can write this in a much better way using fs createreadstream instead of fs readfile js var http require http var fs require fs var server http createserver function req res var stream fs createreadstream dirname data txt stream pipe res server listen 8000 here pipe takes care of listening for data and end events from the fs createreadstream this code is not only cleaner but now the data txt file will be written to clients one chunk at a time immediately as they are received from the disk using pipe has other benefits too like handling backpressure automatically so that node wont buffer chunks into memory needlessly when the remote client is on a really slow or high latency connection want compression there are streaming modules for that too js var http require http var fs require fs var oppressor require oppressor var server http createserver function req res var stream fs createreadstream dirname data txt stream pipe oppressor req pipe res server listen 8000 now our file is compressed for browsers that support gzip or deflate we can just let oppressor handle all that content encoding stuff once you learn the stream api you can just snap together these streaming modules like lego bricks or garden hoses instead of having to remember how to push data through wonky non streaming custom apis streams make programming in node simple elegant and composable basics there are 5 kinds of streams readable writable transform duplex and classic pipe all the different types of streams use pipe to pair inputs with outputs pipe is just a function that takes a readable source stream src and hooks the output to a destination writable stream dst src pipe dst pipe dst returns dst so that you can chain together multiple pipe calls together js a pipe b pipe c pipe d which is the same as js a pipe b b pipe c c pipe d this is very much like what you might do on the command line to pipe programs together a b c d except in node instead of the shell readable streams readable streams produce data that can be fed into a writable transform or duplex stream by calling pipe js readablestream pipe dst creating a readable stream lets make a readable stream js var readable require stream readable var rs new readable rs push beep rs push boop\n rs push null rs pipe process stdout node read0 js beep boop rs push null tells the consumer that rs is done outputting data note here that we pushed content to the readable stream rs before piping to process stdout but the complete message was still written this is because when you push to a readable stream the chunks you push are buffered until a consumer is ready to read them however it would be even better in many circumstances if we could avoid buffering data altogether and only generate the data when the consumer asks for it we can push chunks on demand by defining a read function js var readable require stream readable var rs readable var c 97 rs read function rs push string fromcharcode c if c z charcodeat 0 rs push null rs pipe process stdout node read1 js abcdefghijklmnopqrstuvwxyz here we push the letters a through z inclusive but only when the consumer is ready to read them the read function will also get a provisional size parameter as its first argument that specifies how many bytes the consumer wants to read but your readable stream can ignore the size if it wants note that you can also use util inherits to subclass a readable stream but that approach doesnt lend itself very well to comprehensible examples to show that our read function is only being called when the consumer requests we can modify our readable stream code slightly to add a delay js var readable require stream readable var rs readable var c 97 1 rs read function if c z charcodeat 0 return rs push null settimeout function rs push string fromcharcode c 100 rs pipe process stdout process on exit function console error \n read called c 97 times process stdout on error process exit running this program we can see that read is only called 5 times when we only request 5 bytes of output node read2 js head c5 abcde read called 5 times the settimeout delay is necessary because the operating system requires some time to send us the relevant signals to close the pipe the process stdout on error fn handler is also necessary because the operating system will send a sigpipe to our process when head is no longer interested in our programs output which gets emitted as an epipe error on process stdout these extra complications are necessary when interfacing with the external operating system pipes but are automatic when we interface directly with node streams the whole time if you want to create a readable stream that pushes arbitrary values instead of just strings and buffers make sure to create your readable stream with readable objectmode true consuming a readable stream most of the time its much easier to just pipe a readable stream into another kind of stream or a stream created with a module like through or concat stream but occasionally it might be useful to consume a readable stream directly js process stdin on readable function var buf process stdin read console dir buf echo abc sleep 1 echo def sleep 1 echo ghi node consume0 js buffer 61 62 63 0a buffer 64 65 66 0a buffer 67 68 69 0a null when data is available the readable event fires and you can call read to fetch some data from the buffer when the stream is finished read returns null because there are no more bytes to fetch you can also tell read n to return n bytes of data reading a number of bytes is merely advisory and does not work for object streams but all of the core streams support it heres an example of using read n to buffer stdin into 3 byte chunks js process stdin on readable function var buf process stdin read 3 console dir buf running this example gives us incomplete data echo abc sleep 1 echo def sleep 1 echo ghi node consume1 js buffer 61 62 63 buffer 0a 64 65 buffer 66 0a 67 this is because there is extra data left in internal buffers and we need to give node a kick to tell it that we are interested in more data past the 3 bytes that weve already read a simple read 0 will do this js process stdin on readable function var buf process stdin read 3 console dir buf process stdin read 0 now our code works as expected in 3 byte chunks js echo abc sleep 1 echo def sleep 1 echo ghi node consume2 js buffer 61 62 63 buffer 0a 64 65 buffer 66 0a 67 buffer 68 69 0a you can also use unshift to put back data so that the same read logic will fire when read gives you more data than you wanted using unshift prevents us from making unnecessary buffer copies here we can build a readable parser to split on newlines js var offset 0 process stdin on readable function var buf process stdin read if buf return for offset buf length offset if buf offset 0x0a console dir buf slice 0 offset tostring buf buf slice offset 1 offset 0 process stdin unshift buf return process stdin unshift buf tail n 50000 usr share dict american english head n10 node lines js hearties heartiest heartily heartiness heartiness\s heartland heartland\s heartlands heartless heartlessly however there are modules on npm such as split that you should use instead of rolling your own line parsing logic writable streams a writable stream is a stream you can pipe to but not from js src pipe writablestream creating a writable stream just define a write chunk enc next function and then you can pipe a readable stream in js var writable require stream writable var ws writable ws write function chunk enc next console dir chunk next process stdin pipe ws echo beep sleep 1 echo boop node write0 js buffer 62 65 65 70 0a buffer 62 6f 6f 70 0a the first argument chunk is the data that is written by the producer the second argument enc is a string with the string encoding but only when opts decodestring is false and youve been written a string the third argument next err is the callback that tells the consumer that they can write more data you can optionally pass an error object err which emits an error event on the stream instance if the readable stream youre piping from writes strings they will be converted into buffers unless you create your writable stream with writable decodestrings false if the readable stream youre piping from writes objects create your writable stream with writable objectmode true writing to a writable stream to write to a writable stream just call write data with the data you want to write js process stdout write beep boop\n to tell the destination writable stream that youre done writing just call end you can also give end data some data to write before ending js var fs require fs var ws fs createwritestream message txt ws write beep settimeout function ws end boop\n 1000 node writing1 js cat message txt beep boop if you care about high water marks and buffering write returns false when there is more data than the opts highwatermark option passed to writable in the incoming buffer if you want to wait for the buffer to empty again listen for a drain event transform transform streams are a certain type of duplex stream both readable and writable the distinction is that in transform streams the output is in some way calculated from the input you might also hear transform streams referred to as through streams through streams are simple readable writable filters that transform input and produce output duplex duplex streams are readable writable and both ends of the stream engage in a two way interaction sending back and forth messages like a telephone an rpc exchange is a good example of a duplex stream any time you see something like js a pipe b pipe a youre probably dealing with a duplex stream classic streams classic streams are the old interface that first appeared in node 0 4 you will probably encounter this style of stream for a long time so its good to know how they work whenever a stream has a data listener registered it switches into classic mode and behaves according to the old api classic readable streams classic readable streams are just event emitters that emit data events when they have data for their consumers and emit end events when they are done producing data for their consumers pipe checks whether a classic stream is readable by checking the truthiness of stream readable here is a super simple readable stream that prints a through j inclusive js var stream require stream var stream new stream stream readable true var c 64 var iv setinterval function if c 75 clearinterval iv stream emit end else stream emit data string fromcharcode c 100 stream pipe process stdout node classic0 js abcdefghij to read from a classic readable stream you register data and end listeners heres an example reading from process stdin using the old readable stream style js process stdin on data function buf console log buf process stdin on end function console log end echo beep sleep 1 echo boop node classic1 js buffer 62 65 65 70 0a buffer 62 6f 6f 70 0a end note that whenever you register a data listener you put the stream into compatability mode so you lose the benefits of the new streams2 api you should pretty much never register data and end handlers yourself anymore if you need to interact with legacy streams use libraries that you can pipe to instead where possible for example you can use through to avoid setting up explicit data and end listeners js var through require through process stdin pipe through write end function write buf console log buf function end console log end echo beep sleep 1 echo boop node through js buffer 62 65 65 70 0a buffer 62 6f 6f 70 0a end or use concat stream to buffer up an entire streams contents js var concat require concat stream process stdin pipe concat function body console log json parse body echo beep boop node concat js beep boop classic readable streams have pause and resume logic for provisionally pausing a stream but this was merely advisory if you are going to use pause and resume with classic readable streams you should use through to handle buffering instead of writing that yourself classic writable streams classic writable streams are very simple just define write buf end buf and destroy end buf may or may not get a buf but node people will expect stream end buf to mean stream write buf stream end and you shouldnt violate their expectations read more core stream documentation you can use the readable stream module to make your streams2 code compliant with node 0 8 and below just require readable stream instead of require stream after you npm install readable stream built in streams these streams are built into node itself process process stdin this readable stream contains the standard system input stream for your program it is paused by default but the first time you refer to it resume will be called implicitly on the next tick if process stdin is a tty check with tty isatty then input events will be line buffered you can turn off line buffering by calling process stdin setrawmode true but the default handlers for key combinations such as c and d will be removed process stdout this writable stream contains the standard system output stream for your program write to it if you want to send data to stdout process stderr this writable stream contains the standard system error stream for your program write to it if you want to send data to stderr child process spawn fs fs createreadstream fs createwritestream net net connect this function returns a duplex stream that connects over tcp to a remote host you can start writing to the stream right away and the writes will be buffered until the connect event fires net createserver http http request http createserver zlib zlib creategzip zlib creategunzip zlib createdeflate zlib createinflate control streams through from pause stream concat stream concat stream will buffer up stream contents into a single buffer concat cb just takes a single callback cb body with the buffered body when the stream has finished for example in this program the concat callback fires with the body string beep boop once cs end is called the program takes the body and upper cases it printing beep boop js var concat require concat stream var cs concat function body console log body touppercase cs write beep cs write boop cs end node concat js beep boop heres an example usage of concat stream that will parse incoming url encoded form data and reply with a stringified json version of the form parameters js var http require http var qs require querystring var concat require concat stream var server http createserver function req res req pipe concat function body var params qs parse body tostring res end json stringify params \n server listen 5005 curl x post d beep boop dinosaur trex http localhost 5005 beep boop dinosaur trex duplex duplexer emit stream invert stream map stream remote events buffer stream event stream auth stream meta streams mux demux stream router multi channel mdm state streams crdt delta stream scuttlebutt scuttlebutt can be used for peer to peer state synchronization with a mesh topology where nodes might only be connected through intermediaries and there is no node with an authoritative version of all the data the kind of distributed peer to peer network that scuttlebutt provides is especially useful when nodes on different sides of network barriers need to share and update the same state an example of this kind of network might be browser clients that send messages through an http server to each other and backend processes that the browsers cant directly connect to another use case might be systems that span internal networks since ipv4 addresses are scarce scuttlebutt uses a gossip protocol to pass messages between connected nodes so that state across all the nodes will eventually converge on the same value everywhere using the scuttlebutt model interface we can create some nodes and pipe them to each other to create whichever sort of network we want js var model require scuttlebutt model var am new model var as am createstream var bm new model var bs bm createstream var cm new model var cs cm createstream var dm new model var ds dm createstream var em new model var es em createstream as pipe bs pipe as bs pipe cs pipe bs bs pipe ds pipe bs ds pipe es pipe ds em on update function key value source console log key value from source am set x 555 the network weve created is an undirected graph that looks like a b c v d e note that nodes a and e arent directly connected but when we run this script node model js x 555 from 1347857300518 the value that node a set finds its way to node e by way of nodes b and d here all the nodes are in the same process but because scuttlebutt uses a simple streaming interface the nodes can be placed on any process or server and connected with any streaming transport that can handle string data next we can make a more realistic example that connects over the network and increments a counter variable heres the server which will set the initial count value to 0 and count every 320 milliseconds printing all updates to count js var model require scuttlebutt model var net require net var m new model m set count 0 m on update function key value console log key m get count var server net createserver function stream stream pipe m createstream pipe stream server listen 8888 setinterval function m set count number m get count 1 320 now we can make a client that connects to this server updates the count on an interval and prints all the updates it receives js var model require scuttlebutt model var net require net var m new model var s m createstream s pipe net connect 8888 localhost pipe s m on update function cb key wait until weve gotten at least one count value from the network if key count return m removelistener update cb setinterval function m set count number m get count 1 100 m on update function key value console log key value the client is slightly trickier since it should wait until it has an update from somebody else to start updating the counter itself or else its counter would be zeroed once we get the server and some clients running we should see a sequence like this count 183 count 184 count 185 count 186 count 187 count 188 count 189 occasionally on some of the nodes we might see a sequence with repeated values like count 147 count 148 count 149 count 149 count 150 count 151 these values are due to scuttlebutts history based conflict resolution algorithm which is hard at work ensuring that the state of the system across all nodes is eventually consistent note that the server in this example is just another node with the same privledges as the clients connected to it the terms client and server here dont affect how the state synchronization proceeds just who initiates the connection protocols with this property are often called symmetric protocols see dnode for another example of a symmetric protocol append only http streams request oppressor response stream io streams reconnect kv discovery network parser streams tar trumpet jsonstream use this module to parse and stringify json data from streams if you need to pass a large json collection through a slow connection or you have a json object that will populate slowly this module will let you parse data incrementally as it arrives json scrape stream serializer browser streams shoe domnode sorta graph stream arrow keys attribute data bind html streams hyperstream audio streams baudio rpc streams dnode dnode lets you call remote functions through any kind of stream heres a basic dnode server js var dnode require dnode var net require net var server net createserver function c var d dnode transform function s cb cb s replace aeiou 2 oo touppercase c pipe d pipe c server listen 5004 then you can hack up a simple client that calls the servers transform function js var dnode require dnode var net require net var d dnode d on remote function remote remote transform beep function s console log beep s d end var c net connect 5004 c pipe d pipe c fire up the server then when you run the client you should see node client js beep boop the client sent beep to the servers transform function and the server called the clients callback with the result neat the streaming interface that dnode provides here is a duplex stream since both the client and server are piped to each other c pipe d pipe c with requests and responses coming from both sides the craziness of dnode begins when you start to pass function arguments to stubbed callbacks heres an updated version of the previous server with a multi stage callback passing dance js var dnode require dnode var net require net var server net createserver function c var d dnode transform function s cb cb function n fn var oo array n 1 join o fn s replace aeiou 2 oo touppercase c pipe d pipe c server listen 5004 heres the updated client js var dnode require dnode var net require net var d dnode d on remote function remote remote transform beep function cb cb 10 function s console log beep 10 s d end var c net connect 5004 c pipe d pipe c after we spin up the server when we run the client now we get node client js beep 10 boooooooooop it just works ™ the basic idea is that you just put functions in objects and you call them from the other side of a stream and the functions will be stubbed out on the other end to do a round trip back to the side that had the original function in the first place the best thing is that when you pass functions to a stubbed function as arguments those functions get stubbed out on the other side this approach of stubbing function arguments recursively shall henceforth be known as the turtles all the way down gambit the return values of any of your functions will be ignored and only enumerable properties on objects will be sent json style its turtles all the way down since dnode works in node or on the browser over any stream its easy to call functions defined anywhere and especially useful when paired up with mux demux to multiplex an rpc stream for control alongside some bulk data streams rpc stream test streams tap stream spec power combos distributed partition tolerant chat the append only module can give us a convenient append only array on top of scuttlebutt which makes it really easy to write an eventually consistent distributed chat that can replicate with other nodes and survive network partitions todo the rest roll your own socket io we can build a socket io style event emitter api over streams using some of the libraries mentioned earlier in this document first we can use shoe to create a new websocket handler server side and emit stream to turn an event emitter into a stream that emits objects the object stream can then be fed into jsonstream to serialize the objects and from there the serialized stream can be piped into the remote browser js var eventemitter require events eventemitter var shoe require shoe var emitstream require emit stream var jsonstream require jsonstream var sock shoe function stream var ev new eventemitter emitstream ev pipe jsonstream stringify pipe stream inside the shoe callback we can emit events to the ev function here well just emit different kinds of events on intervals js var intervals intervals push setinterval function ev emit upper abc 500 intervals push setinterval function ev emit lower def 300 stream on end function intervals foreach clearinterval finally the shoe instance just needs to be bound to an http server js var http require http var server http createserver require ecstatic dirname server listen 8080 sock install server sock meanwhile on the browser side of things just parse the json shoe stream and pass the resulting object stream to eventstream eventstream just returns an event emitter that emits the server side events js var shoe require shoe var emitstream require emit stream var jsonstream require jsonstream var parser jsonstream parse true var stream parser pipe shoe sock pipe parser var ev emitstream stream ev on lower function msg var div document createelement div div textcontent msg tolowercase document body appendchild div ev on upper function msg var div document createelement div div textcontent msg touppercase document body appendchild div use browserify to build this browser source code so that you can require all these nifty modules browser side browserify main js o bundle js then drop a script src bundle js script into some html and open it up in a browser to see server side events streamed through to the browser side of things with this streaming approach you can rely more on tiny reusable components that only need to know how to talk streams instead of routing messages through a global event system socket io style you can focus more on breaking up your application into tinier units of functionality that can do exactly one thing well for instance you can trivially swap out jsonstream in this example for stream serializer to get a different take on serialization with a different set of tradeoffs you could bolt layers over top of shoe to handle reconnections or heartbeats using simple streaming interfaces you could even add a stream into the chain to use namespaced events with eventemitter2 instead of the eventemitter in core if you want some different streams that act in different ways it would likewise be pretty simple to run the shoe stream in this example through mux demux to create separate channels for each different kind of stream that you need as the requirements of your system evolve over time you can swap out each of these streaming pieces as necessary without as many of the all or nothing risks that more opinionated framework approaches necessarily entail html streams for the browser and the server we can use some streaming modules to reuse the same html rendering logic for the client and the server this approach is indexable seo friendly and gives us realtime updates our renderer takes lines of json as input and returns html strings as its output text the universal interface render js js var through require through var hyperglue require hyperglue var fs require fs var html fs readfilesync dirname static row html utf8 module exports function return through function line try var row json parse line catch err return this emit error err this queue hyperglue html who row who message row message outerhtml we can use brfs to inline the fs readfilesync call for browser code and hyperglue to update html based on css selectors you dont need to use hyperglue necessarily here anything that can return a string with html in it will work the row html used is just a really simple stub thing row html html div class row div class who div div class message div div the server will just use slice file to keep everything simple slice file is little more than a glorified tail tail f api but the interfaces map well to databases with regular results plus a changes feed like couchdb server js js var http require http var fs require fs var hyperstream require hyperstream var ecstatic require ecstatic dirname static var slicefile require slice file var sf slicefile dirname data txt var render require render var server http createserver function req res if req url var hs hyperstream rows sf slice 5 pipe render hs pipe res fs createreadstream dirname static index html pipe hs else ecstatic req res server listen 8000 var shoe require shoe var sock shoe function stream sf follow 1 0 pipe stream sock install server sock the first part of the server handles the route and streams the last 5 lines from data txt into the rows div the second part of the server handles realtime updates to rows using shoe a simple streaming websocket polyfill next we can write some simple browser code to populate the realtime updates from shoe into the rows div js var through require through var render require render var shoe require shoe var stream shoe sock var rows document queryselector rows stream pipe render pipe through function html rows innerhtml html just compile with browserify and brfs browserify t brfs browser js static bundle js and thats it now we can populate data txt with some silly data echo who substack message beep boop data txt echo who zoltar message cower puny humans data txt then spin up the server node server js then navigate to localhost 8000 where we will see our content if we add some more content echo who substack message oh hello data txt echo who zoltar message hear me data txt then the page updates automatically with the realtime updates hooray were now using exactly the same rendering logic on both the client and the server to serve up seo friendly indexable realtime content hooray